{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import openai\n",
    "from PIL import Image\n",
    "import re\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "img_source_dir = './data/images/'\n",
    "img_meta_file_path = './data/02_img_json_relevant_keys.json'\n",
    "output_file_path = './data/03_img_json_with_questions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(img_meta_file_path, 'r') as json_file:\n",
    "    raw_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2102.09837v1-Figure2-1.png': {'ocr': [[[50.0, 120.0],\n",
       "    [120.0, 123.0],\n",
       "    [119.0, 157.0],\n",
       "    [49.0, 154.0]],\n",
       "   ['Init', 0.999854564666748]],\n",
       "  'True_Statements': ['There can be multiple calibrating steps.',\n",
       "   'There is an init step.'],\n",
       "  'False_Statements': ['There can not be multiple calibrating steps.',\n",
       "   'There is no init step.'],\n",
       "  'Flowchart-to-Caption': 'Figure 2: The platform model of a robot arm.',\n",
       "  'caption': 'Figure 2: The platform model of a robot arm.',\n",
       "  'imageText': ['tp',\n",
       "   ':=',\n",
       "   '0',\n",
       "   '}',\n",
       "   'Calibrating',\n",
       "   's',\n",
       "   'calibrate',\n",
       "   '{',\n",
       "   'tp',\n",
       "   '=',\n",
       "   '5',\n",
       "   'Calibrated',\n",
       "   'e',\n",
       "   'calibrate',\n",
       "   'tp',\n",
       "   ':=',\n",
       "   '0',\n",
       "   '}',\n",
       "   'Calibrating',\n",
       "   's',\n",
       "   'calibrate',\n",
       "   '{',\n",
       "   '{Calibrated}',\n",
       "   'Calibrated',\n",
       "   '{Calibrating}',\n",
       "   'Calibrating',\n",
       "   '{Ready}',\n",
       "   'Init'],\n",
       "  'image_file': '2102.09837v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Platform Models',\n",
       "    'text': \"We model the robot platform with timed automata, an example is shown in Figure 2. Similar to PTAs, we expect a platform model to use an alphabet with symbols of the form {f 1 , . . . f k , a}, where a ‚àà N A \\\\ A Œ£ is a platform action and f i ‚àà P F \\\\ P Œ£ are exactly those primitive formulas that are true after executing the action. We expect f i and a to be from a different alphabet than the BAT, i.e., the platform does not have any effects on the abstract program and vice versa. Further, to guarantee that the platform model does not block the PTA, we expect it to contain self loops, similar to the self loops of a PTA, and as shown in Figure 2.\\nPlatform Constraints Given a determinate fd-BAT Œ£ and a platform model R, we can formulate constraints over Œ£ and R:\\nG¬¨Calibrated ‚äÉ ¬¨F ‚â§10 ‚àÉp:o. Perf (pick (p)) (10) GCalibrating ‚äÉ ‚àÉl:o. RAt(l) ‚àß Spacious(l) (11)\\nThe first constraint states that if the robot's arm is not calibrated, it must not perform a pick action in the next 10 seconds, i.e., it must calibrate the arm before doing pick. The second constraint says that if the robot is calibrating its arm, it must be at a location that provides enough space for doing so, i.e., a Spacious location.\\n7 Synthesizing a Controller Using the TA PTA(Œ£, Œ¥) that represents the program Œ¥, the TA R for the platform, and constraints Œ¶, we can use MTL synthesis to synthesize a controller that executes Œ¥ while satisfying the platform constraints. Specifically, we use 1. the plant P = PTA(Œ£, Œ¥) √ó R, 2. as controllable actions P C all symbols that contain start actions of the program or the platform model, i.e., P C = {S | S ‚àà P, s a( t) ‚àà S for some a( t)}, 3. as environment actions P E all symbols that contain end actions of the program or the platform model, i.e., P E = {E | E ‚àà P, e a( t) ‚àà E} for some a( t),\\n\\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 RAt(m1) At (o1, m2) Calibrating s calibrate \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 RAt(m1) At (o1, m2) Calibrated e calibrate \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe tc := 0 \\uf8f1 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f3 At (o1, m2) s goto(m1, m2) Calibrated \\uf8fc \\uf8f4 \\uf8fd \\uf8f4 \\uf8fe \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 RAt(m2) At (o1, m2) e goto(m1, m2) Calibrated \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe * \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 RAt (m2) At (o1, m2) Perf (pick (o1)) s pick (o1) Calibrated \\uf8fc \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fd \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8fe tc > 10\\n4. a fixed granularity ¬µ, e.g., based on the robot platform's time resolution 5. the set of MTL formulas Œ¶ as specification of desired behaviors.\\nFigure 3 shows a possible controller for our example program from Listing 1, the platform from Figure 2, and the constraints from Section 6.\\nWe can show that (1) the resulting controller indeed satisfies the constraints and (2) each of its traces is equivalent to some trace of the original program, i.e., the resulting controller satisfies the same situation formulas as the original program at any point of the execution: Theorem 6. Let Œ£ be a determinate fd-BAT, Œ¥ a program over Œ£ that only induces finite traces, R a platform model with symbols disjunct with the symbols from Œ£, and let the constraints Œ¶ be a set of MTL formulas. Let C be the synthesized MTL controller with L = L((PTA(Œ£, Œ¥) √ó R) C). Then:\\n1. L ‚äÜ L(Œ¶), i.e., all constraints are satisfied. 2. For every œÅ = œÅ ‚Ä≤ ‚Ä¢ œÅ ‚Ä≤‚Ä≤ ‚àà L, ¬µ(œÅ) ‚àà Œ¥ wŒ£ , and for every fluent state formula restricted to Œ£:\\nœÅ ‚Ä≤ |= Œ± ‚áî w Œ£ , ¬µ(œÅ ‚Ä≤ ) |= Œ±\\nProof.\\n1. Follows directly from Theorem 4. 2. First, note that L ‚äÜ L(PTA(Œ£, Œ¥) √ó R). Second, as R does not contain any action standard name from Œ£, for every œÅ ‚àà L, there is a œÅ ‚Ä≤ ‚àà PTA(Œ£, Œ¥) such that ¬µ(œÅ) = ¬µ(œÅ ‚Ä≤ ). By Theorem 5, for every œÅ ‚Ä≤ ‚àà PTA(Œ£, Œ¥), ¬µ(œÅ ‚Ä≤ ) ‚àà Œ¥ wŒ£ and œÅ ‚Ä≤ |= Œ± iff w Œ£ , ¬µ(œÅ ‚Ä≤ ) |= Œ±.\\nThus, the resulting controller preserves the program's original effects while satisfying all platform constraints.\",\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 4}],\n",
       "  'title': 'Controller Synthesis for Golog Programs over Finite Domains with Metric Temporal Constraints',\n",
       "  'abstract': 'Executing a Golog program on an actual robot typically requires additional steps to account for hardware or software details of the robot platform, which can be formulated as constraints on the program. Such constraints are often temporal, refer to metric time, and require modifications to the abstract Golog program. We describe how to formulate such constraints based on a modal variant of the Situation Calculus. These constraints connect the abstract program with the platform models, which we describe using timed automata. We show that for programs over finite domains and with fully known initial state, the problem of synthesizing a controller that satisfies the constraints while preserving the effects of the original program can be reduced to MTL synthesis. We do this by constructing a timed automaton from the abstract program and synthesizing an MTL controller from this automaton, the platform models, and the constraints. We prove that the synthesized controller results in execution traces which are the same as those of the original program, possibly interleaved with platform-dependent actions, that they satisfy all constraints, and that they have the same effects as the traces of the original program. By doing so, we obtain a decidable procedure to synthesize a controller that satisfies the specification while preserving the original program.',\n",
       "  'paddleOCR': [[[[580.0, 5.0], [729.0, 5.0], [729.0, 29.0], [580.0, 29.0]],\n",
       "    ['Calibrated', 0.999769389629364]],\n",
       "   [[[23.0, 19.0], [144.0, 21.0], [143.0, 53.0], [23.0, 50.0]],\n",
       "    ['{Ready}', 0.9998531341552734]],\n",
       "   [[[346.0, 17.0], [539.0, 19.0], [539.0, 47.0], [346.0, 44.0]],\n",
       "    ['{ Calibrating}', 0.9691534042358398]],\n",
       "   [[[575.0, 49.0], [729.0, 49.0], [729.0, 73.0], [575.0, 73.0]],\n",
       "    ['e-calibrate', 0.95401930809021]],\n",
       "   [[[648.0, 95.0], [696.0, 95.0], [696.0, 114.0], [648.0, 114.0]],\n",
       "    [' 5', 0.8406924605369568]],\n",
       "   [[[50.0, 120.0], [120.0, 123.0], [119.0, 157.0], [49.0, 154.0]],\n",
       "    ['Init', 0.999854564666748]],\n",
       "   [[[351.0, 118.0], [533.0, 123.0], [532.0, 154.0], [350.0, 149.0]],\n",
       "    ['Calibrating', 0.9998288750648499]],\n",
       "   [[[772.0, 121.0], [938.0, 125.0], [938.0, 156.0], [771.0, 153.0]],\n",
       "    ['Calibrated', 0.9996438026428223]],\n",
       "   [[[151.0, 158.0], [316.0, 165.0], [315.0, 192.0], [150.0, 186.0]],\n",
       "    ['Calibrating', 0.9997832775115967]],\n",
       "   [[[155.0, 206.0], [308.0, 206.0], [308.0, 231.0], [155.0, 231.0]],\n",
       "    ['s_calibrate', 0.9874115586280823]],\n",
       "   [[[745.0, 226.0], [941.0, 223.0], [941.0, 252.0], [746.0, 254.0]],\n",
       "    ['{Calibrated}', 0.9989538192749023]],\n",
       "   [[[573.0, 236.0], [732.0, 240.0], [731.0, 264.0], [573.0, 261.0]],\n",
       "    ['Calibrating', 0.9998205304145813]],\n",
       "   [[[575.0, 281.0], [727.0, 281.0], [727.0, 305.0], [575.0, 305.0]],\n",
       "    ['s-calibrate', 0.9589548707008362]]]},\n",
       " '2210.01528v1-Figure3-1.png': {'ocr': [[[579.0, 51.0],\n",
       "    [753.0, 53.0],\n",
       "    [753.0, 72.0],\n",
       "    [579.0, 71.0]],\n",
       "   ['Terrain-Correction', 0.9980838298797607]],\n",
       "  'True_Statements': ['Speckle Filter is before Terrain-Flattening.',\n",
       "   'Subset is after Terrain-Correction.'],\n",
       "  'False_Statements': ['Speckle Filter is after Terrain-Flattening.',\n",
       "   'Subset is before Terrain-Correction.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 3: Illustration of one of the back-scattering SNAP pipeline we have integrated in the system.',\n",
       "  'caption': 'Fig. 3: Illustration of one of the back-scattering SNAP pipeline we have integrated in the system.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2210.01528v1-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Back-scattering analysis',\n",
       "    'text': 'Back-scattering, also known as retro-reflection, is a physical phenomenon in which waves impacting a material at a certain angle are reflected at the same angle, back to the source from whence they originated. This phenomenon is usually explored in Synthetic-aperture radar (SAR) remote sensing data. The analysis of this aspect of the SAR imagery helps monitor different aspects of crop monitoring [8], for instance, detecting flooded using classification models [9].\\nIn this experiment, to generate the product of backscattering analysis, we integrated the ESA Sentinel Applications Platform (SNAP). This framework provides a product processing open toolbox for several satellites. In fact, we have used Snapista, which is a SNAP execution engine that facilitates the programmatic generation of SNAP GPT graphs (see fig. 3). This engine supplies access to all functionalities derived from the toolboxes. Indeed, these graphs determine the necessary processing pipeline in order to obtain the expected product from distinct processed level satellite images (e.g., Sentinel 1, 2,3). In this illustrative example, we consider the back-scattering procedure 3. Once the data is requested, the required source data (Sentinel-1) is loaded on-demand. Straightaway, the Snapista engine loads and executes the processing pipeline defined in the declaration of the virtual product. In this case, the processing is composed of different SNAP toolbox processing components, creating an adapted SNAP pipeline. The final result is visible in Figure 4. This usage of the SNAP platform is extensible to several product-level processing pipelines. It facilitates the application of complex algorithms provided by the toolbox, which is helpful in remote sensing-based analysis.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'INTEGRATING PRE-PROCESSING PIPELINES IN ODC BASED FRAMEWORK',\n",
       "  'abstract': 'Using on-demand processing pipelines to generate virtual geospatial products is beneficial to optimizing resource management and decreasing processing requirements and data storage space. Additionally, pre-processed products improve data quality for data-driven analytical algorithms, such as machine learning or deep learning models. This paper proposes a method to integrate virtual products based on integrating open-source processing pipelines. In order to validate and evaluate the functioning of this approach, we have integrated it into a geo-imagery management framework based on Open Data Cube (ODC). To validate the methodology, we have performed three experiments developing on-demand processing pipelines using multi-sensor remote sensing data, for instance, Sentinel-1 and Sentinel-2. These pipelines are integrated using open-source processing frameworks.',\n",
       "  'paddleOCR': [[[[76.0, 55.0], [122.0, 55.0], [122.0, 72.0], [76.0, 72.0]],\n",
       "    ['Read', 0.9983166456222534]],\n",
       "   [[[178.0, 55.0], [384.0, 55.0], [384.0, 70.0], [178.0, 70.0]],\n",
       "    ['ThermalNoiseRemoval', 0.9832522869110107]],\n",
       "   [[[418.0, 51.0], [546.0, 51.0], [546.0, 71.0], [418.0, 71.0]],\n",
       "    ['Speckle-Filter', 0.9724626541137695]],\n",
       "   [[[579.0, 51.0], [753.0, 53.0], [753.0, 72.0], [579.0, 71.0]],\n",
       "    ['Terrain-Correction', 0.9980838298797607]],\n",
       "   [[[781.0, 54.0], [936.0, 52.0], [936.0, 72.0], [781.0, 73.0]],\n",
       "    ['LinearToFromdB', 0.9961383938789368]],\n",
       "   [[[120.0, 122.0], [265.0, 121.0], [265.0, 141.0], [120.0, 143.0]],\n",
       "    ['Apply-Orbit-File', 0.9971608519554138]],\n",
       "   [[[318.0, 118.0], [420.0, 121.0], [419.0, 142.0], [317.0, 139.0]],\n",
       "    ['Calibration', 0.9975643754005432]],\n",
       "   [[[459.0, 121.0], [624.0, 122.0], [624.0, 141.0], [459.0, 140.0]],\n",
       "    ['Terrain-Flattening', 0.9981461763381958]],\n",
       "   [[[671.0, 121.0], [741.0, 121.0], [741.0, 140.0], [671.0, 140.0]],\n",
       "    ['Subset', 0.9978947043418884]]]},\n",
       " '2205.13948v1-Figure4-1.png': {'ocr': [[[1793.0, 44.0],\n",
       "    [1817.0, 44.0],\n",
       "    [1817.0, 77.0],\n",
       "    [1793.0, 77.0]],\n",
       "   ['1', 0.9998341798782349]],\n",
       "  'True_Statements': ['Selection is before crossover.',\n",
       "   'Evaluation is before selection.'],\n",
       "  'False_Statements': ['Selection is after crossover.',\n",
       "   'Evaluation is after selection.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 4. Overview of PEGA.',\n",
       "  'caption': 'Fig. 4. Overview of PEGA.',\n",
       "  'imageText': ['‚Ä¶Chromosome',\n",
       "   'ùëò',\n",
       "   'EVALUATION9',\n",
       "   '5',\n",
       "   '7',\n",
       "   '√ó',\n",
       "   'ùë°',\n",
       "   '7',\n",
       "   '2',\n",
       "   '6',\n",
       "   '4',\n",
       "   '1',\n",
       "   '7',\n",
       "   '6',\n",
       "   '4',\n",
       "   '1',\n",
       "   '2',\n",
       "   '4',\n",
       "   '1',\n",
       "   '2',\n",
       "   '5',\n",
       "   '7',\n",
       "   '6',\n",
       "   '3',\n",
       "   '1',\n",
       "   '2',\n",
       "   '67',\n",
       "   '5',\n",
       "   '21',\n",
       "   '3',\n",
       "   '1',\n",
       "   'm',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   'Parent',\n",
       "   '1',\n",
       "   'Parent',\n",
       "   '2',\n",
       "   'Children',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   'Start',\n",
       "   'ùëùùëõ',\n",
       "   'ùëù8',\n",
       "   'ùëù7',\n",
       "   'ùëù5ùëù6',\n",
       "   'ùëù4',\n",
       "   'ùëù3',\n",
       "   'ùëù2',\n",
       "   'ùëõ',\n",
       "   '2',\n",
       "   '1',\n",
       "   'Gene',\n",
       "   'm',\n",
       "   'e',\n",
       "   'o',\n",
       "   'so',\n",
       "   'ro',\n",
       "   'm',\n",
       "   'C',\n",
       "   'h',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   '2',\n",
       "   '‚Ä¶',\n",
       "   'GEN_INITIAL_POP',\n",
       "   'SELECTION',\n",
       "   'CROSSOVER',\n",
       "   'MUTATION'],\n",
       "  'image_file': '2205.13948v1-Figure4-1.png',\n",
       "  'sections': [{'heading': 'C. Overview of PEGA',\n",
       "    'text': 'In this section, we give a high-level description of our proposed PEGA. The goal of PEGA is to perform operations of GA over encrypted data and output an encrypted optimization solution. As shown in Fig. 4, PEGA consists of 5 polynomial-time operations, i.e., GEN_INITIAL_POP, EVALUATION, SELECTION, CROSSOVER, and MUTATION, and their briefly description is given as follows.\\n‚Ä¢ GEN_INITIAL_POP: Given an encrypted COP P , GEN_INITIAL_POP randomly generates a population compromising n individuals. Each individual is denoted by a chromosome. Each chromosome consists of m genes. Formally, GEN_INITIAL_POP takes as input P , and outputs n encrypted chromosomes denoted by \\n{ x 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , x n },\\n{ f (x 1 ) , ‚Ä¢ ‚Ä¢ ‚Ä¢ , f (x n ) }.\\nAlso, EVALUATION outputs the optimal chromosome holding minimum fitness value. To this end, we carefully design a secure comparison protocol (SecCmp) that can compare f (x i ) and f (x j ) (i, j ‚àà [1, n] and i = j). Formally, given f (x i ) and f (x j ) , SecCmp outputs f (x i ) , where f \\n(x i ) ‚â§ f (x j ). Thus, given { f (x 1 ) , ‚Ä¢ ‚Ä¢ ‚Ä¢ , f (x n ) }, EVALUA- TION can output f (x k ) via SecCmp, where f (x k ) , s. t. f (x k ) = min{f (x 1 , ), ‚Ä¢ ‚Ä¢ ‚Ä¢ , f (x n )}',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Evolution as a Service: A Privacy-Preserving Genetic Algorithm for Combinatorial Optimization',\n",
       "  'abstract': \"Evolutionary algorithms (EAs), such as the genetic algorithm (GA), offer an elegant way to handle combinatorial optimization problems (COPs). However, limited by expertise and resources, most users do not have enough capability to implement EAs to solve COPs. An intuitive and promising solution is to outsource evolutionary operations to a cloud server, whilst it suffers from privacy concerns. To this end, this paper proposes a novel computing paradigm, evolution as a service (EaaS), where a cloud server renders evolutionary computation services for users without sacrificing users' privacy. Inspired by the idea of EaaS, this paper designs PEGA, a novel privacy-preserving GA for COPs. Specifically, PEGA enables users outsourcing COPs to the cloud server holding a competitive GA and approximating the optimal solution in a privacy-preserving manner. PEGA features the following characteristics. First, any user without expertise and enough resources can solve her COPs. Second, PEGA does not leak contents of optimization problems, i.e., users' privacy. Third, PEGA has the same capability as the conventional GA to approximate the optimal solution. We implements PEGA falling in a twin-server architecture and evaluates it in the traveling salesman problem (TSP, a widely known COP). Particularly, we utilize encryption cryptography to protect users' privacy and carefully design a suit of secure computing protocols to support evolutionary operators of GA on encrypted data. Privacy analysis demonstrates that PEGA does not disclose the contents of the COP to the cloud server. Experimental evaluation results on four TSP datasets show that PEGA is as effective as the conventional GA in approximating the optimal solution.\",\n",
       "  'paddleOCR': [[[[12.0, 7.0], [89.0, 15.0], [86.0, 46.0], [9.0, 38.0]],\n",
       "    ['Gene', 0.9995912313461304]],\n",
       "   [[[674.0, 23.0], [763.0, 23.0], [763.0, 60.0], [674.0, 60.0]],\n",
       "    ['Start', 0.9861580729484558]],\n",
       "   [[[976.0, 29.0], [1082.0, 29.0], [1082.0, 58.0], [976.0, 58.0]],\n",
       "    ['Parent 1', 0.9994198679924011]],\n",
       "   [[[1112.0, 25.0], [1224.0, 25.0], [1224.0, 60.0], [1112.0, 60.0]],\n",
       "    ['Parent 2', 0.9932887554168701]],\n",
       "   [[[1267.0, 22.0], [1377.0, 27.0], [1376.0, 58.0], [1266.0, 54.0]],\n",
       "    ['Children', 0.9994909763336182]],\n",
       "   [[[1503.0, 44.0], [1526.0, 44.0], [1526.0, 81.0], [1503.0, 81.0]],\n",
       "    ['T', 0.7052232623100281]],\n",
       "   [[[1793.0, 44.0], [1817.0, 44.0], [1817.0, 77.0], [1793.0, 77.0]],\n",
       "    ['1', 0.9998341798782349]],\n",
       "   [[[13.0, 78.0], [55.0, 80.0], [46.0, 277.0], [3.0, 274.0]],\n",
       "    ['Chromosome', 0.9997777938842773]],\n",
       "   [[[112.0, 77.0], [130.0, 77.0], [130.0, 104.0], [112.0, 104.0]],\n",
       "    ['1', 0.9003006219863892]],\n",
       "   [[[187.0, 71.0], [211.0, 71.0], [211.0, 108.0], [187.0, 108.0]],\n",
       "    ['2', 0.9714246988296509]],\n",
       "   [[[349.0, 79.0], [379.0, 79.0], [379.0, 106.0], [349.0, 106.0]],\n",
       "    ['m', 0.9990386962890625]],\n",
       "   [[[693.0, 74.0], [750.0, 80.0], [746.0, 117.0], [690.0, 111.0]],\n",
       "    ['[p1]', 0.824289083480835]],\n",
       "   [[[408.0, 85.0], [419.0, 79.0], [425.0, 92.0], [414.0, 97.0]],\n",
       "    ['7', 0.8708301186561584]],\n",
       "   [[[602.0, 89.0], [654.0, 89.0], [654.0, 120.0], [602.0, 120.0]],\n",
       "    ['[pn', 0.947780430316925]],\n",
       "   [[[1020.0, 89.0], [1044.0, 89.0], [1044.0, 124.0], [1020.0, 124.0]],\n",
       "    ['1', 0.9998088479042053]],\n",
       "   [[[1164.0, 91.0], [1184.0, 91.0], [1184.0, 122.0], [1164.0, 122.0]],\n",
       "    ['7', 0.8759877681732178]],\n",
       "   [[[1313.0, 89.0], [1337.0, 89.0], [1337.0, 124.0], [1313.0, 124.0]],\n",
       "    ['1', 0.9998372793197632]],\n",
       "   [[[1503.0, 104.0], [1528.0, 104.0], [1528.0, 145.0], [1503.0, 145.0]],\n",
       "    ['2', 0.9683274626731873]],\n",
       "   [[[1656.0, 108.0], [1674.0, 108.0], [1674.0, 135.0], [1656.0, 135.0]],\n",
       "    ['2', 0.9897878170013428]],\n",
       "   [[[1793.0, 104.0], [1817.0, 104.0], [1817.0, 137.0], [1793.0, 137.0]],\n",
       "    ['7', 0.9997256398200989]],\n",
       "   [[[765.0, 135.0], [817.0, 135.0], [817.0, 168.0], [765.0, 168.0]],\n",
       "    ['[p2]', 0.8769963979721069]],\n",
       "   [[[1018.0, 149.0], [1044.0, 149.0], [1044.0, 189.0], [1018.0, 189.0]],\n",
       "    ['2', 0.9715025424957275]],\n",
       "   [[[1162.0, 151.0], [1186.0, 151.0], [1186.0, 185.0], [1162.0, 185.0]],\n",
       "    ['6', 0.9986408352851868]],\n",
       "   [[[1313.0, 149.0], [1337.0, 149.0], [1337.0, 185.0], [1313.0, 185.0]],\n",
       "    ['2', 0.9770874977111816]],\n",
       "   [[[112.0, 170.0], [130.0, 170.0], [130.0, 195.0], [112.0, 195.0]],\n",
       "    ['1', 0.999933123588562]],\n",
       "   [[[189.0, 164.0], [213.0, 164.0], [213.0, 197.0], [189.0, 197.0]],\n",
       "    ['2', 0.9995427131652832]],\n",
       "   [[[349.0, 164.0], [373.0, 164.0], [373.0, 197.0], [349.0, 197.0]],\n",
       "    ['3', 0.9998396635055542]],\n",
       "   [[[407.0, 172.0], [426.0, 172.0], [426.0, 197.0], [407.0, 197.0]],\n",
       "    ['2', 0.9996949434280396]],\n",
       "   [[[897.0, 172.0], [964.0, 172.0], [964.0, 222.0], [897.0, 222.0]],\n",
       "    ['4', 0.7760007977485657]],\n",
       "   [[[1503.0, 166.0], [1530.0, 166.0], [1530.0, 203.0], [1503.0, 203.0]],\n",
       "    ['6', 0.9988445043563843]],\n",
       "   [[[781.0, 176.0], [835.0, 176.0], [835.0, 209.0], [781.0, 209.0]],\n",
       "    ['[p3]', 0.9460236430168152]],\n",
       "   [[[1795.0, 168.0], [1819.0, 168.0], [1819.0, 201.0], [1795.0, 201.0]],\n",
       "    ['6', 0.9989546537399292]],\n",
       "   [[[550.0, 191.0], [608.0, 191.0], [608.0, 224.0], [550.0, 224.0]],\n",
       "    ['[p8]', 0.7940455675125122]],\n",
       "   [[[771.0, 226.0], [831.0, 226.0], [831.0, 266.0], [771.0, 266.0]],\n",
       "    ['[p4]', 0.9840106964111328]],\n",
       "   [[[1016.0, 220.0], [1046.0, 220.0], [1046.0, 247.0], [1016.0, 247.0]],\n",
       "    ['...', 0.9033405184745789]],\n",
       "   [[[1154.0, 220.0], [1190.0, 220.0], [1190.0, 245.0], [1154.0, 245.0]],\n",
       "    ['...', 0.9030837416648865]],\n",
       "   [[[1499.0, 234.0], [1534.0, 234.0], [1534.0, 257.0], [1499.0, 257.0]],\n",
       "    ['...', 0.9107103943824768]],\n",
       "   [[[564.0, 255.0], [622.0, 255.0], [622.0, 288.0], [564.0, 288.0]],\n",
       "    ['[p7', 0.8689903616905212]],\n",
       "   [[[1020.0, 274.0], [1044.0, 274.0], [1044.0, 311.0], [1020.0, 311.0]],\n",
       "    ['3', 0.9866220355033875]],\n",
       "   [[[1162.0, 274.0], [1186.0, 274.0], [1186.0, 309.0], [1162.0, 309.0]],\n",
       "    ['5', 0.9998069405555725]],\n",
       "   [[[112.0, 284.0], [130.0, 284.0], [130.0, 311.0], [112.0, 311.0]],\n",
       "    ['7', 0.914680004119873]],\n",
       "   [[[187.0, 284.0], [213.0, 284.0], [213.0, 317.0], [187.0, 317.0]],\n",
       "    ['6', 0.9995580315589905]],\n",
       "   [[[347.0, 284.0], [373.0, 284.0], [373.0, 317.0], [347.0, 317.0]],\n",
       "    ['5', 0.9998268485069275]],\n",
       "   [[[1315.0, 276.0], [1339.0, 276.0], [1339.0, 307.0], [1315.0, 307.0]],\n",
       "    ['4', 0.999875545501709]],\n",
       "   [[[405.0, 297.0], [424.0, 297.0], [424.0, 315.0], [405.0, 315.0]],\n",
       "    ['n', 0.9934506416320801]],\n",
       "   [[[612.0, 297.0], [666.0, 297.0], [666.0, 330.0], [612.0, 330.0]],\n",
       "    ['[p6]', 0.8724572062492371]],\n",
       "   [[[711.0, 295.0], [765.0, 295.0], [765.0, 326.0], [711.0, 326.0]],\n",
       "    ['[Sd]', 0.8236539959907532]],\n",
       "   [[[1387.0, 297.0], [1399.0, 297.0], [1399.0, 315.0], [1387.0, 315.0]],\n",
       "    ['-', 0.6880812048912048]],\n",
       "   [[[1503.0, 288.0], [1530.0, 288.0], [1530.0, 324.0], [1503.0, 324.0]],\n",
       "    ['4', 0.9998072981834412]],\n",
       "   [[[1793.0, 290.0], [1819.0, 290.0], [1819.0, 322.0], [1793.0, 322.0]],\n",
       "    ['4', 0.9998611211776733]],\n",
       "   [[[1385.0, 315.0], [1399.0, 315.0], [1399.0, 340.0], [1385.0, 340.0]],\n",
       "    ['-', 0.6207478046417236]],\n",
       "   [[[957.0, 332.0], [968.0, 332.0], [968.0, 346.0], [957.0, 346.0]],\n",
       "    ['1', 0.7517794966697693]],\n",
       "   [[[605.0, 348.0], [765.0, 353.0], [764.0, 388.0], [604.0, 383.0]],\n",
       "    ['SELECTION', 0.9956847429275513]],\n",
       "   [[[1091.0, 348.0], [1263.0, 353.0], [1262.0, 388.0], [1090.0, 383.0]],\n",
       "    ['CROSSOVER', 0.9944118857383728]],\n",
       "   [[[94.0, 353.0], [343.0, 353.0], [343.0, 382.0], [94.0, 382.0]],\n",
       "    ['GeN INITIAL POp', 0.790384829044342]],\n",
       "   [[[1574.0, 350.0], [1727.0, 355.0], [1726.0, 384.0], [1573.0, 379.0]],\n",
       "    ['MUTATION', 0.991000771522522]],\n",
       "   [[[271.0, 460.0], [295.0, 460.0], [295.0, 494.0], [271.0, 494.0]],\n",
       "    ['9', 0.9938594698905945]],\n",
       "   [[[346.0, 455.0], [376.0, 451.0], [383.0, 495.0], [352.0, 499.0]],\n",
       "    ['5', 0.9997445940971375]],\n",
       "   [[[44.0, 465.0], [233.0, 465.0], [233.0, 494.0], [44.0, 494.0]],\n",
       "    ['Chromosome k', 0.9997908473014832]],\n",
       "   [[[422.0, 469.0], [448.0, 469.0], [448.0, 485.0], [422.0, 485.0]],\n",
       "    ['...', 0.6163724660873413]],\n",
       "   [[[510.0, 460.0], [530.0, 460.0], [530.0, 492.0], [510.0, 492.0]],\n",
       "    ['7', 0.9440615177154541]],\n",
       "   [[[606.0, 460.0], [789.0, 460.0], [789.0, 496.0], [606.0, 496.0]],\n",
       "    ['EVALUATION', 0.997122585773468]],\n",
       "   [[[1387.0, 465.0], [1433.0, 465.0], [1433.0, 487.0], [1387.0, 487.0]],\n",
       "    ['X t', 0.9773889183998108]]]},\n",
       " '2012.09727v2-Figure1-1.png': {'ocr': [[[1416.0, 346.0],\n",
       "    [1512.0, 350.0],\n",
       "    [1511.0, 372.0],\n",
       "    [1415.0, 368.0]],\n",
       "   ['Adaptation', 0.9963287115097046]],\n",
       "  'True_Statements': ['The speaker inventory construction module takes an input stream in as input.',\n",
       "   'Profile Selection takes input from mixture embedding and speaker inventory.'],\n",
       "  'False_Statements': ['The speaker inventory construction module outputs an input stream.',\n",
       "   'Profile Selection sends input to mixture embedding and speaker inventory.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording.',\n",
       "  'caption': 'Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2012.09727v2-Figure1-1.png',\n",
       "  'sections': [{'heading': 'CONTINUOUS SSUSI USING SELF-INFORMED MECHANISM FOR INVENTORY CONSTRUCTION',\n",
       "    'text': 'SSUSI assumes that pre-recorded utterances of all speakers are available for the speaker inventory construction. However, such an assumption may not be realistic, especially for unseen speakers or meeting scenarios where the collection of pre-recorded speech from the participants is not feasible.\\nContinuous speech separation (CSS) aims at estimating the individual target signals from a continuous mixed signal which is usually a hours long signal and contains both overlapped and non-overlap speech, but the overlap ratio is low. So, single-speaker regions can be exploited to derive robust acoustic characteristics of participating speakers without the need for external utterances, which makes the self-informed speaker inventory construction possible. This section introduces how we adopt SSUSI in the CSS task and eliminate the need for pre-recorded speech by using a clustering method.  The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles e p 1 and e p 2 . (C) Stitching procedure of adjacent segment outputs in a long recording. performance is insensitive to the choice of M as long as M is no smaller than the actual number of active speakers in the recording.\\nCSUSSI uniformly segments the mixture recording and exploits the inventory to facilitate source separation in each segment. Except for the self-informed speaker inventory, CSSUSI uses the same speaker profile selection and biased speech separation methods as introduced in Section 2.2 and Section 2.3, respectively. To stitch the outputs from different segments to form output streams where each stream only contains non-overlapped speakers, the similarity between the overlapped regions in adjacent blocks determines the pair of segments to be stitched. Figure 1 (C) shows the stitching procedure of adjacent segment outputs.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'CONTINUOUS SPEECH SEPARATION USING SPEAKER INVENTORY FOR LONG MULTI-TALKER RECORDING',\n",
       "  'abstract': \"Leveraging additional speaker information to facilitate speech separation has received increasing attention in recent years. Recent research includes extracting target speech by using the target speaker's voice snippet and jointly separating all participating speakers by using a pool of additional speaker signals, which is known as speech separation using speaker inventory (SSUSI). However, all these systems ideally assume that the pre-enrolled speaker signals are available and are only evaluated on simple data configurations. In realistic multi-talker conversations, the speech signal contains a large proportion of non-overlapped regions, where we can derive robust speaker embedding of individual talkers. In this work, we adopt the SSUSI model in long recordings and propose a self-informed, clustering-based inventory forming scheme for long recording, where the speaker inventory is fully built from the input signal without the need for external speaker signals. Experiment results on simulated noisy reverberant long recording datasets show that the proposed method can significantly improve the separation performance across various conditions.\",\n",
       "  'paddleOCR': [[[[376.0, 31.0], [448.0, 36.0], [446.0, 64.0], [374.0, 59.0]],\n",
       "    ['Stitching', 0.999190628528595]],\n",
       "   [[[1282.0, 45.0], [1375.0, 45.0], [1375.0, 67.0], [1282.0, 67.0]],\n",
       "    ['Separation', 0.9998971223831177]],\n",
       "   [[[1460.0, 39.0], [1541.0, 44.0], [1539.0, 70.0], [1459.0, 65.0]],\n",
       "    ['Stitching', 0.999582052230835]],\n",
       "   [[[1621.0, 45.0], [1709.0, 45.0], [1709.0, 67.0], [1621.0, 67.0]],\n",
       "    ['Streaming', 0.9999315738677979]],\n",
       "   [[[1074.0, 113.0], [1163.0, 113.0], [1163.0, 133.0], [1074.0, 133.0]],\n",
       "    ['Segment i', 0.9723577499389648]],\n",
       "   [[[79.0, 148.0], [155.0, 148.0], [155.0, 170.0], [79.0, 170.0]],\n",
       "    ['Speech', 0.9425051808357239]],\n",
       "   [[[655.0, 140.0], [836.0, 140.0], [836.0, 166.0], [655.0, 166.0]],\n",
       "    ['Output streams', 0.9965068697929382]],\n",
       "   [[[1147.0, 140.0], [1254.0, 140.0], [1254.0, 162.0], [1147.0, 162.0]],\n",
       "    ['Segment i+1', 0.9934895038604736]],\n",
       "   [[[1387.0, 142.0], [1471.0, 142.0], [1471.0, 162.0], [1387.0, 162.0]],\n",
       "    ['Segment i', 0.999517023563385]],\n",
       "   [[[1526.0, 140.0], [1631.0, 140.0], [1631.0, 160.0], [1526.0, 160.0]],\n",
       "    ['Segment i+1', 0.9998376965522766]],\n",
       "   [[[246.0, 156.0], [316.0, 156.0], [316.0, 184.0], [246.0, 184.0]],\n",
       "    ['iSTFT', 0.9977590441703796]],\n",
       "   [[[79.0, 176.0], [187.0, 176.0], [187.0, 198.0], [79.0, 198.0]],\n",
       "    ['separation', 0.977871298789978]],\n",
       "   [[[1471.0, 172.0], [1526.0, 172.0], [1526.0, 212.0], [1471.0, 212.0]],\n",
       "    ['(B)', 0.9676856398582458]],\n",
       "   [[[79.0, 202.0], [161.0, 202.0], [161.0, 223.0], [79.0, 223.0]],\n",
       "    [' module', 0.9741992950439453]],\n",
       "   [[[260.0, 214.0], [292.0, 214.0], [292.0, 239.0], [260.0, 239.0]],\n",
       "    ['0', 0.579361617565155]],\n",
       "   [[[300.0, 231.0], [353.0, 231.0], [353.0, 253.0], [300.0, 253.0]],\n",
       "    ['masks', 0.9997060894966125]],\n",
       "   [[[1454.0, 255.0], [1563.0, 259.0], [1561.0, 287.0], [1453.0, 282.0]],\n",
       "    [' Separation', 0.9882089495658875]],\n",
       "   [[[226.0, 267.0], [329.0, 267.0], [329.0, 289.0], [226.0, 289.0]],\n",
       "    [' Separation', 0.9683386087417603]],\n",
       "   [[[498.0, 271.0], [812.0, 271.0], [812.0, 297.0], [498.0, 297.0]],\n",
       "    ['! Speaker profile selection module', 0.9839155673980713]],\n",
       "   [[[254.0, 295.0], [302.0, 295.0], [302.0, 316.0], [254.0, 316.0]],\n",
       "    ['layer', 0.9927743077278137]],\n",
       "   [[[1486.0, 287.0], [1536.0, 287.0], [1536.0, 310.0], [1486.0, 310.0]],\n",
       "    ['layer', 0.9924201965332031]],\n",
       "   [[[272.0, 320.0], [286.0, 320.0], [286.0, 334.0], [272.0, 334.0]],\n",
       "    ['4', 0.5556814074516296]],\n",
       "   [[[226.0, 344.0], [333.0, 344.0], [333.0, 366.0], [226.0, 366.0]],\n",
       "    ['Adaptation', 0.9999090433120728]],\n",
       "   [[[365.0, 340.0], [498.0, 340.0], [498.0, 362.0], [365.0, 362.0]],\n",
       "    ['Selected profiles', 0.999682605266571]],\n",
       "   [[[722.0, 352.0], [907.0, 356.0], [907.0, 384.0], [721.0, 380.0]],\n",
       "    ['Profile selection', 0.990641176700592]],\n",
       "   [[[1416.0, 346.0], [1512.0, 350.0], [1511.0, 372.0], [1415.0, 368.0]],\n",
       "    ['Adaptation', 0.9963287115097046]],\n",
       "   [[[254.0, 368.0], [302.0, 368.0], [302.0, 392.0], [254.0, 392.0]],\n",
       "    ['layer', 0.9916360974311829]],\n",
       "   [[[1447.0, 372.0], [1490.0, 372.0], [1490.0, 394.0], [1447.0, 394.0]],\n",
       "    ['layer', 0.9048368334770203]],\n",
       "   [[[615.0, 411.0], [631.0, 411.0], [631.0, 445.0], [615.0, 445.0]],\n",
       "    ['4', 0.5009039640426636]],\n",
       "   [[[242.0, 421.0], [314.0, 421.0], [314.0, 443.0], [242.0, 443.0]],\n",
       "    ['Feature', 0.9995357394218445]],\n",
       "   [[[1473.0, 433.0], [1542.0, 433.0], [1542.0, 455.0], [1473.0, 455.0]],\n",
       "    ['concat.', 0.9999244809150696]],\n",
       "   [[[254.0, 445.0], [302.0, 445.0], [302.0, 469.0], [254.0, 469.0]],\n",
       "    ['layer', 0.9912155270576477]],\n",
       "   [[[573.0, 454.0], [670.0, 459.0], [668.0, 487.0], [572.0, 482.0]],\n",
       "    ['Mixture', 0.9994568228721619]],\n",
       "   [[[959.0, 459.0], [1056.0, 459.0], [1056.0, 487.0], [959.0, 487.0]],\n",
       "    ['Speaker', 0.9999300837516785]],\n",
       "   [[[556.0, 492.0], [687.0, 492.0], [687.0, 520.0], [556.0, 520.0]],\n",
       "    ['embedding', 0.9998301267623901]],\n",
       "   [[[950.0, 488.0], [1065.0, 493.0], [1063.0, 521.0], [949.0, 516.0]],\n",
       "    ['inventory', 0.9996296763420105]],\n",
       "   [[[246.0, 512.0], [310.0, 512.0], [310.0, 540.0], [246.0, 540.0]],\n",
       "    ['STFT', 0.9955532550811768]],\n",
       "   [[[591.0, 582.0], [654.0, 591.0], [650.0, 620.0], [587.0, 612.0]],\n",
       "    ['SNet', 0.9989174604415894]],\n",
       "   [[[1548.0, 599.0], [1572.0, 599.0], [1572.0, 617.0], [1548.0, 617.0]],\n",
       "    ['.', 0.793624222278595]],\n",
       "   [[[216.0, 652.0], [333.0, 657.0], [332.0, 685.0], [215.0, 680.0]],\n",
       "    ['Segments', 0.9998643398284912]],\n",
       "   [[[510.0, 674.0], [873.0, 674.0], [873.0, 700.0], [510.0, 700.0]],\n",
       "    ['Speaker inventory construction module', 0.9898386597633362]],\n",
       "   [[[753.0, 731.0], [887.0, 736.0], [886.0, 766.0], [751.0, 761.0]],\n",
       "    ['Embedding', 0.9998355507850647]],\n",
       "   [[[593.0, 752.0], [655.0, 752.0], [655.0, 781.0], [593.0, 781.0]],\n",
       "    ['SNet', 0.9991708993911743]],\n",
       "   [[[959.0, 753.0], [1056.0, 753.0], [1056.0, 781.0], [959.0, 781.0]],\n",
       "    [' Kmeans', 0.9319196343421936]],\n",
       "   [[[1473.0, 755.0], [1544.0, 760.0], [1543.0, 784.0], [1471.0, 779.0]],\n",
       "    ['Feature', 0.9994067549705505]],\n",
       "   [[[764.0, 773.0], [877.0, 773.0], [877.0, 795.0], [764.0, 795.0]],\n",
       "    ['sequence', 0.9990643262863159]],\n",
       "   [[[1484.0, 785.0], [1536.0, 785.0], [1536.0, 809.0], [1484.0, 809.0]],\n",
       "    ['layer', 0.9870215654373169]],\n",
       "   [[[202.0, 807.0], [349.0, 807.0], [349.0, 827.0], [202.0, 827.0]],\n",
       "    ['Input stream', 0.9684414863586426]],\n",
       "   [[[589.0, 839.0], [645.0, 839.0], [645.0, 878.0], [589.0, 878.0]],\n",
       "    ['(A)', 0.983386218547821]],\n",
       "   [[[1490.0, 837.0], [1542.0, 837.0], [1542.0, 878.0], [1490.0, 878.0]],\n",
       "    ['(c)', 0.9021182060241699]]]},\n",
       " '407011-Figure3-1.png': {'ocr': [[[939.0, 233.0],\n",
       "    [954.0, 233.0],\n",
       "    [954.0, 252.0],\n",
       "    [939.0, 252.0]],\n",
       "   ['F', 0.9972977042198181]],\n",
       "  'True_Statements': ['Convolution layers are used on the Positive and Negative set.',\n",
       "   'Group ID and Category Label are inputs for the Group Sensitive Structure.'],\n",
       "  'False_Statements': ['Convolution layers are used only on the Positive set.',\n",
       "   'Group ID and Category Label are outputs of the Group Sensitive Structure.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 3. Illustration of a triplet network by incorporating intra-class variance into triplet embedding, in which the joint learning objective is to minimize the combination of softmax loss and triplet loss (consisting of inter-class and intra-class triplet loss).',\n",
       "  'caption': 'Fig. 3. Illustration of a triplet network by incorporating intra-class variance into triplet embedding, in which the joint learning objective is to minimize the combination of softmax loss and triplet loss (consisting of inter-class and intra-class triplet loss).',\n",
       "  'imageText': [],\n",
       "  'image_file': '407011-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Joint Optimization of Multiple Loss Function',\n",
       "    'text': 'ICV triplet loss alone does not suffice for effective and efficient feature learning in triplet network. Firstly, given a dataset of N images, the number of triplet units is O(N 3 ), while each iteration in training often selects dozens of triplet units, and only a minority may violate the constraints. So the solely ICV triplet loss based learning incurs much slower convergence than classification. Secondly, as the triplet loss works on similarity distance learning rather than hyperplane decision, the discriminative ability of features can be improved by adding the classification loss to the learning objective. Hence, we propose a GS-TRS loss to jointly optimize the ICV triplet combinatin loss and softmax loss in a multitask learning manner. A simple linear weighting is applied to construct the final loss function as follows:\\nL GS‚àíT RS = œâL sof tmax + (1 ‚àí œâ)L ICV trplet ,(12)\\nwhere œâ is fusion weight. Fig. 3 illustrates the triplet network. Optimizing this multi-loss function helps accomplish promising fine-grained categorization performance as well as discriminative features for fine-grained retrieval. We will investigate the effects of ICV triplet loss with or without meanvalued anchor on GS-TRS loss in the experiments.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': '',\n",
       "  'abstract': 'Fine-grained visual recognition aims to capture discriminative characteristics amongst visually similar categories. The state-of-the-art research work has significantly improved the fine-grained recognition performance by deep metric learning using triplet network. However, the impact of intra-category variance on the performance of recognition and robust feature representation has not been well studied. In this paper, we propose to leverage intra-class variance in metric learning of triplet network to improve the performance of fine-grained recognition. Through partitioning training images within each category into a few groups, we form the triplet samples across different categories as well as different groups, which is called Group Sensitive TRiplet Sampling (GS-TRS). Accordingly, the triplet loss function is strengthened by incorporating intra-class variance with GS-TRS, which may contribute to the optimization objective of triplet network. Extensive experiments over benchmark datasets CompCar and VehicleID show that the proposed GS-TRS has significantly outperformed state-of-the-art approaches in both classification and retrieval tasks.',\n",
       "  'paddleOCR': [[[[86.0, 12.0], [177.0, 18.0], [176.0, 42.0], [84.0, 36.0]],\n",
       "    ['Category', 0.9998186826705933]],\n",
       "   [[[105.0, 43.0], [162.0, 43.0], [162.0, 62.0], [105.0, 62.0]],\n",
       "    ['Label', 0.9985993504524231]],\n",
       "   [[[920.0, 104.0], [1055.0, 104.0], [1055.0, 128.0], [920.0, 128.0]],\n",
       "    ['4096 1024', 0.9730604887008667]],\n",
       "   [[[1216.0, 100.0], [1365.0, 102.0], [1365.0, 126.0], [1215.0, 124.0]],\n",
       "    ['SoftMax Loss', 0.9997441172599792]],\n",
       "   [[[49.0, 164.0], [157.0, 164.0], [157.0, 181.0], [49.0, 181.0]],\n",
       "    ['Positive Set', 0.9815304279327393]],\n",
       "   [[[185.0, 158.0], [321.0, 164.0], [320.0, 192.0], [183.0, 186.0]],\n",
       "    [\"INegative Set'\", 0.9451627135276794]],\n",
       "   [[[1282.0, 193.0], [1592.0, 193.0], [1592.0, 217.0], [1282.0, 217.0]],\n",
       "    ['Group Sensitive Structure', 0.999898374080658]],\n",
       "   [[[939.0, 233.0], [954.0, 233.0], [954.0, 252.0], [939.0, 252.0]],\n",
       "    ['F', 0.9972977042198181]],\n",
       "   [[[1026.0, 231.0], [1040.0, 231.0], [1040.0, 255.0], [1026.0, 255.0]],\n",
       "    ['F', 0.5362445712089539]],\n",
       "   [[[1665.0, 239.0], [1694.0, 233.0], [1698.0, 254.0], [1669.0, 260.0]],\n",
       "    ['rn', 0.8582788109779358]],\n",
       "   [[[939.0, 262.0], [954.0, 262.0], [954.0, 283.0], [939.0, 283.0]],\n",
       "    ['C', 0.702346920967102]],\n",
       "   [[[1369.0, 260.0], [1477.0, 260.0], [1477.0, 279.0], [1369.0, 279.0]],\n",
       "    ['Inter-class', 0.9999017119407654]],\n",
       "   [[[1360.0, 290.0], [1488.0, 290.0], [1488.0, 314.0], [1360.0, 314.0]],\n",
       "    ['Triplet Loss', 0.9999435544013977]],\n",
       "   [[[941.0, 300.0], [954.0, 300.0], [954.0, 312.0], [941.0, 312.0]],\n",
       "    ['6', 0.7066113352775574]],\n",
       "   [[[1203.0, 300.0], [1233.0, 300.0], [1233.0, 321.0], [1203.0, 321.0]],\n",
       "    ['L2', 0.9992234706878662]],\n",
       "   [[[1192.0, 329.0], [1242.0, 329.0], [1242.0, 348.0], [1192.0, 348.0]],\n",
       "    ['norm', 0.9988072514533997]],\n",
       "   [[[546.0, 359.0], [692.0, 359.0], [692.0, 381.0], [546.0, 381.0]],\n",
       "    ['Convolutional', 0.9997857213020325]],\n",
       "   [[[1367.0, 364.0], [1475.0, 364.0], [1475.0, 383.0], [1367.0, 383.0]],\n",
       "    ['Intra-class', 0.9998575448989868]],\n",
       "   [[[586.0, 386.0], [654.0, 392.0], [651.0, 418.0], [583.0, 411.0]],\n",
       "    ['layers', 0.9805703163146973]],\n",
       "   [[[1360.0, 392.0], [1486.0, 392.0], [1486.0, 416.0], [1360.0, 416.0]],\n",
       "    ['Triplet Loss', 0.9963631629943848]],\n",
       "   [[[1625.0, 396.0], [1655.0, 401.0], [1652.0, 418.0], [1622.0, 413.0]],\n",
       "    ['p,g', 0.7388848662376404]],\n",
       "   [[[99.0, 482.0], [167.0, 489.0], [164.0, 513.0], [97.0, 506.0]],\n",
       "    ['Group', 0.9998172521591187]],\n",
       "   [[[117.0, 512.0], [148.0, 512.0], [148.0, 533.0], [117.0, 533.0]],\n",
       "    ['ID', 0.988982081413269]]]},\n",
       " '2204.07810v1-Figure3-1.png': {'ocr': [[[829.0, 271.0],\n",
       "    [881.0, 271.0],\n",
       "    [881.0, 296.0],\n",
       "    [829.0, 296.0]],\n",
       "   ['ML', 0.9911338090896606]],\n",
       "  'True_Statements': ['Final mean flow is higher than final stress.',\n",
       "   'The initial mean flow is higher than the initial turb flow.'],\n",
       "  'False_Statements': ['Final mean flow is lower than final stress.',\n",
       "   'The initial mean flow is lower than the initial turb flow.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 3 CFD solution process of the iterative framework',\n",
       "  'caption': 'Fig. 3 CFD solution process of the iterative framework',\n",
       "  'imageText': [],\n",
       "  'image_file': '2204.07810v1-Figure3-1.png',\n",
       "  'sections': [{'heading': 'B. Effect of the ML model calling frequency',\n",
       "    'text': 'As described in Fig. 3 and Fig. 14, the original iterative flow chart needs to call the ML model and update the Reynolds stress in each iteration step. This will increase the time cost significantly compared with the baseline RANS computation. Still taking the Œ± = 1.0 case as an example, the grid number of the case is 77 in the normal direction and 89 in the streamwise direction, for a total 6853 of points. The time costs of the direct SST computation and the iterative coupling calculation are listed in Table 2. It can be observed that the iteration cost increases considerably, but the time cost relative to unsteady simulations such as LES or DNS is still acceptable.\\nA natural idea is to execute the ML model after an interval of several steps. To verify the effect, we modify the program and test four intervals: executing the ML model every 1, 3, 5, and 10 steps.\\nThe time costs are also listed in Table 2. The results show that the computation will diverge if the interval is too large. The ratio of total time cost between different intervals is essentially the same as the ratio of the interval steps, which indicates that the data transfer and Python computation occupy the main proportion of the time cost. The 3 converged mean flow results are generally the same, but the per 5 step case shows vibration near the periodic hill top, as shown in Fig. 21.\\nIn summary, executing the ML model after an interval can accelerate the computation. However, the smoothness and the convergence will be affected if the interval step is too large.  ',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'An iterative data-driven turbulence modeling framework based on Reynolds stress representation Yuhui Yin(Â∞πÂÆáËæâ), 1 Yufei Zhang(Âº†ÂÆáÈ£û), 1, a) Haixin Chen(ÈôàÊµ∑Êòï), 1 and Song',\n",
       "  'abstract': 'Data-driven turbulence modeling studies have reached such a stage that the fundamental framework is basically settled, but several essential issues remain that strongly affect the performance, including accuracy, smoothness, and generalization capacity. Two problems are studied in the current research: (1) the processing of the Reynolds stress tensor and (2) the coupling method between the machine learning turbulence model and CFD solver. The first determines the form of predicting targets and the resulting physical completeness and interpretability. The second determines the training process and intrinsic relevance between the mean flow features and Reynolds stress. For the Reynolds stress processing issue, we perform the theoretical derivation to extend the relevant tensor arguments of Reynolds stress in addition to the strain rate and rotation rate. Then, the tensor representation theorem is employed to give the complete irreducible invariants and integrity basis. In addition, an adaptive regularization term is employed to enhance the representation performance. For the CFD coupling issue, an iterative coupling data-driven',\n",
       "  'paddleOCR': [[[[1224.0, 20.0],\n",
       "     [1463.0, 20.0],\n",
       "     [1463.0, 51.0],\n",
       "     [1224.0, 51.0]],\n",
       "    ['Final mean flow', 0.9997134804725647]],\n",
       "   [[[539.0, 31.0], [704.0, 31.0], [704.0, 64.0], [539.0, 64.0]],\n",
       "    ['Converged', 0.9996106624603271]],\n",
       "   [[[1245.0, 62.0], [1446.0, 62.0], [1446.0, 98.0], [1245.0, 98.0]],\n",
       "    ['{p, u, v,w,p}', 0.8907875418663025]],\n",
       "   [[[543.0, 76.0], [699.0, 76.0], [699.0, 102.0], [543.0, 102.0]],\n",
       "    ['mean flow', 0.9670043587684631]],\n",
       "   [[[523.0, 114.0], [718.0, 114.0], [718.0, 151.0], [523.0, 151.0]],\n",
       "    ['{p,u,v, w,p}', 0.8894341588020325]],\n",
       "   [[[1219.0, 154.0], [1449.0, 154.0], [1449.0, 185.0], [1219.0, 185.0]],\n",
       "    ['Final stress {t}', 0.9860246777534485]],\n",
       "   [[[548.0, 267.0], [711.0, 267.0], [711.0, 300.0], [548.0, 300.0]],\n",
       "    ['Converged', 0.9996901750564575]],\n",
       "   [[[1225.0, 258.0], [1458.0, 258.0], [1458.0, 283.0], [1225.0, 283.0]],\n",
       "    ['Final turb.flow', 0.9769731760025024]],\n",
       "   [[[31.0, 278.0], [191.0, 278.0], [191.0, 303.0], [31.0, 303.0]],\n",
       "    ['Mean flow', 0.9770618081092834]],\n",
       "   [[[829.0, 271.0], [881.0, 271.0], [881.0, 296.0], [829.0, 296.0]],\n",
       "    ['ML', 0.9911338090896606]],\n",
       "   [[[550.0, 301.0], [706.0, 305.0], [705.0, 343.0], [549.0, 339.0]],\n",
       "    ['turb. flow', 0.9996253848075867]],\n",
       "   [[[1300.0, 300.0], [1387.0, 300.0], [1387.0, 332.0], [1300.0, 332.0]],\n",
       "    ['{k,w}', 0.9456663131713867]],\n",
       "   [[[582.0, 349.0], [676.0, 349.0], [676.0, 387.0], [582.0, 387.0]],\n",
       "    ['{k,w}', 0.9217128753662109]],\n",
       "   [[[29.0, 434.0], [187.0, 434.0], [187.0, 465.0], [29.0, 465.0]],\n",
       "    ['Turb. flow', 0.9996207356452942]],\n",
       "   [[[390.0, 519.0], [620.0, 517.0], [621.0, 548.0], [390.0, 550.0]],\n",
       "    ['Baseline RANS', 0.9995396137237549]],\n",
       "   [[[1061.0, 516.0], [1394.0, 518.0], [1394.0, 548.0], [1061.0, 546.0]],\n",
       "    ['Bidirectional iteration', 0.9993757009506226]]]},\n",
       " '2107.03564v1-Figure2-1.png': {'ocr': [[[1073.0, 358.0],\n",
       "    [1233.0, 365.0],\n",
       "    [1232.0, 399.0],\n",
       "    [1072.0, 392.0]],\n",
       "   ['embedding', 0.9998342990875244]],\n",
       "  'True_Statements': ['The output is distance between s and i.',\n",
       "   'Proxy selection includes point-wise feed-forward network, mean and softmax.'],\n",
       "  'False_Statements': ['The input is distance between s and i.',\n",
       "   'Distance function includes point-wise feed-forward network, mean and softmax.'],\n",
       "  'Flowchart-to-Caption': 'Figure 2: The overall architecture of ProxySR.',\n",
       "  'caption': 'Figure 2: The overall architecture of ProxySR.',\n",
       "  'imageText': [':',\n",
       "   'PosiÔøΩonal',\n",
       "   'embedding',\n",
       "   'Self-aÔøΩenÔøΩon',\n",
       "   'Network‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   'Target',\n",
       "   'item',\n",
       "   'embedding',\n",
       "   'Item',\n",
       "   'embeddings',\n",
       "   '‚Ä¶',\n",
       "   'Session',\n",
       "   '‚ä•',\n",
       "   '‚ä•',\n",
       "   '‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   'SoÔøΩmax',\n",
       "   'with',\n",
       "   'temperature',\n",
       "   '0.2',\n",
       "   '2.7',\n",
       "   '‚Ä¶',\n",
       "   '5.3',\n",
       "   '-0.4',\n",
       "   'Mean',\n",
       "   'Point-wise',\n",
       "   'feed-forward',\n",
       "   'network',\n",
       "   '‚Ä¶',\n",
       "   'Short-term',\n",
       "   'interest',\n",
       "   'encoder',\n",
       "   'Proxy',\n",
       "   'SelecÔøΩon',\n",
       "   '(',\n",
       "   ',',\n",
       "   ')',\n",
       "   'ProjecÔøΩon',\n",
       "   '0',\n",
       "   '0.01',\n",
       "   '‚Ä¶',\n",
       "   '0.99',\n",
       "   '0',\n",
       "   'ProjecÔøΩon',\n",
       "   'Distance',\n",
       "   'funcÔøΩon',\n",
       "   'Item',\n",
       "   'embedding',\n",
       "   'set',\n",
       "   'Proxy',\n",
       "   'embedding',\n",
       "   'set',\n",
       "   'Unit',\n",
       "   'normal',\n",
       "   'vector',\n",
       "   'set'],\n",
       "  'image_file': '2107.03564v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'METHOD',\n",
       "    'text': 'This section first introduces the task of SRSs and the notation in this paper (Section 3.1), then describes the details of ProxySR (Fig. 2). ProxySR selects a proxy for the input session (Section 3.2) and encodes the session into a short-term interest representation (Section 3.3), and then uses the aggregation of them to define the distance function between the session and the target item (Section 3.4). Finally, the loss function for training ProxySR is proposed using the distance function (Section 3.5). Moreover, we establish another real-world scenario for SRSs, and propose a revised version of ProxySR for the scenario (Section 3.6).',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Unsupervised Proxy Selection for Session-based Recommender Systems',\n",
       "  'abstract': \"Session-based Recommender Systems (SRSs) have been actively developed to recommend the next item of an anonymous short item sequence (i.e., session). Unlike sequence-aware recommender systems where the whole interaction sequence of each user can be used to model both the short-term interest and the general interest of the user, the absence of user-dependent information in SRSs makes it difficult to directly derive the user's general interest from data. Therefore, existing SRSs have focused on how to effectively model the information about short-term interest within the sessions, but they are insufficient to capture the general interest of users. To this end, we propose a novel framework to overcome the limitation of SRSs, named ProxySR, which imitates the missing information in SRSs (i.e., general interest of users) by modeling proxies of sessions. ProxySR selects a proxy for the input session in an unsupervised manner, and combines it with the encoded short-term interest of the session. As a proxy is jointly learned with the short-term interest and selected by multiple sessions, a proxy learns to play the role of the general interest of a user and ProxySR learns how to select a suitable proxy for an input session. Moreover, we propose another real-world situation of SRSs where a few users are logged-in and leave their identifiers in sessions, and a revision of ProxySR for the situation. Our experiments on real-world datasets show that ProxySR considerably outperforms the state-of-the-art competitors, and the proxies successfully imitate the general interest of the users without any user-dependent information.\",\n",
       "  'paddleOCR': [[[[663.0, 3.0], [780.0, 3.0], [780.0, 29.0], [663.0, 29.0]],\n",
       "    ['dist(s,i)', 0.9981402158737183]],\n",
       "   [[[33.0, 47.0], [328.0, 47.0], [328.0, 68.0], [33.0, 68.0]],\n",
       "    [' : Positional embedding', 0.9822009801864624]],\n",
       "   [[[596.0, 70.0], [854.0, 70.0], [854.0, 101.0], [596.0, 101.0]],\n",
       "    ['Distance function', 0.9999067783355713]],\n",
       "   [[[830.0, 175.0], [958.0, 175.0], [958.0, 203.0], [830.0, 203.0]],\n",
       "    ['Projection', 0.999864935874939]],\n",
       "   [[[1136.0, 175.0], [1267.0, 175.0], [1267.0, 203.0], [1136.0, 203.0]],\n",
       "    ['Projection', 0.9998300671577454]],\n",
       "   [[[327.0, 226.0], [384.0, 207.0], [395.0, 236.0], [338.0, 256.0]],\n",
       "    ['p(s)', 0.9890902042388916]],\n",
       "   [[[610.0, 225.0], [630.0, 225.0], [630.0, 247.0], [610.0, 247.0]],\n",
       "    ['V', 0.550366997718811]],\n",
       "   [[[1261.0, 259.0], [1481.0, 259.0], [1481.0, 281.0], [1261.0, 281.0]],\n",
       "    ['Item embedding set', 0.9743931293487549]],\n",
       "   [[[560.0, 296.0], [691.0, 296.0], [691.0, 318.0], [560.0, 318.0]],\n",
       "    ['Unit normal', 0.9969802498817444]],\n",
       "   [[[247.0, 306.0], [487.0, 309.0], [487.0, 335.0], [246.0, 332.0]],\n",
       "    ['Proxy embedding set', 0.994579017162323]],\n",
       "   [[[853.0, 308.0], [905.0, 294.0], [914.0, 325.0], [861.0, 339.0]],\n",
       "    ['s(s)', 0.9441310167312622]],\n",
       "   [[[1351.0, 316.0], [1379.0, 316.0], [1379.0, 346.0], [1351.0, 346.0]],\n",
       "    ['Ii', 0.5846691131591797]],\n",
       "   [[[566.0, 328.0], [680.0, 324.0], [681.0, 346.0], [567.0, 349.0]],\n",
       "    ['vector set', 0.9996402859687805]],\n",
       "   [[[1074.0, 326.0], [1236.0, 329.0], [1235.0, 362.0], [1073.0, 358.0]],\n",
       "    ['Target item', 0.9652976989746094]],\n",
       "   [[[1073.0, 358.0], [1233.0, 365.0], [1232.0, 399.0], [1072.0, 392.0]],\n",
       "    ['embedding', 0.9998342990875244]],\n",
       "   [[[490.0, 396.0], [504.0, 396.0], [504.0, 408.0], [490.0, 408.0]],\n",
       "    ['0', 0.9702267050743103]],\n",
       "   [[[515.0, 391.0], [573.0, 391.0], [573.0, 413.0], [515.0, 413.0]],\n",
       "    ['0.01', 0.999855101108551]],\n",
       "   [[[627.0, 391.0], [683.0, 391.0], [683.0, 413.0], [627.0, 413.0]],\n",
       "    ['0.99', 0.9998953342437744]],\n",
       "   [[[20.0, 411.0], [326.0, 414.0], [326.0, 441.0], [20.0, 437.0]],\n",
       "    ['Softmax with temperature', 0.999912440776825]],\n",
       "   [[[715.0, 425.0], [732.0, 425.0], [732.0, 441.0], [715.0, 441.0]],\n",
       "    ['IC', 0.5581725835800171]],\n",
       "   [[[44.0, 484.0], [148.0, 484.0], [148.0, 506.0], [44.0, 506.0]],\n",
       "    ['0.22.7', 0.9990776181221008]],\n",
       "   [[[193.0, 481.0], [303.0, 481.0], [303.0, 509.0], [193.0, 509.0]],\n",
       "    ['5.3-0.4', 0.9124813079833984]],\n",
       "   [[[811.0, 493.0], [972.0, 493.0], [972.0, 524.0], [811.0, 524.0]],\n",
       "    ['Short-term', 0.9998030662536621]],\n",
       "   [[[1187.0, 487.0], [1420.0, 487.0], [1420.0, 507.0], [1187.0, 507.0]],\n",
       "    ['Self-attention Network.', 0.9806221723556519]],\n",
       "   [[[282.0, 517.0], [304.0, 517.0], [304.0, 537.0], [282.0, 537.0]],\n",
       "    ['a', 0.9872640371322632]],\n",
       "   [[[556.0, 510.0], [642.0, 517.0], [639.0, 547.0], [553.0, 539.0]],\n",
       "    ['Proxy', 0.9997577667236328]],\n",
       "   [[[833.0, 532.0], [947.0, 532.0], [947.0, 560.0], [833.0, 560.0]],\n",
       "    ['interest', 0.9991547465324402]],\n",
       "   [[[131.0, 551.0], [215.0, 551.0], [215.0, 579.0], [131.0, 579.0]],\n",
       "    ['Mean', 0.9997847080230713]],\n",
       "   [[[531.0, 546.0], [662.0, 549.0], [661.0, 580.0], [530.0, 577.0]],\n",
       "    ['Selection', 0.9998896718025208]],\n",
       "   [[[832.0, 569.0], [947.0, 569.0], [947.0, 597.0], [832.0, 597.0]],\n",
       "    ['encoder', 0.9997214078903198]],\n",
       "   [[[17.0, 625.0], [325.0, 625.0], [325.0, 647.0], [17.0, 647.0]],\n",
       "    ['Point-wise feed-forward network', 0.9999483823776245]],\n",
       "   [[[822.0, 720.0], [1069.0, 721.0], [1069.0, 753.0], [822.0, 751.0]],\n",
       "    ['Item embeddings', 0.9932976961135864]],\n",
       "   [[[927.0, 813.0], [1061.0, 816.0], [1061.0, 843.0], [927.0, 839.0]],\n",
       "    ['Session s', 0.9991748332977295]]]},\n",
       " '2210.00705v2-Figure2-1.png': {'ocr': [[[1235.0, 483.0],\n",
       "    [1547.0, 483.0],\n",
       "    [1547.0, 510.0],\n",
       "    [1235.0, 510.0]],\n",
       "   ['Audio Feature Extractor', 0.9998517036437988]],\n",
       "  'True_Statements': ['The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence.',\n",
       "   'The vector is then used to compute contrastive loss with the CLIP image encoder‚Äôs output [23].',\n",
       "   '(b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal.',\n",
       "   'The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder.'],\n",
       "  'False_Statements': ['The features are concatenated with a learnable CLS token and fed into a CLIP Image encoder layer to obtain a single vector representing the information of the entire sequence.',\n",
       "   'The vector is then used to compute cross entropy loss with the CLIP image encoder‚Äôs output.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 2: An illustration of SpeechCLIP models. (a) A pre-trained HuBERT [12] extracts audio features. The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence. The vector is then used to compute contrastive loss with the CLIP image encoder‚Äôs output [23]. (b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal. The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder. BN and VQ respectively denote batch normalization and vector quantization.',\n",
       "  'caption': 'Fig. 2: An illustration of SpeechCLIP models. (a) A pre-trained HuBERT [12] extracts audio features. The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence. The vector is then used to compute contrastive loss with the CLIP image encoder‚Äôs output [23]. (b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal. The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder. BN and VQ respectively denote batch normalization and vector quantization.',\n",
       "  'imageText': ['(b)',\n",
       "   'Cascaded',\n",
       "   'SpeechCLIP',\n",
       "   'Audio',\n",
       "   'Feature',\n",
       "   'Extractor',\n",
       "   '(HuBERT,',\n",
       "   'frozen)',\n",
       "   'Keyword',\n",
       "   'x',\n",
       "   'K',\n",
       "   'Contrastive',\n",
       "   'Loss',\n",
       "   'CLIP',\n",
       "   'Text',\n",
       "   'Encoder',\n",
       "   '(frozen)',\n",
       "   'BN',\n",
       "   '+',\n",
       "   'VQ',\n",
       "   'Transformer',\n",
       "   'Encoder',\n",
       "   'CLS',\n",
       "   'x',\n",
       "   'K',\n",
       "   'Audio',\n",
       "   'Features',\n",
       "   'CLIP',\n",
       "   'Image',\n",
       "   'Encoder',\n",
       "   '(frozen)',\n",
       "   '(a)',\n",
       "   'Parallel',\n",
       "   'SpeechCLIP',\n",
       "   '(HuBERT,',\n",
       "   'frozen)',\n",
       "   'CLS',\n",
       "   'Audio',\n",
       "   'Feature',\n",
       "   'Extractor',\n",
       "   'Contrastive',\n",
       "   'Loss',\n",
       "   'Transformer',\n",
       "   'Encoder',\n",
       "   'Audio',\n",
       "   'Features',\n",
       "   'CLIP',\n",
       "   'Image',\n",
       "   'Encoder',\n",
       "   '(frozen)'],\n",
       "  'image_file': '2210.00705v2-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Preliminaries',\n",
       "    'text': \"We briefly explain pre-trained models used in SpeechCLIP. Contrastive Language-Image Pre-training (CLIP) [23]. CLIP uses contrastive learning to pre-train visual models from natural language supervision on an enormous scale, where the supervision comes from paired image-text data. Composing two encoders processing image and text separately, CLIP aims to align semantically similar images and text captions. CLIP can easily transfer across various computer vision tasks with little supervision. Hidden-unit BERT (HuBERT) [12]. HuBERT is a speech SSL method similar to masked language modeling, predicting labels generated by clustered acoustic features. HuBERT comprises a CNN feature extractor followed by a transformer encoder [44] and offers good initialization for many speech processing tasks [19,21].\\nIn SpeechCLIP, pre-trained CLIP and HuBERT models are frozen and serve as feature extractors, as shown in Fig. 2. The CLIP model extracts image and sentence embeddings to supervise SpeechCLIP. Following SUPERB [19], HuBERT's CNN output and transformer encoder's hidden representations are weighted and summed by a set of learnable weights. The weights automatically assign importance to each hidden layer to minimize the overall objective function. Only the newly added components excluding HuBERT and CLIP are learnable during training, reducing the computational cost significantly, thus enabling a larger batch size for contrastive pre-training. In the following sections, we introduce two SpeechCLIP architectures: parallel and cascaded.\",\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': '',\n",
       "  'abstract': 'Data-driven speech processing models usually perform well with a large amount of text supervision, but collecting transcribed speech data is costly. Therefore, we propose Speech-CLIP, a novel framework bridging speech and text through images to enhance speech models without transcriptions. We leverage state-of-the-art pre-trained HuBERT and CLIP, aligning them via paired images and spoken captions with minimal fine-tuning. SpeechCLIP outperforms prior stateof-the-art on image-speech retrieval and performs zero-shot speech-text retrieval without direct supervision from transcriptions. Moreover, SpeechCLIP can directly retrieve semantically related keywords from speech.',\n",
       "  'paddleOCR': [[[[1048.0, 25.0],\n",
       "     [1220.0, 25.0],\n",
       "     [1220.0, 52.0],\n",
       "     [1048.0, 52.0]],\n",
       "    ['Keyword x K', 0.9727369546890259]],\n",
       "   [[[1293.0, 21.0], [1423.0, 21.0], [1423.0, 54.0], [1293.0, 54.0]],\n",
       "    ['CLIP Text', 0.9919963479042053]],\n",
       "   [[[1663.0, 45.0], [1813.0, 45.0], [1813.0, 72.0], [1663.0, 72.0]],\n",
       "    ['Contrastive', 0.9998629093170166]],\n",
       "   [[[1299.0, 58.0], [1415.0, 58.0], [1415.0, 91.0], [1299.0, 91.0]],\n",
       "    ['Encoder', 0.9998584985733032]],\n",
       "   [[[1705.0, 77.0], [1772.0, 82.0], [1770.0, 109.0], [1703.0, 104.0]],\n",
       "    ['Loss', 0.9839274287223816]],\n",
       "   [[[522.0, 95.0], [673.0, 95.0], [673.0, 122.0], [522.0, 122.0]],\n",
       "    ['Contrastive', 0.9998573064804077]],\n",
       "   [[[1305.0, 95.0], [1409.0, 95.0], [1409.0, 128.0], [1305.0, 128.0]],\n",
       "    ['(frozen)', 0.9855647087097168]],\n",
       "   [[[564.0, 127.0], [631.0, 133.0], [629.0, 162.0], [562.0, 156.0]],\n",
       "    ['Loss', 0.970967710018158]],\n",
       "   [[[1073.0, 173.0], [1191.0, 173.0], [1191.0, 206.0], [1073.0, 206.0]],\n",
       "    ['BN + VQ', 0.9976045489311218]],\n",
       "   [[[91.0, 266.0], [360.0, 266.0], [360.0, 291.0], [91.0, 291.0]],\n",
       "    ['Transformer Encoder', 0.9990549683570862]],\n",
       "   [[[1172.0, 260.0], [1444.0, 260.0], [1444.0, 287.0], [1172.0, 287.0]],\n",
       "    ['Transformer Encoder', 0.9973373413085938]],\n",
       "   [[[147.0, 343.0], [350.0, 345.0], [350.0, 373.0], [147.0, 370.0]],\n",
       "    ['Audio Features', 0.999725878238678]],\n",
       "   [[[1290.0, 337.0], [1493.0, 340.0], [1492.0, 367.0], [1289.0, 365.0]],\n",
       "    ['Audio Features', 0.999905526638031]],\n",
       "   [[[521.0, 376.0], [673.0, 383.0], [672.0, 418.0], [520.0, 411.0]],\n",
       "    ['CLIP Image', 0.9901038408279419]],\n",
       "   [[[1662.0, 378.0], [1816.0, 385.0], [1814.0, 420.0], [1660.0, 413.0]],\n",
       "    ['CLIP Image', 0.9897125959396362]],\n",
       "   [[[541.0, 423.0], [653.0, 423.0], [653.0, 450.0], [541.0, 450.0]],\n",
       "    ['Encoder', 0.9998106360435486]],\n",
       "   [[[1684.0, 425.0], [1794.0, 425.0], [1794.0, 452.0], [1684.0, 452.0]],\n",
       "    ['Encoder', 0.9997037053108215]],\n",
       "   [[[23.0, 454.0], [72.0, 454.0], [72.0, 477.0], [23.0, 477.0]],\n",
       "    ['CLS', 0.9967215061187744]],\n",
       "   [[[545.0, 456.0], [648.0, 456.0], [648.0, 491.0], [545.0, 491.0]],\n",
       "    ['(frozen)', 0.999852180480957]],\n",
       "   [[[1686.0, 460.0], [1792.0, 460.0], [1792.0, 493.0], [1686.0, 493.0]],\n",
       "    ['(frozen)', 0.9998908042907715]],\n",
       "   [[[91.0, 485.0], [408.0, 485.0], [408.0, 510.0], [91.0, 510.0]],\n",
       "    ['Audio Feature Extractor', 0.9999162554740906]],\n",
       "   [[[1235.0, 483.0], [1547.0, 483.0], [1547.0, 510.0], [1235.0, 510.0]],\n",
       "    ['Audio Feature Extractor', 0.9998517036437988]],\n",
       "   [[[139.0, 520.0], [360.0, 520.0], [360.0, 547.0], [139.0, 547.0]],\n",
       "    ['(HuBERT, frozen)', 0.9972265362739563]],\n",
       "   [[[1284.0, 520.0], [1500.0, 520.0], [1500.0, 547.0], [1284.0, 547.0]],\n",
       "    ['(HuBERT, frozen)', 0.9995437264442444]],\n",
       "   [[[158.0, 704.0], [555.0, 700.0], [555.0, 735.0], [159.0, 739.0]],\n",
       "    ['(a) Parallel SpeechCLIP', 0.9988799095153809]],\n",
       "   [[[1224.0, 704.0], [1655.0, 704.0], [1655.0, 737.0], [1224.0, 737.0]],\n",
       "    ['(b) Cascaded SpeechCLIP', 0.999265730381012]]]},\n",
       " '2011.06150v1-Figure1-1.png': {'ocr': [[[948.0, 322.0],\n",
       "    [997.0, 322.0],\n",
       "    [997.0, 352.0],\n",
       "    [948.0, 352.0]],\n",
       "   ['J13', 0.9802848696708679]],\n",
       "  'True_Statements': ['vB4 is connected to jL4.', 'vB3 is connected to jL2.'],\n",
       "  'False_Statements': [']vB4 is connected to jL1.',\n",
       "   'vB3 is connected to jL4.'],\n",
       "  'Flowchart-to-Caption': 'Figure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by',\n",
       "  'caption': 'Figure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by',\n",
       "  'imageText': ['m4',\n",
       "   'j10',\n",
       "   'j12',\n",
       "   'j14',\n",
       "   'j7',\n",
       "   'm3',\n",
       "   'j2',\n",
       "   'j3',\n",
       "   'j13',\n",
       "   'j8',\n",
       "   'm2',\n",
       "   'j9',\n",
       "   'j4',\n",
       "   'j5',\n",
       "   'j16',\n",
       "   'm1',\n",
       "   'j1',\n",
       "   'j6',\n",
       "   'j11',\n",
       "   'j15',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4',\n",
       "   'vB',\n",
       "   '[4]',\n",
       "   'Schedule',\n",
       "   'S',\n",
       "   '‚Ä≤',\n",
       "   'vB',\n",
       "   '[3]',\n",
       "   'vB',\n",
       "   '[2]',\n",
       "   'vB',\n",
       "   '[1]',\n",
       "   'jL[4]',\n",
       "   'jL[3]',\n",
       "   'jL[2]',\n",
       "   'm4',\n",
       "   'j10',\n",
       "   'j12',\n",
       "   'j14',\n",
       "   'j16',\n",
       "   'jL[1]',\n",
       "   'm3',\n",
       "   'j9',\n",
       "   'j3',\n",
       "   'j13',\n",
       "   'j8',\n",
       "   'm2',\n",
       "   'j2',\n",
       "   'j4',\n",
       "   'j5',\n",
       "   'j7',\n",
       "   'm1',\n",
       "   'j1',\n",
       "   'j6',\n",
       "   'j11',\n",
       "   'j15',\n",
       "   'Schedule',\n",
       "   'S',\n",
       "   '1',\n",
       "   '2',\n",
       "   '3',\n",
       "   '4'],\n",
       "  'image_file': '2011.06150v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'XX:6',\n",
       "    'text': 'Schedule S 1 2 3 4 m 1 j 1 j 6 j 11 j 15 m 2 j 2 j 4 j 5 j 7 m 3 j 9 j 3 j 13 j 8 m 4 j 10 j 12 j 14 j 16 jL [1] jL [2] jL [3] jL [4] vB [1] vB [2] vB [3] vB [4] Schedule S 1 2 3 4 m 1 j 1 j 6 j 11 j 15 m 2 j 9 j 4 j 5 j 16 m 3 j 2 j 3 j 13 j 8 m 4 j 10 j 12 j 14 j 7\\nFigure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by {j1, j2, j3, j4}, {j5, j6, j7, j8}, {j9, j10, j11, j12}, {j13, j14, j15, j16} and let i = 2 (which means that m1 has already a set of compatible jobs assigned). For clarity we identify the labels of the vertices with the labels of the jobs. Notice how using a matching in the constructed graph the jobs can be exchanged in a way that m2 has only compatible jobs assigned.\\nProof. Remember that each of the cliques has exactly m jobs. We prove that it is always possible to exchange the jobs inside the layers 1, . . . , b for machines m i , . . . , m m , so that the load on m i consists of compatible jobs. Consider the structure of the graph constructed in Algorithm 1. Take any V B ‚äÜ V B . Notice that the cliques corresponding to this vertices, have exactly m ‚àí i + 1 jobs on the machines m i , . . . , m m . A layer i on the machines m i , . . . , m m has exactly m ‚àí i + 1 jobs in total. Hence, clearly the size of neighbors of V B in V L is at least |V B |; hence by Theorem 1 there is a perfect matching in the graph.\\nThe complexity of the procedure is O(mn 3/2 ) by the observation that the complexity of the Hopcrof-Karp algorithm is O(n 3/2 ).\\nConsider an instance of P |cliques|Œ£C j . Assume that each of the cliques have m jobs. If this is not the case, add jobs with processing time 0. Order the jobs non-increasingly according to their processing times and schedule them in a round robin fashion without respect to the cliques, which is by the Smith\\'s Rule optimal [30]. By Lemma 2 we may easily construct a method to change the schedule to respect the cliques. Hence, the following theorem follows: Theorem 3. P |cliques|Œ£C j can be solved to optimality in O(mn 3/2 ) time.\\nProof. First let us notice that by adding the jobs with processing time equal to 0 we do not increase the total completion time, because we can always schedule these jobs before the \"normal\" jobs, even when scheduling with respect to cliques. Then we may use a round robin to obtain an optimal schedule without respect to the cliques. The round robin means an assignment of the job with largest processing time to the position (1, 1), . . . , the job with m-th largest processing time to (m, 1), the job with m + 1-th largest processing time to (1,2), etc. Hence by Lemma 2 and a simple inductive argument the correctness follows. The complexity follows from the fact that we may reuse the graph constructed by Algorithm 1 in the consecutive calls of this procedure. Precisely, consider the consecutive calls of Algorithm 1. During the first call we have to construct the graph (\\nV L ‚à™ V B , E); we have |V L ‚à™ V B | = O(n)\\nand |E| = O(mn). During the construction of the graph, with each edge let us associate a list L(V i , l) a list of machines which have a job from V i as l-th job (on l-th layer). Let us assume that we constructed a perfect matching M in i-th calling of the procedure. Assume that {v L [l], v B [j]} ‚àà M . Moreover assume that l-th job on m i is j ‚àà V j . Let us take the first machine m from L(V j , l), remove it from the list, removing the edge if the list is empty. Let us exchange the l-th job from m with j . Let us add m to L(V j , l) if m = m i . Notice, XX:7\\ns s m1, 1 . . . m1, n . . . mm, n m1 m1, 1 . . . m1, b v V 1 1 . . . v V 1 |V 1 | . . . v V b |V b | t (c, 0) (1, 1) (. . .) (1, n) (. . .) (1, n) (1, 0) (1, 0) (1, 0) (1, 0) (1, 0) (1, 0) (1, cost) (1, cost) (1, cost) (1, cost) (1, cost) (1, 0) (1, 0) (1, 0) (1, 0) (1, 0)\\nFigure 2 An illustration of the flow network constructed for Theorem 5. The first field of an edge\\'s label is its capacity, the second one is the cost per unit of flow. For a vj ‚àà V k the cost field in an arc ((mi, k), vj) is p i j ‚àí 1, hence it is 0 or ‚àû. Notice how the cost of a flow by the network corresponds to a cost of a schedule and how a capacity of an edge forces the flow to \"respect the cliques\".\\nafter exchanging the jobs and updating the lists for all layers, in time O(n), we obtained the graph for the next iteration. Hence, the time complexity of m consecutive calls of the procedure can be bounded, with this trick, by O(mn + m(n\\n3/2 + n)) = O(mn 3/2 ).',\n",
       "    'n_publication_ref': 11,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Total Completion Time Minimization for Scheduling with Incompatibility Cliques',\n",
       "  'abstract': 'This paper considers parallel machine scheduling with incompatibilities between jobs. The jobs form a graph and no two jobs connected by an edge are allowed to be assigned to the same machine. In particular, we study the case where the graph is a collection of disjoint cliques. Scheduling with incompatibilities between jobs represents a well-established line of research in scheduling theory and the case of disjoint cliques has received increasing attention in recent years. While the research up to this point has been focused on the makespan objective, we broaden the scope and study the classical total completion time criterion. In the setting without incompatibilities, this objective is well known to admit polynomial time algorithms even for unrelated machines via matching techniques. We show that the introduction of incompatibility cliques results in a richer, more interesting picture. Scheduling on identical machines remains solvable in polynomial time, while scheduling on unrelated machines becomes APX-hard. We identify several more subcases of theses problems that are polynomial time solvable or NP-hard, respectively. Furthermore, we study the problem under the paradigm of fixed-parameter tractable algorithms (FPT). In particular, we consider a problem variant with assignment restrictions for the cliques rather than the jobs. We prove that it is NP-hard and can be solved in FPT time with respect to the number of cliques. Moreover, we show that the problem on unrelated machines can be solved in FPT time for reasonable parameters, e.g., the parameter pair: number of machines and maximum processing time. The latter result is a natural extension of known results for the case without incompatibilities and can even be extended to the case of total weighted completion time. All of the FPT results make use of n-fold Integer Programs that recently have received great attention by proving their usefulness for scheduling problems.',\n",
       "  'paddleOCR': [[[[96.0, 17.0], [279.0, 17.0], [279.0, 48.0], [96.0, 48.0]],\n",
       "    ['Schedule S', 0.9997008442878723]],\n",
       "   [[[845.0, 16.0], [999.0, 16.0], [999.0, 47.0], [845.0, 47.0]],\n",
       "    ['Schedule', 0.9998905062675476]],\n",
       "   [[[401.0, 34.0], [480.0, 29.0], [482.0, 63.0], [404.0, 68.0]],\n",
       "    ['UB4', 0.8049519062042236]],\n",
       "   [[[646.0, 32.0], [717.0, 29.0], [718.0, 62.0], [648.0, 65.0]],\n",
       "    ['jL[4', 0.8432774543762207]],\n",
       "   [[[995.0, 31.0], [1023.0, 8.0], [1040.0, 27.0], [1013.0, 50.0]],\n",
       "    ['S', 0.8215264081954956]],\n",
       "   [[[90.0, 77.0], [107.0, 77.0], [107.0, 108.0], [90.0, 108.0]],\n",
       "    ['1', 0.9548484086990356]],\n",
       "   [[[146.0, 76.0], [168.0, 76.0], [168.0, 109.0], [146.0, 109.0]],\n",
       "    ['2', 0.9950489401817322]],\n",
       "   [[[205.0, 76.0], [228.0, 76.0], [228.0, 109.0], [205.0, 109.0]],\n",
       "    ['3', 0.9998176693916321]],\n",
       "   [[[265.0, 77.0], [288.0, 77.0], [288.0, 109.0], [265.0, 109.0]],\n",
       "    ['4', 0.9998177886009216]],\n",
       "   [[[842.0, 76.0], [864.0, 76.0], [864.0, 109.0], [842.0, 109.0]],\n",
       "    ['1', 0.9053313136100769]],\n",
       "   [[[899.0, 75.0], [926.0, 75.0], [926.0, 110.0], [899.0, 110.0]],\n",
       "    ['2', 0.9996329545974731]],\n",
       "   [[[960.0, 76.0], [982.0, 76.0], [982.0, 109.0], [960.0, 109.0]],\n",
       "    ['3', 0.983025848865509]],\n",
       "   [[[1020.0, 77.0], [1043.0, 77.0], [1043.0, 108.0], [1020.0, 108.0]],\n",
       "    ['4', 0.9995680451393127]],\n",
       "   [[[399.0, 153.0], [482.0, 145.0], [486.0, 182.0], [403.0, 191.0]],\n",
       "    ['VB[3', 0.8199791312217712]],\n",
       "   [[[642.0, 145.0], [719.0, 145.0], [719.0, 186.0], [642.0, 186.0]],\n",
       "    ['jL[3]', 0.8721555471420288]],\n",
       "   [[[0.0, 160.0], [54.0, 166.0], [51.0, 196.0], [0.0, 190.0]],\n",
       "    ['m1', 0.9730830192565918]],\n",
       "   [[[79.0, 161.0], [116.0, 161.0], [116.0, 196.0], [79.0, 196.0]],\n",
       "    ['J1', 0.8468246459960938]],\n",
       "   [[[137.0, 160.0], [176.0, 160.0], [176.0, 195.0], [137.0, 195.0]],\n",
       "    ['j6', 0.8795937299728394]],\n",
       "   [[[183.0, 155.0], [301.0, 158.0], [300.0, 201.0], [182.0, 198.0]],\n",
       "    ['1115', 0.9980089068412781]],\n",
       "   [[[754.0, 160.0], [808.0, 165.0], [805.0, 195.0], [751.0, 190.0]],\n",
       "    ['m1', 0.9844818115234375]],\n",
       "   [[[832.0, 160.0], [870.0, 160.0], [870.0, 196.0], [832.0, 196.0]],\n",
       "    ['J1', 0.8388209342956543]],\n",
       "   [[[894.0, 161.0], [931.0, 161.0], [931.0, 195.0], [894.0, 195.0]],\n",
       "    ['J6', 0.6984229683876038]],\n",
       "   [[[946.0, 161.0], [995.0, 161.0], [995.0, 196.0], [946.0, 196.0]],\n",
       "    ['J11', 0.7927144169807434]],\n",
       "   [[[1006.0, 157.0], [1059.0, 167.0], [1053.0, 202.0], [999.0, 192.0]],\n",
       "    ['j15', 0.9196911454200745]],\n",
       "   [[[1.0, 238.0], [56.0, 244.0], [53.0, 273.0], [0.0, 267.0]],\n",
       "    ['m2', 0.955999493598938]],\n",
       "   [[[83.0, 244.0], [115.0, 244.0], [115.0, 271.0], [83.0, 271.0]],\n",
       "    ['12', 0.9143328070640564]],\n",
       "   [[[137.0, 240.0], [179.0, 240.0], [179.0, 272.0], [137.0, 272.0]],\n",
       "    ['j4', 0.7974820733070374]],\n",
       "   [[[187.0, 237.0], [291.0, 237.0], [291.0, 277.0], [187.0, 277.0]],\n",
       "    ['j5|j7', 0.9106480479240417]],\n",
       "   [[[754.0, 240.0], [810.0, 245.0], [808.0, 273.0], [751.0, 268.0]],\n",
       "    ['m2', 0.9670727849006653]],\n",
       "   [[[837.0, 240.0], [871.0, 245.0], [867.0, 274.0], [833.0, 270.0]],\n",
       "    ['J9', 0.812545657157898]],\n",
       "   [[[896.0, 242.0], [930.0, 242.0], [930.0, 271.0], [896.0, 271.0]],\n",
       "    ['14', 0.9852458834648132]],\n",
       "   [[[953.0, 242.0], [989.0, 242.0], [989.0, 271.0], [953.0, 271.0]],\n",
       "    ['15', 0.7485800981521606]],\n",
       "   [[[1003.0, 236.0], [1056.0, 243.0], [1052.0, 278.0], [999.0, 271.0]],\n",
       "    ['j16', 0.8233941197395325]],\n",
       "   [[[401.0, 273.0], [480.0, 266.0], [483.0, 299.0], [404.0, 306.0]],\n",
       "    ['UB2', 0.8224649429321289]],\n",
       "   [[[642.0, 268.0], [715.0, 264.0], [717.0, 301.0], [644.0, 306.0]],\n",
       "    ['jL[2', 0.9313476085662842]],\n",
       "   [[[0.0, 316.0], [56.0, 323.0], [52.0, 353.0], [0.0, 345.0]],\n",
       "    ['m3', 0.9954589605331421]],\n",
       "   [[[82.0, 322.0], [114.0, 322.0], [114.0, 351.0], [82.0, 351.0]],\n",
       "    ['19', 0.7497678399085999]],\n",
       "   [[[142.0, 314.0], [179.0, 321.0], [172.0, 355.0], [136.0, 348.0]],\n",
       "    ['j3', 0.9910944104194641]],\n",
       "   [[[189.0, 316.0], [247.0, 321.0], [244.0, 355.0], [186.0, 350.0]],\n",
       "    ['J13', 0.8808913230895996]],\n",
       "   [[[255.0, 321.0], [290.0, 321.0], [290.0, 350.0], [255.0, 350.0]],\n",
       "    ['J8', 0.8391735553741455]],\n",
       "   [[[754.0, 318.0], [812.0, 324.0], [809.0, 353.0], [751.0, 347.0]],\n",
       "    ['m3', 0.9938926696777344]],\n",
       "   [[[834.0, 320.0], [871.0, 323.0], [869.0, 352.0], [832.0, 350.0]],\n",
       "    ['J2', 0.9916762113571167]],\n",
       "   [[[896.0, 321.0], [930.0, 321.0], [930.0, 351.0], [896.0, 351.0]],\n",
       "    ['J3', 0.8729721903800964]],\n",
       "   [[[948.0, 322.0], [997.0, 322.0], [997.0, 352.0], [948.0, 352.0]],\n",
       "    ['J13', 0.9802848696708679]],\n",
       "   [[[1012.0, 319.0], [1050.0, 324.0], [1046.0, 354.0], [1008.0, 349.0]],\n",
       "    ['J8', 0.8044618368148804]],\n",
       "   [[[5.0, 399.0], [51.0, 406.0], [47.0, 427.0], [2.0, 420.0]],\n",
       "    ['m', 0.9070515036582947]],\n",
       "   [[[79.0, 399.0], [299.0, 399.0], [299.0, 432.0], [79.0, 432.0]],\n",
       "    ['10J12J14J16', 0.8822585940361023]],\n",
       "   [[[405.0, 391.0], [454.0, 391.0], [454.0, 420.0], [405.0, 420.0]],\n",
       "    ['UB', 0.8350434899330139]],\n",
       "   [[[756.0, 398.0], [809.0, 404.0], [807.0, 429.0], [753.0, 423.0]],\n",
       "    ['m4', 0.9336919188499451]],\n",
       "   [[[827.0, 396.0], [1046.0, 396.0], [1046.0, 433.0], [827.0, 433.0]],\n",
       "    ['J10|J12[J14J7', 0.8526625037193298]]]},\n",
       " '2103.14969v2-Figure7-1.png': {'ocr': [[[505.0, 497.0],\n",
       "    [584.0, 497.0],\n",
       "    [584.0, 513.0],\n",
       "    [505.0, 513.0]],\n",
       "   ['Conv3x3x3', 0.9892444610595703]],\n",
       "  'True_Statements': ['Input is a four channel 3D MRI crop.',\n",
       "   'Output contains the segmentation map and the MRI image.',\n",
       "   'Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters.'],\n",
       "  'False_Statements': ['Input is a two channel 3D MRI crop.',\n",
       "   'Output does not contain the MRI image.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 7. V-Net with autoencoder regularization: ‚ÄúSchematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.‚Äù [33]',\n",
       "  'caption': 'Fig. 7. V-Net with autoencoder regularization: ‚ÄúSchematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.‚Äù [33]',\n",
       "  'imageText': [],\n",
       "  'image_file': '2103.14969v2-Figure7-1.png',\n",
       "  'sections': [{'heading': 'C. The Evolution of Modern Segmentation Networks',\n",
       "    'text': '1) U-Net: Ronneberger et al. [29] sought to address the issue of both local and global feature integration with their proposal of U-Net, the progenitor to many modern deep learning models applied to segmentation which takes its name from the \"U\" shape the canonical graphical representation of the network resembles (see Figure 3).\\nUp until then, a major issue in segmentation methods was balancing the integration of varying levels of spatial context as input to segmentation methods. As segmentation relies on both local and global contextual information, a successful method would necessarily include both types of information.\\nU-Net solves this problem via a series of downsampling, upsampling, and feature concatenation operations in its architecture, enabling simultaneous multi-scale feature learning and efficient segmentation map generation with a single forward pass of the network (see Figure 3 for an overview of the architecture). The network takes as input 572x572 grayscale image slices and is divided into two halves: a contracting path which comprises the first part of the network and an expansive path which comprises the latter half. The contracting half is characterized by the presence of downsampling operations which occur at a rate of 2x via max pooling layers with 2x2 kernels and a stride of 2. The expansive path symmetrically contains all the upsampling layers, with upsampling also occurring at a rate of 2x via deconvolution layers with 2x2 kernels. In the contracting path, the feature maps for each layer immediately preceding a downsampling layer are copied and concatenated to the input features of the corresponding post-upsampling layer in the expansive path after being cropped to matching spatial dimensions. The final layer of the network is a convolutional layer with a 1x1 kernel which classifies each voxel of the central 388x388 image crop (see Figure 4). The segmentation task was framed as a pixel classification problem and the authors consequently utilized a cross-entropy loss.\\nThe authors tested U-net on several datasets, notably the ISBI cell tracking challenge 2015 where they bested the previous SoTA on segmentation of brain tumor cells captured by phase-contrast microscopy (9% improvement in IOU score), and cervical cancer cells captured by differential interference contrast microscopy (31% improvement in IOU score).\\n2) U-Net with Residual Connections: Drozdzal et al. [30] explored the use of short and long skip connections in a U-net-like model (modifying [27] by adding an expanding Fig. 4. U-Net: \"Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring\" [29] path and corresponding connections from the contracting path). They noted that the copy and concatenation of features in U-Net\\'s contracting path with features in the expanding path are akin to long skip connections and so choose to sum rather than concatenate the features in their models. The combination of both short and long skip connections led to better convergence and training stability relative to variants of the network that either utilized only one type of connection, or neither.\\n3) 3D U-Net: √á i√ßek et al. [31] directly extended U-net to process all three spatial dimensions simultaneously, proposing a variant that utilized 3D convolutions in place of 2D convolutions for full volumetric segmentation. Aside from reducing the number of output features in every layer of the the contracting path by half, save for those directly preceding downsampling operations, the 3D U-net was identical to the original U-net (see Figure 5). Fig. 5. 3D U-Net: \"The 3D u-net architecture. Blue boxes represent feature maps. The number of channels is denoted above each feature map.\" [31] Given sparsely annotated training data (volumes with only a few slices annotated), the authors used the 3D U-net to produce dense volumetric segmentation of Xenopus kidney embryos captured by confocal microscopy in two tasks. The first was a \\'semi-automated\" segmentation task where dense (complete) volume segmentation was produced for a sparsely annotated training sample, achieving a 7% higher IOU score relative to a 2D U-Net.\\nThe second was a fully-automated segmentation task Fig. 6. V-Net: \"Schematic representation of our network architecture.\" \"...processes 3D data by performing volumetric convolutions.\" [32] where a dense volume segmentation was produced for an unlabeled volume on which the network had not been trained, achieving an 18% higher IOU compared to a 2D U-Net. 4) V-Net: Milletari et al. [32] combined the above ideas in \"V-Net\", a 3D U-net with residual blocks applied to the task of 3D prostate MRI segmentation (see Figure 6).\\nThe integration of greater spatial context and residual learning led to remarkable performance benefits, being on par with the then SoTA model on the \"PROMISE 2012\" challenge dataset at a reduced training convergence time common to residual networks. Unlike U-net [29] and 3D Unet [31], the authors eschew batch normalization and follow the increasingly common trend of eliminating pooling layers, performing downsampling via convolutions kernels of size 2x2x2 and a stride of two. They also performed segmentation on the entire image patch as opposed to previous works which only segmented the central section of the image patch.\\nAnother major contribution of the authors was the proposal of a soft DICE loss which they used in their loss function in an attempt to directly optimize the network for segmentation accuracy. This version led to 13% greater performance than one trained using multinomial logistic loss with sample weighting. The resulting segmentation maps were not only more accurate, but also smoother and more visually pleasing.\\nLL dice is a soft DICE loss applied to the decoder output p pred to match the segmentation mask p true :\\nLL dice = 2 √ó N i p true √ó p pred N i p 2 true + N i p 2 pred (1\\n)\\nwhere summation is voxel-wise. 5) V-net with Autoencoder Regularization: Myronenko [33] devised the current SoTA for 3D MRI brain tumor subregion segmentation and won the Medical Image Computing and Computer Assisted Intervention (MICCAI) Multimodal Brain Tumor Segmentation (BraTS) 2018 challenge. The author extended the V-net model by emphasizing the V-Net as an auto-encoder and imposing regularization contraints during training via a variational auto-encoder (VAE) [34] decoder branch attached to the encoder layer which bifurcated output to both the segmentation decoder branch and the VAE decoder branch (see Figure 7). They then were able to leverage the KL divergence and L2 loss of the VAE branch in addition to a soft DICE loss (see Equation 1) [32] of the segmentation decoder branch in a composite loss function to both regularize the encoder and impose additional constraints. This had the effect of ensuring that features learned in the layers prior to the upsampling portion of the net minimized reconstruction error; in other words, biasing learned features to those that are the most salient and independent. The VAE branch was only used during training and was removed at test time.\\nThe output of the segmentation decoder branch is a direct segmentation map in the form of a three-channel image with spatial dimensions identical to the input image and each channel corresponding to one of three tumor subregion classes (i.e., enhancing tumor-core, whole tumor, tumor core).\\nAnother performance driver was their use of group normalization [35], being especially prudent given that the author forewent the use of multiple samples per batch (i.e., used a batch size of 1) in favor of maximizing the input image crop size, precluding the use of batch normalization.\\nThe aforementioned maximization of input crop size enabled the use of an extremely large input (160x192x128) relative to the original image size (240x240x155 for all samples). This is in contrast to the much smaller input used in EMMA [36] (64x64x64), the prior year\\'s SoTA approach, and No New-Net [37] (128x128x128), the current year\\'s 2 nd place method which incidentally was simply a 3D U-net [31] with the larger crop size (in addition to minor training differences and post-processing) being the only notable modification.\\nThese results seem to support the idea that the amount of spatial information has a much a greater impact on segmentation performance than complicated architectural modifications and pre-/post-processing techniques. Indeed, the author noted that experiments conducted with smaller input sizes, larger batch sizes, and the use of batch normalization resulted in worse performance. Experiments utilizing sophisticated data augmentation techniques (i.e., random histogram matching, affine image transforms, and random image filtering) showed no performance benefits. Tuning the network\\'s segmentation results with conditional random fields showed mixed results, improving performance in some cases while reducing it in others. Performing test-time data augmentation by applying the neural network to 8 mirror flips of the 3D image and averaging realigned segmentation results led to an insignificant performance improvement. Ensembling 10 separately trained version of the model produced a roughly 1% performance improvement.\\nThe author also found that increasing the amount of information at each layer (i.e., number of learned filters) was able to consistently improve performance while increasing depth did not result in any performance gains, lending support to the idea that more salient low-to mid-level feature representations exist relative to higher-level feature spaces in the domain of medical imaging. This theory is further reinforced by [38] which was simply a U-Net [29] with eight additional convolutional layers immediately following the input layer that surpassed an ensemble of U-nets on the 2017 MICCAI WMH challenge. Fig. 7. V-Net with autoencoder regularization: \"Schematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.\" [33] V. DEEP LEARNING FOR ULTRASOUND: APPLICATIONS, IMPLICATIONS, AND CHALLENGES Ultrasound is the most widely used modality in medical imaging but among the least researched in terms of automated analysis, possibly due to the fact that areas of clinical significance are fewer and less severe than, say, MRI, CT, or X-ray, which are generally reserved for the diagnosis of serious, often life-threatening pathologies. In contrast, ultrasound has historically being used chiefly in obstetrics, though it in theory could be used to support clinicians in a wide array of applications. Indeed, it is now often used to diagnose pathologies in parts of the body such as the heart, lung, prostate, thyroid, and breast; in image-assisted surgical procedures; and in point-of-care diagnostic pipelines in emergency medicine.\\nUltrasound is an imaging modality with particular clinical significance, being a safe, portable, relatively low-cost realtime diagnostic tool. It can be quickly and easily deployed in the field, making it especially valuable in disaster response scenarios and areas without adequate access to well-equipped medical facilities. Point-of-care ultrasound has been proven to provide faster, more precise diagnoses [39], [40], reduce procedural complications [41], and decrease time-totreatment [42]. In combination with its use of non-ionizing radiation and fairly non-invasive application, it is afforded the unique ability to image \"any anatomy often, anywhere\", a feature that may otherwise be contraindicated, infeasible, or even impossible.\\nUnfortunately, it possesses unique considerations absent in other modalities. Given the nature of ultrasonography and the typically free-hand nature of ultrasound image acquisition, image quality is highly dependent on the particular device, device settings, and acquisition technique used. This leads to the presence of speckle noise, artifacts, a greater emphasis on operator expertise, and significant variability between, and even within, observers. Additionally, ultrasound image interpretation relies on being able to dynamically investigate the same anatomical areas at different viewing angles in real-time, rather than over a set of static images obtained in advance. These factors result in the need for a high level of expertise to properly acquire and interpret ultrasound images, a major barrier to full adoption by clinicians in all applicable areas.\\nUltrasound technicians require much more training relative to other types of radiologists, training which necessarily must include exposure to a wide variety of pathologies across a wide variety of patients in a format that enables dynamic investigation, historically in-person workshops, with the confluence of these factors resulting in fewer opportunities to receive this training. Indeed, a 2018 study found that 84% of the physician assistants surveyed believed that the training they received during their clinical rotations was insufficient preparation for clinical practice [43]. Among the many potential solutions to achieving sufficient ultrasonography expertise, increasing access to effective training opportunities via computer-based simulation platforms and developing automated image analysis systems to assist radiologists are two of the more promising areas.\\nA. Prospective Solutions 1) Computer-based Simulation: Hardware and software platforms for ultrasound simulation have been proposed to directly empower radiologists-in-training and address the growing need for accessible ultrasound training [44]. Of the many commercially available platforms, the SonoSim SonoSimulator in particular is among the most popular [45], [46]. It has been shown to be as effective as live model training at teaching image acquisition [47] and more effective for image interpretation [48] with particular implications for urgent and logistically challenging scenarios, namely disaster-response training which can be greatly catalyzed by the focused assessment with sonography in trauma (FAST) protocol [49]. The major advantage of the SonoSim SonoSimulator over other platforms is its highfidelity simulation, usage of real patient data (as opposed to most other commercially available platforms which use synthetically generated data due to the difficulties in obtaining high-quality in-vivo data), and thousands of cases with a wide array of pathologies. This not only allows for repeated, realistic training on typical cases but also on rare but serious conditions which, due to their infrequence, are likely to be underdetected when presented in the clinical setting [44].\\n2) Automated Image Analysis Algorithms: In conjunction with greater access to high-quality training, automated image analysis could increase clinical effectiveness and lower the amount of time, expertise, and cognitive resources required of sonographers by facilitating image interpretation through capabilities such as anatomy detection, classification, and semantic segmentation; object disambiguation with noise and artifact reduction through \"salient signal recovery\" [50]; and directional guidance/feedback in image-assisted interventions.\\n3) A Dual Pathway Approach: Finally, the combination of automated image analysis into an ultrasound training platform could catalyze expertise acquisition while reducing inter-and intra-observer variability by providing a highquality, standardized training experience. This could be achieved via the integration of automated analyses that would be used in the clinic or by scaffolding learning with tools tailor-made to complement a didactic pedagogy.',\n",
       "    'n_publication_ref': 33,\n",
       "    'n_figure_ref': 10}],\n",
       "  'title': 'Catalyzing Clinical Diagnostic Pipelines Through Volumetric Medical Image Segmentation Using Deep Neural Networks: Past, Present, & Future',\n",
       "  'abstract': 'Deep learning has made a remarkable impact in the field of natural image processing over the past decade. Consequently, there is a great deal of interest in replicating this success across unsolved tasks in related domains, such as medical image analysis. Core to medical image analysis is the task of semantic segmentation which enables various clinical workflows. Due to the challenges inherent in manual segmentation, many decades of research have been devoted to discovering extensible, automated, expert-level segmentation techniques. Given the groundbreaking performance demonstrated by recent neural network-based techniques, deep learning seems poised to achieve what classic methods have historically been unable. This paper will briefly overview some of the state-of-the-art (SoTA) neural network-based segmentation algorithms with a particular emphasis on the most recent architectures, comparing and contrasting the contributions and characteristics of each network topology. Using ultrasonography as a motivating example, it will also demonstrate important clinical implications of effective deep learning-based solutions, articulate challenges unique to the modality, and discuss novel approaches developed in response to those challenges, concluding with the proposal of future directions in the field. Given the generally observed ephemerality of the best deep learning approaches (i.e., the extremely quick succession of the SoTA), the main contributions of the paper are its contextualization of modern deep learning architectures with historical background and the elucidation of the current trajectory of volumetric medical image segmentation research.',\n",
       "  'paddleOCR': [[[[783.0, 128.0],\n",
       "     [812.0, 124.0],\n",
       "     [814.0, 143.0],\n",
       "     [785.0, 146.0]],\n",
       "    ['x2', 0.6799076199531555]],\n",
       "   [[[723.0, 144.0], [752.0, 144.0], [752.0, 164.0], [723.0, 164.0]],\n",
       "    ['x2', 0.9493083357810974]],\n",
       "   [[[665.0, 174.0], [691.0, 174.0], [691.0, 191.0], [665.0, 191.0]],\n",
       "    ['x2', 0.9109082221984863]],\n",
       "   [[[339.0, 263.0], [367.0, 263.0], [367.0, 280.0], [339.0, 280.0]],\n",
       "    ['x2', 0.8741878271102905]],\n",
       "   [[[511.0, 255.0], [631.0, 255.0], [631.0, 274.0], [511.0, 274.0]],\n",
       "    ['256x20x24x16', 0.9532525539398193]],\n",
       "   [[[7.0, 279.0], [135.0, 279.0], [135.0, 298.0], [7.0, 298.0]],\n",
       "    ['4x160x192x128', 0.9952942132949829]],\n",
       "   [[[247.0, 280.0], [275.0, 280.0], [275.0, 298.0], [247.0, 298.0]],\n",
       "    ['x2', 0.9110003113746643]],\n",
       "   [[[184.0, 298.0], [211.0, 298.0], [211.0, 315.0], [184.0, 315.0]],\n",
       "    ['x2', 0.9702319502830505]],\n",
       "   [[[682.0, 290.0], [736.0, 290.0], [736.0, 305.0], [682.0, 305.0]],\n",
       "    ['N', 0.8710970282554626]],\n",
       "   [[[689.0, 338.0], [722.0, 342.0], [720.0, 359.0], [687.0, 356.0]],\n",
       "    ['256', 0.9991815090179443]],\n",
       "   [[[740.0, 348.0], [765.0, 352.0], [763.0, 365.0], [738.0, 362.0]],\n",
       "    ['X', 0.7851033806800842]],\n",
       "   [[[795.0, 367.0], [817.0, 367.0], [817.0, 379.0], [795.0, 379.0]],\n",
       "    ['X', 0.8004477620124817]],\n",
       "   [[[825.0, 392.0], [852.0, 389.0], [854.0, 406.0], [827.0, 410.0]],\n",
       "    ['x2', 0.7746484875679016]],\n",
       "   [[[724.0, 469.0], [933.0, 468.0], [933.0, 487.0], [724.0, 488.0]],\n",
       "    ['x2 = conv3x3x3 stride 2', 0.9704545140266418]],\n",
       "   [[[88.0, 484.0], [133.0, 489.0], [131.0, 505.0], [86.0, 501.0]],\n",
       "    ['Group', 0.995071530342102]],\n",
       "   [[[352.0, 486.0], [398.0, 488.0], [397.0, 505.0], [351.0, 502.0]],\n",
       "    ['Group', 0.9989336729049683]],\n",
       "   [[[87.0, 504.0], [131.0, 509.0], [129.0, 524.0], [86.0, 520.0]],\n",
       "    ['Norm', 0.9984579086303711]],\n",
       "   [[[165.0, 496.0], [207.0, 496.0], [207.0, 513.0], [165.0, 513.0]],\n",
       "    ['ReLU', 0.9984917640686035]],\n",
       "   [[[240.0, 497.0], [319.0, 497.0], [319.0, 513.0], [240.0, 513.0]],\n",
       "    ['Conv3x3x3', 0.989684522151947]],\n",
       "   [[[352.0, 505.0], [397.0, 507.0], [396.0, 524.0], [351.0, 521.0]],\n",
       "    ['Norm', 0.99861741065979]],\n",
       "   [[[431.0, 496.0], [472.0, 496.0], [472.0, 513.0], [431.0, 513.0]],\n",
       "    ['ReLU', 0.9976228475570679]],\n",
       "   [[[505.0, 497.0], [584.0, 497.0], [584.0, 513.0], [505.0, 513.0]],\n",
       "    ['Conv3x3x3', 0.9892444610595703]],\n",
       "   [[[726.0, 504.0], [1041.0, 504.0], [1041.0, 523.0], [726.0, 523.0]],\n",
       "    ['x2 = conv1x1x1, 3D bilinear upsizing', 0.9933507442474365]]]},\n",
       " '1245438-Figure1-1.png': {'ocr': [[[356.0, 69.0],\n",
       "    [390.0, 69.0],\n",
       "    [390.0, 242.0],\n",
       "    [356.0, 242.0]],\n",
       "   ['Conv+ELU', 0.9995455741882324]],\n",
       "  'True_Statements': ['An image is the input for Conv+ELU.',\n",
       "   'Conv+ELU + Conv+BN is the input for Conv.'],\n",
       "  'False_Statements': ['An image is the output of Conv+ELU.',\n",
       "   'Conv+ELU + Conv+BN is the output of Conv.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 1: The network architecture with pipe-lined components.',\n",
       "  'caption': 'Fig. 1: The network architecture with pipe-lined components.',\n",
       "  'imageText': ['Conv+BN',\n",
       "   '.',\n",
       "   '.',\n",
       "   '.',\n",
       "   'on',\n",
       "   'vConv+ELU+',\n",
       "   'C',\n",
       "   'LU',\n",
       "   'v+',\n",
       "   'E',\n",
       "   'C',\n",
       "   'on'],\n",
       "  'image_file': '1245438-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Network Architecture',\n",
       "    'text': \"Our model is derived from the vgg-verydeep-19 pre-trained network [17], and includes a total of 15 convolutional layer blocks and 2 separate convolutional layers. There is no fully connected layer. The network architecture is shown in Fig. 1. The first convolutional layer is connected to an ELU layer to add nonlinearity, and the output of the  last convolutional layer is fed into the loss layer. Between the two ends, the network is composed of 15 convolutional layer blocks with 'Conv-ELU-Conv-BN' pattern.\\nIt has been shown that ELU can replace ReLU as the activation function in section 2.2. Therefore, 'Conv-ELU' is built in each convolutional block. Batch normalization (BN) is necessary for residual denoising as reported by Zhang et al. [22]. However, direct combination of BN and ELU will adversely affect the network performance [4]. Fortunately, it is known that the pixel-wise co-efficient transformation can be achieved by a 1√ó1 convolutional layer, which can also increase the non-linearity of the decision function [17,9]. We thus utilize a 1√ó1 convolutional layer between ELU and BN layer. Every second 'Conv' in each block holds 1√ó1 filters, and other filters are all in the size of 3√ó3. Such configuration not only exerts the advantages of 1√ó1 convolutional layer, but also avoids direct connection of BN and ELU.\\nNote that our model does not contain any pooling layer since the final output must have the same size as the original input. One may argue that fully convolutional networks (FCN) [14] can also restore the output size, however it cannot be used in our case because it contains a pooling layer and thus needs up-sampling operation, which is not desirable for image denoising. Furthermore, FCN was originally designed for pixel-level classification without fully considering the relationships between pixels.\",\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'An ELU Network with Total Variation for Image Denoising',\n",
       "  'abstract': \"In this paper, we propose a novel convolutional neural network (CNN) for image denoising, which uses exponential linear unit (ELU) as the activation function. We investigate the suitability by analyzing ELU's connection with trainable nonlinear reaction diffusion model (TNRD) and residual denoising. On the other hand, batch normalization (BN) is indispensable for residual denoising and convergence purpose. However, direct stacking of BN and ELU degrades the performance of CNN. To mitigate this issue, we design an innovative combination of activation layer and normalization layer to exploit and leverage the ELU network, and discuss the corresponding rationale. Moreover, inspired by the fact that minimizing total variation (TV) can be applied to image denoising, we propose a TV regularized L2 loss to evaluate the training effect during the iterations. Finally, we conduct extensive experiments, showing that our model outperforms some recent and popular approaches on Gaussian denoising with specific or randomized noise levels for both gray and color images.\",\n",
       "  'paddleOCR': [[[[356.0, 69.0],\n",
       "     [390.0, 69.0],\n",
       "     [390.0, 242.0],\n",
       "     [356.0, 242.0]],\n",
       "    ['Conv+ELU', 0.9995455741882324]],\n",
       "   [[[471.0, 96.0], [660.0, 94.0], [660.0, 121.0], [471.0, 123.0]],\n",
       "    ['Conv+ELU+', 0.9996992945671082]],\n",
       "   [[[763.0, 115.0], [797.0, 116.0], [794.0, 202.0], [759.0, 201.0]],\n",
       "    ['Conv', 0.9991246461868286]],\n",
       "   [[[486.0, 135.0], [643.0, 135.0], [643.0, 166.0], [486.0, 166.0]],\n",
       "    ['Conv+BN', 0.9997163414955139]]]},\n",
       " '2011.09340v3-Figure9-1.png': {'ocr': [[[350.0, 341.0],\n",
       "    [393.0, 341.0],\n",
       "    [393.0, 387.0],\n",
       "    [350.0, 387.0]],\n",
       "   ['A', 0.9998718500137329]],\n",
       "  'True_Statements': [\"Input A' to EB will get A.\",\n",
       "   \"Input C' to EB will result in C.\"],\n",
       "  'False_Statements': [\"Inputs A' to EB will get B'.\",\n",
       "   \"Input C' to EB will result in B'.\"],\n",
       "  'Flowchart-to-Caption': 'Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb Œ•ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that Œ•ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently.',\n",
       "  'caption': 'Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb Œ•ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that Œ•ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2011.09340v3-Figure9-1.png',\n",
       "  'sections': [{'heading': 'Channel steering and quantum combs',\n",
       "    'text': \"We finish our discussion of sufficient conditions for bipartite entanglement in combs with the case E(A : BC) > 0. While the corresponding conditions on their own are not very illuminating, it is nonetheless insightful to consider this type of bipartite quantum correlations and connect it to the concept of channel steering [35].\\nTo this end, consider a quantum channel K : B(H B ) ‚Üí B(H A )‚äóB(H C ) with corresponding Choi matrix K BAC (such channels with one input and two output spaces are also called 'broadcasting channels' [75]). As K is assumed to be a channel, it satisfies tr AC (K BAC ) = 1 B . From Eq. (8), we see that any comb Œ• ABC also satisfies tr AC (Œ• ABC ) = 1 B (with the additional constraint tr C (Œ• ABC ) = 1 B ‚äó Œ• A ), and can thus be considered as a quantum broadcast channel (see Fig. 8). Consequently, while originally applied to general broadcast channels [35], all of the following considerations directly apply to combs.\\nGiven such a channel K BAC , Alice could measure the system A, using a POVM {(E a|x A ) T } a , where x denotes the choice of POVM, a corresponds to the respective outcome, and the transpose is added to simplify the subsequent notation. Conditionally on each of Alice's outcomes, the mapping between Bob and Charlie would be given by (trace non-increasing) CP maps\\nK a|x BC := K BAC E a|x A (28)\\nthat add up to a CPTP map K BC = a K a|x BC . In this way, Alice could create a collection {K x|a BC } a,x of instruments that all add up to the same channel K BC . Such a collection -in close analogy with the theory of steering of quantum states [13][14][15] -is called a 'channel assemblage' of the channel K BC . A channel assemblage is said to be unsteerable, if there exists an instrument {K Œª BC } Œª , probabilities p(Œª) and conditional probabilities p(a|x, Œª), such that for all {a, x} one has 3\\nK a|x BC = Œª p(Œª)p(a|x, Œª)K Œª BC .(29)\\nOtherwise, the assemblage is steerable. If Alice can create a steerable assemblage, then the channel K BAC is said to be steerable. Evidently, channel steerability in the above sense is equivalent to steerability of the (normalized) state K BAC by Alice [35].\\nIn our case, channel steerability implies that Alice, by performing measurements, can steer the mapping between Bob and Charlie. As entanglement is a prerequisite for steerability [13], here, entanglement of Œ• ABC in the splitting A : BC is a prerequisite for Alice being able to steer Bob and Charlie. In line with our above considerations, we thus obtain the following Proposition: Proposition 5. If the comb Œ• ABC is steerable by Alice acting on A, then the initial system environment state œÅ AR is steerable by Alice acting on A.\\nProof. Using different instruments (labelled by x) with outcomes a, Alice can create a state assemblage {œÅ a|x R } a|x of the state œÅ R = tr A (œÅ AR ), i.e., œÅ a|x R = œÅ AR E a|x A and a œÅ a|x R = œÅ R for all choices of instruments x. Let us assume that the state œÅ AR is unsteerable on Alice's side. This means that, for any state assemblage {œÅ a|x R } a|x , there exists a set {œÅ Œª R } of subnormalized states, probabilities p(Œª) and conditional probabilities p(a|x, Œª), such that\\nœÅ a|x R = Œª p(Œª)p(a|x, Œª)œÅ Œª R .(30)\\nUsing this identity, we see that for any conditional mapping Œ• a|x BC between Bob and Alice, we have\\nŒ• a|x BC = œÅ x|a R L BRC = Œª p(Œª)p(a|x, Œª)œÅ Œª R L BRC := Œª p(Œª)p(a|x, Œª)L Œª BC .(31)\\nIt is easy to check that {L Œª BC } constitutes an instrument, implying that any channel assemblage {Œ• a|x BC } is unsteerable if œÅ AR is unsteerable on Alice's side. This, in turn, means that if the broadcast channel Œ• ABC is steerable by Alice, then so is œÅ AR . Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb Œ• ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that Œ• ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently.\\nHaving analysed necessary and sufficient conditions on the dynamical building blocks for entanglement in different splittings of combs, as well as the interpretation of these quantum correlations, we finish our discussion of the bipartite case, providing a connection between biseparable combs and entanglement breaking channels.\",\n",
       "    'n_publication_ref': 8,\n",
       "    'n_figure_ref': 2},\n",
       "   {'heading': 'Separability and entanglement breaking channels',\n",
       "    'text': 'As we have seen in Sec. 4.1, separability of a comb Œ• ABC in the splittings AB : C and AC : B is directly related to the entanglement breaking property of a map (given by Eq. (26)) that arises naturally from the building blocks of the comb. Such a relation between the separability of a comb Œ• ABC in a given splitting and entanglement breaking (EB) maps within the circuit that yields said comb can be made more concrete, complementing the results of Sec. 4.1. Specifically, here we investigate the question if the presence of an entanglement breaking map on one of the wires (see Fig. 9 for a graphical representation) implies separability of Œ• ABC in a given splitting, and vice versa. We give an affirmative answer for the splittings C : AB and provide a partial resolution for the cases A : BC and B : AC.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Entanglement breaking channels imply separable combs',\n",
       "    'text': 'First, it is straightforward to show that the presence of an EB channel on any of the wires leads to a comb Œ• ABC that is separable with respect to a particular splitting. Concretely, we will say that a comb Œ• ABC can be represented with an EB channel on one of its wires, if it admits a decomposition with an EB channel. For example, for the wire R, this would mean that Œ• ABC can be written as Œ• ABC = œÅ AR N EB R R L BRC (see Fig. 9), where N EB R R is the Choi state of an entanglement breaking channel. With this, we have the following Proposition: Proposition 6. If a comb Œ• ABC can be represented with an entanglement breaking channel on one of its wires, then it is separable in at least one possible splitting. In particular, an entanglement breaking channel on A or R implies E(A : BC) = 0, on B implies E(B : AC) = 0, and on C implies E(C : AB) = 0.\\nThe proof can be found in App. B. For the case of an EB channel on R, within the study of quantum memory, a similar Proposition has been shown in [32]. There, processes on two times that only display classical memory were defined as those that have an EB channel on R. Additionally, related results can be found in [34], where entanglement breaking superchannels and their representations are analysed. Here, we provide direct simple proofs that will also allow us to show the converse for one of the cases and to pinpoint the difficulties with proving the converse of the other two.\\nFrom the above Proposition, it is evident, that a circuit with at least two entanglement breaking channels in its representation (as long as they are not on A and R) is separable with respect to two distinct splittings. While this fact in itself does not yet imply that it is fully separable, we have the following Lemma: Lemma 7. If a circuit can be represented with an EB channel on any two of the wires {A/R, B, C} it is fully separable.\\nThe proof can be found in App. C. While an EB channel on one of the wires always leads to a comb that is separable in the corresponding splitting, it is a priori unclear, if every separable comb Œ• ABC has a representation that contains an EB channel on the correct wire.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': '',\n",
       "    'text': 'First, for better book-keeping of the involved spaces, we will label the respective input and output spaces of the entanglement breaking maps differently, i.e, we have N EB C ‚ÜíC : B(H C ) ‚Üí B(H C ) and N EB A ‚ÜíA : B(H R ) ‚Üí B(H R ). Their Choi matrices are then denoted by N EB C C and N EB A A , respectively. Additionally, in order for the resulting comb Œ• ABC to be defined on the spaces ABC, we add some primes to the spaces the building blocks of Œ• ABC are defined on (see Fig. 9). These relabellings are a mere notational aid and have no bearing on the results.\\nAs each entanglement breaking channel can be represented as a measurement followed by a repreparation, we have\\nX } Œ± is a collection of quantum states, and X ‚àà {C, A}. With this, a comb Œ• ABC with an entanglement breaking channel on C (and building blocks œÅ AR , L BRC ) is given by\\nwhich, as\\nC > 0 is separable in the splitting C : AB. Analogously, a comb Œ• ABC resulting from a circuit with an entanglement breaking channel on the wire A is given by\\nwhich -for the same reason as above -is separable in the splitting A : BC. The remaining cases follow in a similar vein.\\nNote that Eqs. ( 59) and ( 60) provide the most general form of a comb that contains an EB channel on the respective wire.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'C Proof of Lem. 7',\n",
       "    'text': 'Here, we provide a proof of Lem. 7 in the main text: Lemma 7. If a circuit can be represented with an EB channel on any two of the wires {A/R, B, C} it is fully separable.\\nProof. We present an explicit proof for the case of entanglement breaking channels on the wires A and B. the other cases follow analogously. In this case, the resulting comb Œ• ABC can be written as (see Fig. 9):\\nwhere N EB A A and M EB BB are entanglement breaking channels. Consequently, each of them can be decomposed as\\nB , where {E \\nwhere p(Œ±) = tr(œÅ A R E (Œ±)T A\\n), and Œæ\\nAdditionally, we can show directly, that Œ• ABC satisfies the sufficient conditions for entanglement in the splittings AB : C and AC : B laid out in Sec. 4.1. There, we showed that if the Choi matrix L BRC [tr A (œÅ AR )] is entangled, then Œ• ABC is entangled with respect to both of the splittings AB : C and AC : B. In our case, L BRC tr A (œÅ AR ) reads (with spaces arranged in the order BC)\\nThen, from Eq. ( 72) we have\\nwhich is (proportional to) an entangled state. Consequently, the maximally tripartite entangled comb Œ• ABC we found satisfies all of the necessary and sufficient conditions for bipartite entanglement we discussed in the main text.\\nE Non-zero tangle for the process in Eq. (50) with n = 3\\nIn order to see that this process has indeed a non-zero tangle note first that if œÑ (œÅ) = 0, then also œÑ (AœÅA ‚Ä† / tr(A ‚Ä† AœÅ)) = 0 with A = ‚äó i A i and A i ‚àà GL(2, C) [62,65]. However, as we will show there exist local invertible operators that transform Œ• (3) to a state which has non-zero tangle and therefore also œÑ (Œ• (3) ) = 0. These operators can be chosen to be A 1 = A 2 = Diag(1/ ‚àö Œ± , ‚àö Œ± ) and A 3 = Diag(Œ±, 1/Œ±) with 0 < Œ± < 1/ ‚àö 3 and Diag(x, y) denoting a diagonal matrix with entries x and y. The transformed state is of the form\\nNote that for convenience we used here the normalization for states. This state can be detected for example for Œ± < 1/ ‚àö 3 to be not in the W-class by using the witness [63] W = 3/41 ‚àí |GHZ 3 GHZ 3 | and therefore has non-zero tangle. Hence, also Œ• (3) is not in the W-class.',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Genuine Multipartite Entanglement in Time',\n",
       "  'abstract': 'While spatial quantum correlations have been studied in great detail, much less is known about the genuine quantum correlations that can be exhibited by temporal processes. Employing the quantum comb formalism, processes in time can be mapped onto quantum states, with the crucial difference that temporal correlations have to satisfy causal ordering, while their spatial counterpart is not constrained in the same way. Here, we exploit this equivalence and use the tools of multipartite entanglement theory to provide a comprehensive picture of the structure of correlations that (causally ordered) temporal quantum processes can display. First, focusing on the case of a process that is probed at two points in time -which can equivalently be described by a tripartite quantum statewe provide necessary as well as sufficient conditions for the presence of bipartite entanglement in different splittings. Next, we connect these scenarios to the previously studied concepts of quantum memory, entanglement breaking superchannels, and quantum steering, thus providing both a physical interpretation for entanglement in temporal quantum processes, and a determination of the resources required for its creation. Additionally, we construct explicit examples of W-type and GHZ-type genuinely multipartite entangled two-time processes and prove that genuine multipartite entanglement in temporal processes can be an emergent phenomenon. Finally, we show that genuinely entangled processes across multiple times exist for any number of probing times.',\n",
       "  'paddleOCR': [[[[246.0, 11.0], [302.0, 17.0], [296.0, 70.0], [240.0, 64.0]],\n",
       "    [\"R'\", 0.9847578406333923]],\n",
       "   [[[589.0, 19.0], [636.0, 19.0], [636.0, 67.0], [589.0, 67.0]],\n",
       "    ['R', 0.9995871186256409]],\n",
       "   [[[399.0, 51.0], [482.0, 51.0], [482.0, 100.0], [399.0, 100.0]],\n",
       "    ['EB', 0.9994239211082458]],\n",
       "   [[[99.0, 76.0], [145.0, 78.0], [141.0, 219.0], [94.0, 217.0]],\n",
       "    [\"PA'R'\", 0.9901915788650513]],\n",
       "   [[[763.0, 107.0], [820.0, 107.0], [820.0, 172.0], [763.0, 172.0]],\n",
       "    ['L', 0.9597309231758118]],\n",
       "   [[[165.0, 156.0], [232.0, 156.0], [232.0, 208.0], [165.0, 208.0]],\n",
       "    [\"A'\", 0.9673219919204712]],\n",
       "   [[[643.0, 162.0], [714.0, 152.0], [721.0, 201.0], [651.0, 211.0]],\n",
       "    [\"B'\", 0.9516098499298096]],\n",
       "   [[[864.0, 168.0], [919.0, 150.0], [930.0, 181.0], [875.0, 199.0]],\n",
       "    ['C', 0.6994652152061462]],\n",
       "   [[[250.0, 189.0], [335.0, 193.0], [333.0, 242.0], [248.0, 239.0]],\n",
       "    ['EB', 0.9993143081665039]],\n",
       "   [[[545.0, 191.0], [631.0, 191.0], [631.0, 239.0], [545.0, 239.0]],\n",
       "    ['EB', 0.8571491241455078]],\n",
       "   [[[950.0, 192.0], [1035.0, 192.0], [1035.0, 241.0], [950.0, 241.0]],\n",
       "    ['EB', 0.9990522265434265]],\n",
       "   [[[1048.0, 211.0], [1090.0, 211.0], [1090.0, 267.0], [1048.0, 267.0]],\n",
       "    ['7', 0.7594781517982483]],\n",
       "   [[[487.0, 277.0], [523.0, 277.0], [523.0, 336.0], [487.0, 336.0]],\n",
       "    ['T', 0.5274481177330017]],\n",
       "   [[[350.0, 341.0], [393.0, 341.0], [393.0, 387.0], [350.0, 387.0]],\n",
       "    ['A', 0.9998718500137329]],\n",
       "   [[[481.0, 341.0], [524.0, 341.0], [524.0, 386.0], [481.0, 386.0]],\n",
       "    ['B', 0.9974780678749084]],\n",
       "   [[[1054.0, 337.0], [1100.0, 337.0], [1100.0, 387.0], [1054.0, 387.0]],\n",
       "    ['C', 0.905934751033783]]]},\n",
       " '2012.05647v1-Figure2-1.png': {'ocr': [[[1201.0, 240.0],\n",
       "    [1414.0, 249.0],\n",
       "    [1412.0, 296.0],\n",
       "    [1199.0, 287.0]],\n",
       "   ['Scattering', 0.9999353289604187]],\n",
       "  'True_Statements': ['Sparse intensity measurements are sent to neural network.',\n",
       "   'Reward decides the NN parameters update in Neural Network.'],\n",
       "  'False_Statements': ['Sparse intensity measurements are sent to the reward function.',\n",
       "   'Reward decide the scattering update in Neural Network.'],\n",
       "  'Flowchart-to-Caption': 'Figure 2: Schematic of the specific reinforcement learning loop used for each phase pattern of the training data set, with a particular example of desired phase pattern.',\n",
       "  'caption': 'Figure 2: Schematic of the specific reinforcement learning loop used for each phase pattern of the training data set, with a particular example of desired phase pattern.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2012.05647v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Specific quasi-reinforcement learning',\n",
       "    'text': \"Generally speaking, reinforcement learning is learning by interacting with an environment which rewards action made by an agent. Basically, from an observable environment state, the agent chooses to take an action in the environment for which it is rewarded accordingly. The way in which the agent chooses action is called a policy. Obviously, the agent aims at increasing the reward it receives and so must learn an optimal policy for interacting with the environment [15,16].\\nIn our particular case, the environment at the k th step, consists in the n laser fields in the array n k z \\uf0ce , the user-defined desired signals a \\uf0ce , as it is concerned, is a signal correction resulting from the agent's policy whom relevance is assessed from the reward k r . For that purpose, we chose as a reward the following resemblance parameter \\uf028 \\uf029 which is usually named phasing quality in the context of laser coherent beam combining. The reward is maximum and equals one if and only if arg( ) arg( )\\nkk az \\uf03d\\nup to a constant phase. Finally, the agent's interaction with the environment, since the action is just a phase correction, can be expressed as\\n\\uf028 \\uf029 \\uf028 \\uf029 \\uf028 \\uf029 arg arg 1 dk i z a kk z z e \\uf02d \\uf02b \\uf03d\\uf0d7\\n.\\nAs mentioned before, reinforcement learning approach proposes to find the agent's policy from the parametric family of neural network functions. The main difficulty in the general case for reinforcement learning is that we do not know the correct action In effect, during the learning process, known array phase patterns feed the process, so that z k is known and so that reward can be computed\\n\\uf028 \\uf029 , k k k r R z a \\uf03d\\n. We observed that if the agent is trained to maximize the reward at each iteration, for a fixed total number of iterations T , then actions\\nk a are such that \\uf028 \\uf029 \\uf028 \\uf029 lim arg arg kd k zz \\uf0ae\\uf0a5 \\uf03d\\nup to a constant. To emphasize the fact that it is not the classical case, we call it quasi-reinforcement learning (QRL). A simplified picture of the learning scheme is given in Fig. 2.\\nIn practice, in our simulations, to achieve a wavefront setting with a rms accuracy\\n/ 30 \\uf06c \\uf0a3 ( max 0.96 k r \\uf0b3\\n) [17], T must be greater than a critical value (typically 4 to 8) which depends on the number of beams n and on the number of measurements m . More details are provided in the next section.\\nNote, we can either predict a phases vector or, directly, real and imaginary parts to build an action .\",\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Experimental phase control of a 100 laser beam array with quasi-reinforcement learning of a neural network in an error reduction loop',\n",
       "  'abstract': 'An innovative scheme is proposed for the dynamic control of phase in two-dimensional laser beam array. It is based on a simple neural network that predicts the complex field array from the intensity of the induced scattered pattern through a phase intensity transformer made of a diffuser. Iterated phase corrections are applied on the laser field array by phase modulators via a feedback loop to set the array to prescribed phase values. A crucial feature is the use of a kind of reinforcement learning approach for the neural network training which takes account of the iterated corrections. Experiments on a proof of concept system demonstrated the high performance and scalability of the scheme with an array of up to 100 laser beams and a phase setting at \\uf06c/30.Recently, the number of applications requiring laser beam of high average power has strongly increased, addressing large projects such as space cleaning [1], spacecraft propulsion [2], particle acceleration [3], as well as industrial processes [4] or defense systems [5]. Laser beam combining is one of the most studied approach to reach very high power level, in particular the coherent beam combining (CBC) techniques [6]. They aim to phase lock the emission of a tiled laser beam array delivered by a network of amplifiers to generate a synthetic beam of high brightness. As the phase relationships between the beams in the array evolves over time in an actual laser system, especially in fiber laser system, these techniques have to correct the phase deviations from a synthetic plane wave in real time via a servo loop. CBC techniques have been widely developed in recent years, exploring different approaches to adjust the individual phases in the synthetic discrete wavefront. They can be classified in two broad categories. In the first one, the phase relationships of the beams in the array are measured and then corrected [7]. In the second one, the discrepancy between the actual wavefront and the desired wavefront is compensated in an iterative process [8]. In the latter case, an optimization algorithm drives the feedback loop, analyzing more global data on the array phase state from interference between all of the beams [9,10]. These techniques are often simpler to implement, with less electronic devices, at the expense of a more complex numerical processing and for some of them, at the cost of a lower speed for a large number of beams. This last issue is connected with the',\n",
       "  'paddleOCR': [[[[93.0, 60.0], [219.0, 60.0], [219.0, 93.0], [93.0, 93.0]],\n",
       "    ['KNOWN', 0.9988471269607544]],\n",
       "   [[[109.0, 102.0], [204.0, 102.0], [204.0, 131.0], [109.0, 131.0]],\n",
       "    ['INPUT', 0.9988307952880859]],\n",
       "   [[[431.0, 201.0], [546.0, 201.0], [546.0, 241.0], [431.0, 241.0]],\n",
       "    ['Spatial', 0.9985460042953491]],\n",
       "   [[[703.0, 192.0], [757.0, 205.0], [748.0, 244.0], [694.0, 231.0]],\n",
       "    ['Zk', 0.8515685200691223]],\n",
       "   [[[852.0, 222.0], [994.0, 222.0], [994.0, 262.0], [852.0, 262.0]],\n",
       "    ['for k=1', 0.9995078444480896]],\n",
       "   [[[442.0, 246.0], [539.0, 256.0], [535.0, 292.0], [438.0, 283.0]],\n",
       "    ['Phase', 0.9998558759689331]],\n",
       "   [[[1201.0, 240.0], [1414.0, 249.0], [1412.0, 296.0], [1199.0, 287.0]],\n",
       "    ['Scattering', 0.9999353289604187]],\n",
       "   [[[888.0, 284.0], [975.0, 284.0], [975.0, 320.0], [888.0, 320.0]],\n",
       "    ['to T', 0.9042180180549622]],\n",
       "   [[[394.0, 295.0], [584.0, 301.0], [582.0, 336.0], [393.0, 329.0]],\n",
       "    ['Modulation', 0.9999362826347351]],\n",
       "   [[[51.0, 387.0], [256.0, 396.0], [254.0, 436.0], [49.0, 428.0]],\n",
       "    ['beam array', 0.9972014427185059]],\n",
       "   [[[77.0, 441.0], [222.0, 450.0], [220.0, 491.0], [74.0, 481.0]],\n",
       "    ['training', 0.9509185552597046]],\n",
       "   [[[32.0, 501.0], [271.0, 501.0], [271.0, 542.0], [32.0, 542.0]],\n",
       "    ['phase pattern', 0.9825060963630676]],\n",
       "   [[[1014.0, 509.0], [1064.0, 509.0], [1064.0, 556.0], [1014.0, 556.0]],\n",
       "    ['^k', 0.8352476358413696]],\n",
       "   [[[1180.0, 503.0], [1497.0, 514.0], [1495.0, 566.0], [1178.0, 555.0]],\n",
       "    ['NN parameters', 0.9995505809783936]],\n",
       "   [[[670.0, 538.0], [824.0, 538.0], [824.0, 579.0], [670.0, 579.0]],\n",
       "    ['Reward', 0.9999533295631409]],\n",
       "   [[[1271.0, 575.0], [1407.0, 570.0], [1408.0, 613.0], [1272.0, 618.0]],\n",
       "    ['update', 0.9886501431465149]],\n",
       "   [[[807.0, 643.0], [858.0, 651.0], [852.0, 684.0], [801.0, 676.0]],\n",
       "    ['Z d', 0.845851480960846]],\n",
       "   [[[880.0, 735.0], [1012.0, 735.0], [1012.0, 770.0], [880.0, 770.0]],\n",
       "    ['desired', 0.9998737573623657]],\n",
       "   [[[878.0, 789.0], [1121.0, 789.0], [1121.0, 829.0], [878.0, 829.0]],\n",
       "    ['phase pattern', 0.9642329812049866]],\n",
       "   [[[1659.0, 868.0], [1781.0, 868.0], [1781.0, 908.0], [1659.0, 908.0]],\n",
       "    ['Sparse', 0.9999632239341736]],\n",
       "   [[[189.0, 891.0], [303.0, 896.0], [301.0, 936.0], [187.0, 931.0]],\n",
       "    ['Phase', 0.9998167157173157]],\n",
       "   [[[1253.0, 889.0], [1394.0, 889.0], [1394.0, 932.0], [1253.0, 932.0]],\n",
       "    ['Neural', 0.9995068907737732]],\n",
       "   [[[1640.0, 916.0], [1794.0, 923.0], [1792.0, 965.0], [1638.0, 958.0]],\n",
       "    ['intensity', 0.9997901916503906]],\n",
       "   [[[144.0, 949.0], [348.0, 949.0], [348.0, 988.0], [144.0, 988.0]],\n",
       "    ['corrections', 0.9999116659164429]],\n",
       "   [[[1234.0, 951.0], [1416.0, 951.0], [1416.0, 991.0], [1234.0, 991.0]],\n",
       "    ['Network', 0.9998806715011597]],\n",
       "   [[[1036.0, 972.0], [1091.0, 993.0], [1077.0, 1029.0], [1022.0, 1007.0]],\n",
       "    ['dk', 0.7059798240661621]],\n",
       "   [[[1487.0, 968.0], [1532.0, 968.0], [1532.0, 1011.0], [1487.0, 1011.0]],\n",
       "    ['bk', 0.9546093940734863]],\n",
       "   [[[1588.0, 982.0], [1847.0, 982.0], [1847.0, 1009.0], [1588.0, 1009.0]],\n",
       "    ['measurements', 0.9996915459632874]]]},\n",
       " '2011.03307v2-Figure11-1.png': {'ocr': [[[563.0, 556.0],\n",
       "    [794.0, 556.0],\n",
       "    [794.0, 582.0],\n",
       "    [563.0, 582.0]],\n",
       "   ['Accretion Disk', 0.9997926354408264]],\n",
       "  'True_Statements': ['Above a spinning black hole, X-rays are emitted isotropically.',\n",
       "   'Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole.'],\n",
       "  'False_Statements': ['Above a static black hole, X-rays are emitted isotropically.',\n",
       "   'Because of the compact corona very close to the black hole, the majority of the photons does not hit the accretion disc.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high- to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation.',\n",
       "  'caption': 'Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high- to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2011.03307v2-Figure11-1.png',\n",
       "  'sections': [{'heading': 'Changing partial covering fractions causing',\n",
       "    'text': 'large-amplitude and ultra-soft count-rate variations\\nThe most important result of the analysis presented here is that the major source of variability observed in the spectrum can be explained by variation of the covering fraction of the absorber.\\nOur spectral analysis shows that the variation of the X-ray spectrum is consistent with changes induced by a partial absorber of varying covering factor and constant column density in front of the X-ray-emitting corona and accretion disc. As expected, the covering fraction is increasing significantly with decreasing flux of the source. With N H = 12 +6 ‚àí4 √ó10 22 cm ‚àí2 , the equivalent hydrogen column density of the partial coverer is consistent with that seen in typical AGN absorption events. Markowitz et al. (2014) find peak N H column densities of 4-26 √ó10 22 cm ‚àí2 in the largest sample of cloud obscuration events. Studying the long-term Xray spectral variability of a sample of 20 Compton-thin type II galaxies, Laha et al. (2020) find 11 sources that require a partialcovering obscuring component in all or some of the observations. Not only are the N H ranges quoted in both studies fully consistent with our derived value, but also the presence of a varying partial cover seems to be present in a significant fraction of AGNs.\\nWe note that there has been controversial discussion of whether a partial coverer in 1H0707‚àí495 can explain the strong 7 keV edge (e.g. Fabian et al. 2004;Gallo et al. 2004;Done et al. 2007). In our model, the partial coverer does not explain the 7 keV edge. While this edge is mainly modelled by relativistically smeared reflection from the accretion disc, our partial covering model describes the varying absorption in the soft X-rays. In this paper we combine relativistic reflection very close to the black hole, that is, at a few R G , with partial covering occurring at larger distances up to a few hundred R G . From analyses of much longer observations (Dauser et al. 2012;Kosec et al. 2018), it is known that a strongly ionised wind is present in 1H0707‚àí495. The absorption feature around 0.8 keV is evidence that this outflow is also present in the low flux state of our observation (c.f. Fig. 9). The wind is not detected in the higher flux states, as the outflowing winds are strongly flux dependent as shown for example by Parker et al. (2017) and Reeves et al. (2018). As the existence of such an ultra-fast outflow (UFO) has been shown to be connected with the observed partial covering in other sources (e.g. PDS 456, Reeves et al. 2018), it is possible that the observed partial covering in the soft X-rays is connected to these previously detected UFOs. This absorption is likely connected to or even directly caused by the UFO detected previously. The UFO will also affect the Fe-K region around 7 keV (Kosec et al. 2018), but was not detected in our observations because of the lower S/N.\\nThe change in partial covering fractions combined with UFO features may also explain the observed shape of the NEV spectra. On short timescales, the absorber is likely driving the variability, which is probably because of the small variations in ionisation and covering fraction as the material passes along the line of sight. As seen in Fig. 10, the absorber seems to affect the spectral shape between 0.3 and 4 keV, which explains why these energy bins have higher NEV values. In particular, most of the variability is seen below 0.8keV, in agreement with what is seen in the light curve.\\nThe NEV spectra also reveal very little variability in the 0.8-2.0 keV and 4-8 keV bands. This may be explained by the presence of UFO features in these energy bands. The outflow may be more stable on shorter timescales, instead varying on longer timescales. This behaviour would suppress the variability on short timescales in these energy ranges, explaining the drops in the NEV (c.f. Sect. 5.3 for a more detailed discussion on the connection between outflowing winds and partial covering).\\nHowever, at the same time, we also measure a change in the ionisation parameter of the relativistic reflection component. We consider it unlikely that this change of ionisation is indicative of changes in the accretion disc, and is probably rather caused by the simplified (neutral) absorber model. As discussed in Sect. 5.1, the data do not allow us to constrain the ionisation of the absorber.\\nDue to lack of additional information, such as the ionisation of the absorber, it is not easy to estimate the distance and size of the obscuring cloud. Given the short timescale of the putative absorption event and the strong change of the covering fraction within 20-40 ks, the absorber will probably be much closer to the X-ray source than the BLR (see Sect. 5.3). This short distance makes it very likely that it will be partly ionised. However, ionised absorbers are more transparent in the soft X-rays than neutral absorbers and therefore show leakage effects in the soft X-rays. The change in log Œæ of the reflector seen here mainly affects the soft X-rays, and thus might mimic this effect of ionised absorption. We note that longer observations of such a partly obscured state would be necessary to constrain more detailed ionised absorption models for the partial coverer 3 .\\nAn illustration of the changing partial coverer scenario with relativistic reflection is shown in Fig. 11. Because of gravitational light bending, the majority of the photons emitted from the corona are bent towards the black hole and onto the accretion disc in approximately equal parts (c.f. Fig. 1 and 2 of Dauser et al. 2014b)). While in the high-flux state we have an unobscured view onto the inner parts of the accretion disc, partially covering clouds absorb the reflected spectrum in the lower flux states with increasing covering fraction for a decreasing observed soft X-ray flux.',\n",
       "    'n_publication_ref': 13,\n",
       "    'n_figure_ref': 4},\n",
       "   {'heading': 'Partially covering absorbers and ultra-fast outflows',\n",
       "    'text': \"Outflowing winds launched from the accretion disc by radiation pressure or magnetic fields are considered as an important AGN feedback process. For radiation-pressure-dominated winds, outflows can reach velocities up to about 0.3 c and can drive sub-A&A proofs: manuscript no. 39316corr Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high-to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation. stantial amounts of material into the interstellar medium. These winds have been discovered mainly based on XMM-Newton observations (e.g. Pounds et al. 2003a,b,c;King & Pounds 2003;Reeves et al. 2003). Outflowing winds with such high velocities have been named as UFOs by Tombesi et al. (2010) in a systematic study of bright XMM-Newton AGNs.\\nMultiple outflow absorption lines have been detected in one of the most variable AGNs, IRAS 13224‚àí3809, by (Parker et al. 2017). These latter authors argue that the X-ray emission from within a few gravitational radii of the black hole is ionising the disc winds up to hundreds of R G . It was also shown that the outflow absorption lines are strongly flux dependent, with strongest being found in the low-flux state and weakest in the high-flux state, which is due to increasing ionisation towards higher flux values. When the ionisation becomes sufficiently high, the outflow may become 'over-ionised' and may no longer be visible. Such a scenario was also discussed by Gallo et al. (2019), where absorption features were detected in the beginning of a flare in the NLS1 Mrk 335 but not in the brightest prolonged flare states.\\nIonised outflowing winds have been connected to absorbing partial covering by for example Reeves et al. (2018) and references therein. These latter authors argue that the outflowing wind is inhomogeneous and more complex than a simple homogeneous outflow, which is capable of partially covering the X-ray source. In this scenario, the X-ray absorption depends on the ionisation state, the distance of the absorber, and the covering fraction.\\nThe two XMM-Newton observations of PDS 456 reported by Reeves et al. (2018) were made over two consecutive satellite orbits. These latter authors argue that much of the spectral variability between the observations appears to be reproduced by the variability of the low-ionisation partial covering absorber, which is primarily driven by a change of the covering fraction. This appears consistent with the low-flux states and the varying covering fractions reported for PDS 456 and now for 1H 0707‚àí495 in this paper.\\nPartial covering absorbers have been put into context with UFOs and winds in several other papers. Reeves et al. (2020) provide a further exploration for the spectral shape and vari-ability of PDS 456, noting in particular the significant differences in the soft-band fit when using neutral and ionised partialcovering components. There are many other works that analyse the soft-and hard-band emission and absorption features in PDS 456, concluding that an outflowing absorber can explain these features as well as the observed variability (e.g. Matzeu et al. 2016b,a;Parker et al. 2018).\\nA larger sample of Seyfert galaxies analysed in Tombesi et al. (2013) also reveals that many AGNs that display UFO signatures also show evidence for warm absorption, and based on their observed properties, these latter authors propose that these may actually be part of a single large-scale outflow. Simultaneous observations of absorption and outflowing components are also presented for individual sources, including Mrk 335 (Longinotti et al. 2019, but see also Gallo et al. 2019) and PG 1211+143 (Pounds et al. 2016). This lends further support to the idea that such components may be physically linked and appear simultaneously, as in the observations presented in this work.\",\n",
       "    'n_publication_ref': 15,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Extreme ultra-soft X-ray variability in an eROSITA observation of the narrow-line Seyfert 1 galaxy 1H 0707‚àí495',\n",
       "  'abstract': 'The ultra-soft narrow-line Seyfert 1 galaxy 1H 0707‚àí495 is a well-known and highly variable active galactic nucleus (AGN), with a complex, steep X-ray spectrum, and has been studied extensively with XMM-Newton. 1H 0707‚àí495 was observed with the extended ROentgen Survey with an Imaging Telescope Array (eROSITA) aboard the Spectrum-Roentgen-Gamma (SRG) mission on October 11, 2019, for about 60,000 seconds as one of the first calibration and pointed verification phase (CalPV) observations. The eROSITA light curves show significant variability in the form of a flux decrease by a factor of 58 with a 1 œÉ error confidence interval between 31 and 235. This variability is primarily in the soft band, and is much less extreme in the hard band. No strong ultraviolet variability has been detected in simultaneous XMM-Newton Optical Monitor observations. The UV emission is L UV ‚âà 10 44 erg s ‚àí1 , close to the Eddington limit. 1H 0707‚àí495 entered the lowest hard flux state seen in 20 years of XMM-Newton observations. In the eROSITA All-Sky Survey (eRASS) observations taken in April 2020, the X-ray light curve is still more variable in the ultra-soft band, but with increased soft and hard band count rates more similar to previously observed flux states. A model including relativistic reflection and a variable partial covering absorber is able to fit the spectra and provides a possible explanation for the extreme light-curve behaviour. The absorber is probably ionised and therefore more transparent to soft X-rays. This leaks soft X-rays in varying amounts, leading to large-amplitude soft-X-ray variability.',\n",
       "  'paddleOCR': [[[[677.0, 3.0], [780.0, 3.0], [780.0, 30.0], [677.0, 30.0]],\n",
       "    ['Partial', 0.9997181296348572]],\n",
       "   [[[675.0, 41.0], [822.0, 46.0], [821.0, 76.0], [674.0, 71.0]],\n",
       "    ['Covering', 0.999758780002594]],\n",
       "   [[[676.0, 82.0], [826.0, 86.0], [825.0, 116.0], [676.0, 113.0]],\n",
       "    ['Absorber', 0.9996666312217712]],\n",
       "   [[[613.0, 125.0], [780.0, 128.0], [779.0, 154.0], [613.0, 151.0]],\n",
       "    ['(20-1000Rg)', 0.9903290271759033]],\n",
       "   [[[124.0, 412.0], [212.0, 418.0], [209.0, 449.0], [122.0, 442.0]],\n",
       "    ['X-ray', 0.9987174868583679]],\n",
       "   [[[93.0, 453.0], [210.0, 456.0], [209.0, 485.0], [93.0, 483.0]],\n",
       "    ['Corona', 0.9995678067207336]],\n",
       "   [[[208.0, 529.0], [298.0, 532.0], [297.0, 564.0], [207.0, 561.0]],\n",
       "    ['Black', 0.9837309718132019]],\n",
       "   [[[563.0, 556.0], [794.0, 556.0], [794.0, 582.0], [563.0, 582.0]],\n",
       "    ['Accretion Disk', 0.9997926354408264]],\n",
       "   [[[214.0, 573.0], [292.0, 573.0], [292.0, 604.0], [214.0, 604.0]],\n",
       "    ['Hole', 0.999854564666748]]]},\n",
       " '1356505-Figure1-1.png': {'caption': 'Figure 1: Local model with neural attention. Inputs: context word vectors, candidate entity priors and embeddings. Outputs: entity scores. All parts are differentiable and trainable with backpropagation.',\n",
       "  'imageText': [],\n",
       "  'image_file': '1356505-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Context Scores.',\n",
       "    'text': 'Let us assume that we have computed a mention-entity priorp(e|m) (procedure detailed in Section 6). In addition, for each mention m, a pruned candidate set Œì(m) of at most S entities has been identified. Our model, depicted in Figure 1, computes a score for each e ‚àà Œì(m) based on the K-word local context c = {w 1 , . . . , w K } surrounding m, as well as on the prior. It is a composition of differentiable functions, thus it is smooth from input to output, allowing us to easily compute gradients and backpropagate through it.\\nEach word w ‚àà c and entity e ‚àà Œì(m) is mapped to its embedding via the pre-trained map x (cf. Section 3). We then compute an unnormalized support score for each word in the context as follows:\\nu(w) = max e‚ààŒì(m) x e Ax w (2\\n)\\nwhere A is a parameterized diagonal matrix. The weight is high if the word is strongly related to at least one candidate entity. We often observe that uninformative words (e.g. similar to stop words) receive non-negligible scores which add undesired noise to our local context model. As a consequence, we (hard) prune to the top R ‚â§ K words with the highest scores 2 and apply a softmax function on these weights. Define the reduced context:c\\n= {w ‚àà c|u(w) ‚àà topR(u)} (3)\\nThen, the final attention weights are explicitly\\nŒ≤(w) = exp[u(w)] v‚ààc exp[u(v)] . if w ‚ààc 0 otherwise.(4)\\nFinally, we define a Œ≤-weighted context-based entity-mention score via\\nŒ®(e, c) = w‚ààc Œ≤(w) x e B x w (5\\n)\\nwhere B is another trainable diagonal matrix. We will later use the same architecture for the unary scores of our global ED model. Local Score Combination.\\nWe integrate these context scores with the context-independent scores encoded inp(e|m). \\nWe find a flexible choice for f to be important and superior to a na√Øve weighted average combination model. We therefore use a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities, which we regularize as suggested in (Denton et al., 2015) by constraining the sum of squares of all weights in the linear layer. We use standard projected SGD for training. The same network is also used in Section 5.\\nPrediction is done independently for each mention m i and context c i by maximizing the Œ®(e, m i , c i ) score.\\nLearning the Local Model.\\nEntity and word embeddings are pre-trained as discussed in Section 3. Thus, the only learnable parameters are the diagonal matrices A and B, plus the parameters of f . Having few parameters helps to avoid overfitting and to be able to train with little annotated data. We assume that a set of known mention-entity pairs {(m, e * )} with their respective context windows have been extracted from a corpus. For model fitting, we then utilize a max-margin loss that ranks ground truth entities higher than other candidate entities. This leads us to the objective:\\nŒ∏ * = arg min Œ∏ D‚ààD m‚ààD e‚ààŒì(m)\\ng(e, m),\\ng(e, m)\\n:= [Œ≥ ‚àí Œ®(e * , m, c) + Œ®(e, m, c)] +\\nwhere Œ≥ > 0 is a margin parameter and D is a training set of entity annotated documents. We aim to find a Œ® (i.e. parameterized by Œ∏) such that the score of the correct entity e * referenced by m is at least a margin Œ≥ higher than that of any other candidate entity e. Whenever this is not the case, the margin violation becomes the experienced loss.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Deep Joint Entity Disambiguation with Local Neural Attention',\n",
       "  'abstract': 'We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or stateof-the-art accuracy at moderate computational costs.',\n",
       "  'paddleOCR': [[[[699.0, 4.0], [940.0, 7.0], [940.0, 33.0], [699.0, 30.0]],\n",
       "    ['context embedding Xc', 0.9901671409606934]],\n",
       "   [[[349.0, 55.0], [499.0, 55.0], [499.0, 80.0], [349.0, 80.0]],\n",
       "    ['weighted sum', 0.999875009059906]],\n",
       "   [[[1236.0, 174.0], [1323.0, 174.0], [1323.0, 198.0], [1236.0, 198.0]],\n",
       "    ['Output:', 0.9999006390571594]],\n",
       "   [[[123.0, 209.0], [331.0, 209.0], [331.0, 229.0], [123.0, 229.0]],\n",
       "    ['embedding matrix B', 0.9836894273757935]],\n",
       "   [[[846.0, 201.0], [1005.0, 201.0], [1005.0, 225.0], [846.0, 225.0]],\n",
       "    ['entity - context', 0.9879834651947021]],\n",
       "   [[[1196.0, 202.0], [1362.0, 202.0], [1362.0, 226.0], [1196.0, 226.0]],\n",
       "    ['candidate entity', 0.9899100065231323]],\n",
       "   [[[893.0, 232.0], [967.0, 232.0], [967.0, 252.0], [893.0, 252.0]],\n",
       "    ['scores', 0.9995428919792175]],\n",
       "   [[[1244.0, 235.0], [1316.0, 235.0], [1316.0, 255.0], [1244.0, 255.0]],\n",
       "    ['scores', 0.9992296695709229]],\n",
       "   [[[873.0, 262.0], [1348.0, 262.0], [1348.0, 294.0], [873.0, 294.0]],\n",
       "    ['=< xc,xe>->f-> (e,m,c)', 0.8653895854949951]],\n",
       "   [[[426.0, 284.0], [521.0, 288.0], [520.0, 313.0], [425.0, 310.0]],\n",
       "    ['softmax', 0.9995197653770447]],\n",
       "   [[[195.0, 331.0], [349.0, 331.0], [349.0, 356.0], [195.0, 356.0]],\n",
       "    ['word attention', 0.9992579817771912]],\n",
       "   [[[378.0, 351.0], [391.0, 351.0], [391.0, 360.0], [378.0, 360.0]],\n",
       "    ['X', 0.7577298879623413]],\n",
       "   [[[469.0, 349.0], [499.0, 349.0], [499.0, 366.0], [469.0, 366.0]],\n",
       "    ['8', 0.7474040389060974]],\n",
       "   [[[227.0, 361.0], [312.0, 361.0], [312.0, 386.0], [227.0, 386.0]],\n",
       "    ['weights', 0.9998054504394531]],\n",
       "   [[[384.0, 380.0], [667.0, 381.0], [667.0, 407.0], [384.0, 405.0]],\n",
       "    ['hard attention (keep top R)', 0.9829733967781067]],\n",
       "   [[[385.0, 487.0], [672.0, 487.0], [672.0, 511.0], [385.0, 511.0]],\n",
       "    ['soft attention: max(column)', 0.999918520450592]],\n",
       "   [[[811.0, 595.0], [836.0, 595.0], [836.0, 612.0], [811.0, 612.0]],\n",
       "    ['e1', 0.9387235641479492]],\n",
       "   [[[860.0, 595.0], [887.0, 595.0], [887.0, 613.0], [860.0, 613.0]],\n",
       "    ['e2', 0.962265133857727]],\n",
       "   [[[978.0, 592.0], [1008.0, 592.0], [1008.0, 616.0], [978.0, 616.0]],\n",
       "    ['es', 0.9985852241516113]],\n",
       "   [[[33.0, 623.0], [113.0, 623.0], [113.0, 647.0], [33.0, 647.0]],\n",
       "    ['W1 W2', 0.9417324066162109]],\n",
       "   [[[193.0, 623.0], [231.0, 623.0], [231.0, 644.0], [193.0, 644.0]],\n",
       "    ['WK', 0.974189281463623]],\n",
       "   [[[870.0, 626.0], [938.0, 626.0], [938.0, 647.0], [870.0, 647.0]],\n",
       "    ['Input:', 0.9998998045921326]],\n",
       "   [[[1112.0, 623.0], [1182.0, 623.0], [1182.0, 649.0], [1112.0, 649.0]],\n",
       "    ['Input:', 0.999946117401123]],\n",
       "   [[[100.0, 652.0], [170.0, 652.0], [170.0, 677.0], [100.0, 677.0]],\n",
       "    ['Input:', 0.9999461770057678]],\n",
       "   [[[767.0, 654.0], [1047.0, 654.0], [1047.0, 674.0], [767.0, 674.0]],\n",
       "    ['pre-trained embeddings of', 0.9999215602874756]],\n",
       "   [[[1085.0, 652.0], [1211.0, 652.0], [1211.0, 676.0], [1085.0, 676.0]],\n",
       "    ['entity priors', 0.9998924732208252]],\n",
       "   [[[10.0, 680.0], [262.0, 680.0], [262.0, 704.0], [10.0, 704.0]],\n",
       "    ['pre-trained embeddings.', 0.973578929901123]],\n",
       "   [[[764.0, 680.0], [1044.0, 682.0], [1044.0, 706.0], [764.0, 704.0]],\n",
       "    ['mention candidate entities', 0.9725322127342224]],\n",
       "   [[[1071.0, 684.0], [1224.0, 687.0], [1223.0, 722.0], [1071.0, 718.0]],\n",
       "    ['log p(e|m)', 0.9685745239257812]],\n",
       "   [[[46.0, 707.0], [223.0, 709.0], [222.0, 733.0], [46.0, 731.0]],\n",
       "    ['of context words', 0.9970519542694092]],\n",
       "   [[[649.0, 804.0], [715.0, 824.0], [708.0, 846.0], [643.0, 825.0]],\n",
       "    ['Axwi', 0.7449318766593933]],\n",
       "   [[[217.0, 819.0], [425.0, 819.0], [425.0, 839.0], [217.0, 839.0]],\n",
       "    ['embedding matrix A.', 0.969843327999115]]],\n",
       "  'ocr': [[[1244.0, 235.0], [1316.0, 235.0], [1316.0, 255.0], [1244.0, 255.0]],\n",
       "   ['scores', 0.9992296695709229]]},\n",
       " '2109.09214v1-Figure2-1.png': {'caption': 'Fig. 2. The architecture of the proposed transfer learning process.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2109.09214v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'IV. METHODOLOGY',\n",
       "    'text': \"Problem 1 is solved by leveraging SCM to comformally map between the teacher's and the learner's command domains. Problem 2 is addressed by constraining the teacher's control and planning policy in accordance with the learner's limitation. The block diagram in Fig. 2 shows the architecture of the whole process. The remainder of this section describes the details of the components of the proposed approach. \",\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'A Conformal Mapping-based Framework for Robot-to-Robot and Sim-to-Real Transfer Learning',\n",
       "  'abstract': \"This paper presents a novel method for transferring motion planning and control policies between a teacher and a learner robot. With this work, we propose to reduce the sim-to-real gap, transfer knowledge designed for a specific system into a different robot, and compensate for system aging and failures. To solve this problem we introduce a Schwarz-Christoffel mapping-based method to geometrically stretch and fit the control inputs from the teacher into the learner command space. We also propose a method based on primitive motion generation to create motion plans and control inputs compatible with the learner's capabilities. Our approach is validated with simulations and experiments with different robotic systems navigating occluding environments.\",\n",
       "  'paddleOCR': [[[[604.0, 21.0], [652.0, 28.0], [647.0, 59.0], [599.0, 52.0]],\n",
       "    ['UL', 0.8123829364776611]],\n",
       "   [[[156.0, 48.0], [495.0, 53.0], [495.0, 78.0], [155.0, 73.0]],\n",
       "    ['Schwarz-Christoffel Mapping', 0.999596357345581]],\n",
       "   [[[18.0, 151.0], [71.0, 158.0], [65.0, 192.0], [13.0, 184.0]],\n",
       "    ['UT', 0.9056233167648315]],\n",
       "   [[[611.0, 152.0], [656.0, 159.0], [650.0, 191.0], [605.0, 183.0]],\n",
       "    ['XL', 0.7280603647232056]],\n",
       "   [[[155.0, 183.0], [496.0, 183.0], [496.0, 204.0], [155.0, 204.0]],\n",
       "    ['Teacher Planner & Controller', 0.9928077459335327]],\n",
       "   [[[723.0, 180.0], [827.0, 182.0], [826.0, 208.0], [722.0, 206.0]],\n",
       "    ['Learner', 0.9997142553329468]]],\n",
       "  'ocr': [[[156.0, 48.0], [495.0, 53.0], [495.0, 78.0], [155.0, 73.0]],\n",
       "   ['Schwarz-Christoffel Mapping', 0.999596357345581]]},\n",
       " '12030503-Figure3-1.png': {'caption': 'Fig. 3. Training our explanation model. Our explanation model differs from other caption models because it (1) includes the object category as an additional input and (2) incorporates a reinforcement learning based discriminative loss',\n",
       "  'imageText': ['Image',\n",
       "   'Category:',\n",
       "   'Cardinal',\n",
       "   'Sentence',\n",
       "   'ClassifierConcat',\n",
       "   'Compact',\n",
       "   'Bilinear',\n",
       "   'Feature',\n",
       "   'orange',\n",
       "   'beak.‚Äù',\n",
       "   'Target',\n",
       "   'Sentence',\n",
       "   '‚Äúa',\n",
       "   'bright',\n",
       "   'red',\n",
       "   'bird',\n",
       "   'with',\n",
       "   'an',\n",
       "   'Loss',\n",
       "   'Cross',\n",
       "   'Entropy',\n",
       "   'Reward',\n",
       "   'Function',\n",
       "   'cheeks.‚Äù',\n",
       "   'Sampled',\n",
       "   'Sentence:',\n",
       "   '‚Äúa',\n",
       "   'red',\n",
       "   'bird',\n",
       "   'with',\n",
       "   'black',\n",
       "   'Discriminative',\n",
       "   'Loss',\n",
       "   'wT-1:',\n",
       "   'beak',\n",
       "   'w1:',\n",
       "   'a',\n",
       "   'w0:',\n",
       "   '<SOS>',\n",
       "   'p(wT|w0:T-1,I,C)',\n",
       "   'p(w2|w0:1,I,C)',\n",
       "   'LS',\n",
       "   'TM',\n",
       "   'LS',\n",
       "   'TM',\n",
       "   'Relevance',\n",
       "   'Loss',\n",
       "   'p(w1|w0,I,C)',\n",
       "   'p(w2|w0:1,I,C)',\n",
       "   '‚Ä¶',\n",
       "   'p(wT|w0:T-1,I,C)',\n",
       "   'Deep',\n",
       "   'Finegrained',\n",
       "   'Classifier',\n",
       "   'Compact',\n",
       "   'Bilinear',\n",
       "   'Classifier',\n",
       "   'p(w1|w0,I,C)',\n",
       "   'LS',\n",
       "   'TM',\n",
       "   'LS',\n",
       "   'TM',\n",
       "   'LS',\n",
       "   'TM',\n",
       "   'LS',\n",
       "   'TM'],\n",
       "  'image_file': '12030503-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Visual Explanation Model',\n",
       "    'text': 'Our visual explanation model (Figure 3) aims to produce an explanation which (1) describes visual content present in a specific image instance and (2) contains appropriate information to explain why an image instance belongs to a specific category. We ensure generated descriptions meet these two requirements for explanation by including both a relevance loss (Figure 3, bottom right) and discriminative loss (Figure 3, top right). Our main technical contribution is the inclusion of a loss which acts on sampled word sequences during training. Our proposed loss enables us to enforce global sentence constraints on sentences and by applying our loss to sampled sentences, we ensure that the final output of our system fulfills our criteria for an explanation. In the following sections we consider a sentence to be a word sequence comprising either a complete sentence or a sentence fragment.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 3},\n",
       "   {'heading': 'Relevance Loss',\n",
       "    'text': 'Image relevance can be accomplished by training a visual description model. Our model is based on LRCN [8], which consists of a convolutional neural network, which extracts powerful high level visual features, and two stacked recurrent networks (specifically LSTMs), which learn how to generate a description conditioned on visual features. During inference, the first LSTM receives the previously generated word w t‚àí1 as input (at time t = 0 the model receives a \"start-of-sentence\" token), and produces an output l t . The second LSTM, receives the output of the first LSTM l t as well as an image feature f and produces a probability distribution p(w t ) over the next word. At each time step, the word w t is generated by sampling from the distribution p(w t ). Generation continues until an \"end-of-sentence\" token is generated. We propose two modifications to the LRCN framework to increase the image relevance of generated sequences (Figure 3, top left). First, our explanation model uses category predictions as an additional input to the second LSTM in the sentence generation model. Intuitively, category information can help inform the caption generation model which words and attributes are more likely to occur in a description. For example, if the caption generation model conditioned only on images mistakes a red eye for a red eyebrow, category level information could indicate the red eye is more likely for a given class. We experimented with a few methods to represent class labels, but found a vector representation in which we first train a language model, e.g., an LSTM, to generate word sequences conditioned on images, then compute the average hidden state of the LSTM across all sequences for all classes in the train set worked best. Second, we use rich category specific features [3] to generate relevant explanations.\\nEach training instance consists of an image, category label, and a ground truth sentence. During training, the model receives the ground truth word w t for each time step t ‚àà T . We define the relevance loss as:\\nL R = 1 N N ‚àí1 n=0 T ‚àí1 t=0 log p(w t+1 |w 0:t , I, C)\\nwhere w t is a ground truth word, I is the image, C is the category, and N is the batch size. By training the model to predict each word in a ground truth sentence, the model is trained to produce sentences which correspond to image content. However, this loss does not explicitly encourage generated sentences to discuss discerning visual properties. In order to generate sentences which are both image relevant and category specific, we include a discriminative loss to focus sentence generation on discriminative visual properties of an image.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Generating Visual Explanations',\n",
       "  'abstract': 'Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.',\n",
       "  'paddleOCR': [[[[60.0, 25.0], [384.0, 25.0], [384.0, 47.0], [60.0, 47.0]],\n",
       "    ['Deep Finegrained Classifier', 0.9858424663543701]],\n",
       "   [[[502.0, 19.0], [642.0, 19.0], [642.0, 41.0], [502.0, 41.0]],\n",
       "    ['Image Category:', 0.9869371652603149]],\n",
       "   [[[1091.0, 19.0], [1322.0, 20.0], [1322.0, 42.0], [1090.0, 41.0]],\n",
       "    ['Discriminative Loss', 0.9970090985298157]],\n",
       "   [[[538.0, 42.0], [612.0, 42.0], [612.0, 64.0], [538.0, 64.0]],\n",
       "    ['Cardinal', 0.9997314214706421]],\n",
       "   [[[1319.0, 60.0], [1390.0, 60.0], [1390.0, 82.0], [1319.0, 82.0]],\n",
       "    ['Reward', 0.9995782375335693]],\n",
       "   [[[160.0, 94.0], [239.0, 98.0], [239.0, 120.0], [159.0, 116.0]],\n",
       "    ['Compact', 0.9996139407157898]],\n",
       "   [[[1319.0, 85.0], [1391.0, 85.0], [1391.0, 102.0], [1319.0, 102.0]],\n",
       "    ['Function', 0.9973341226577759]],\n",
       "   [[[308.0, 105.0], [454.0, 105.0], [454.0, 125.0], [308.0, 125.0]],\n",
       "    ['Compact Bilinear', 0.9950394630432129]],\n",
       "   [[[165.0, 115.0], [234.0, 121.0], [232.0, 142.0], [163.0, 136.0]],\n",
       "    ['Bilinear', 0.9876025915145874]],\n",
       "   [[[346.0, 129.0], [415.0, 129.0], [415.0, 151.0], [346.0, 151.0]],\n",
       "    ['Feature', 0.9694817662239075]],\n",
       "   [[[1024.0, 130.0], [1190.0, 132.0], [1190.0, 154.0], [1024.0, 152.0]],\n",
       "    ['Sampled Sentence:', 0.9998919367790222]],\n",
       "   [[[161.0, 143.0], [238.0, 143.0], [238.0, 160.0], [161.0, 160.0]],\n",
       "    ['Classifier', 0.997454047203064]],\n",
       "   [[[639.0, 154.0], [707.0, 154.0], [707.0, 176.0], [639.0, 176.0]],\n",
       "    ['Concat', 0.9997417330741882]],\n",
       "   [[[1023.0, 155.0], [1194.0, 155.0], [1194.0, 176.0], [1023.0, 176.0]],\n",
       "    ['\"a red bird with black.', 0.9715707302093506]],\n",
       "   [[[1247.0, 146.0], [1325.0, 146.0], [1325.0, 163.0], [1247.0, 163.0]],\n",
       "    ['Sentence', 0.9986723065376282]],\n",
       "   [[[1246.0, 163.0], [1328.0, 167.0], [1328.0, 188.0], [1245.0, 185.0]],\n",
       "    ['Classifier', 0.9995091557502747]],\n",
       "   [[[1072.0, 179.0], [1146.0, 179.0], [1146.0, 196.0], [1072.0, 196.0]],\n",
       "    ['cheeks.\"', 0.9920015335083008]],\n",
       "   [[[624.0, 218.0], [647.0, 218.0], [647.0, 274.0], [624.0, 274.0]],\n",
       "    ['LSTM', 0.9886965751647949]],\n",
       "   [[[514.0, 229.0], [576.0, 229.0], [576.0, 251.0], [514.0, 251.0]],\n",
       "    ['<SOS>', 0.9762542843818665]],\n",
       "   [[[696.0, 220.0], [719.0, 220.0], [719.0, 273.0], [696.0, 273.0]],\n",
       "    ['LSTM', 0.9936115741729736]],\n",
       "   [[[480.0, 235.0], [502.0, 235.0], [502.0, 254.0], [480.0, 254.0]],\n",
       "    ['NO:', 0.7266700267791748]],\n",
       "   [[[782.0, 232.0], [885.0, 229.0], [886.0, 255.0], [783.0, 259.0]],\n",
       "    ['p(w,|w,,I,C)', 0.9004395008087158]],\n",
       "   [[[623.0, 270.0], [648.0, 270.0], [648.0, 359.0], [623.0, 359.0]],\n",
       "    ['LSTM', 0.7444961667060852]],\n",
       "   [[[704.0, 274.0], [716.0, 274.0], [716.0, 303.0], [704.0, 303.0]],\n",
       "    ['4', 0.8984180092811584]],\n",
       "   [[[699.0, 301.0], [722.0, 301.0], [722.0, 361.0], [699.0, 361.0]],\n",
       "    ['LSTM', 0.9937279224395752]],\n",
       "   [[[469.0, 320.0], [530.0, 316.0], [532.0, 345.0], [470.0, 349.0]],\n",
       "    ['w: a', 0.9780398607254028]],\n",
       "   [[[781.0, 320.0], [890.0, 320.0], [890.0, 347.0], [781.0, 347.0]],\n",
       "    ['p(w,|Wo:1,I,C)', 0.8517941236495972]],\n",
       "   [[[1122.0, 323.0], [1307.0, 323.0], [1307.0, 345.0], [1122.0, 345.0]],\n",
       "    ['Relevance Loss', 0.9924582242965698]],\n",
       "   [[[513.0, 369.0], [540.0, 369.0], [540.0, 433.0], [513.0, 433.0]],\n",
       "    ['...', 0.9987847805023193]],\n",
       "   [[[629.0, 368.0], [650.0, 369.0], [646.0, 436.0], [625.0, 435.0]],\n",
       "    ['...', 0.9882403016090393]],\n",
       "   [[[701.0, 370.0], [722.0, 370.0], [722.0, 431.0], [701.0, 431.0]],\n",
       "    ['...', 0.9925987720489502]],\n",
       "   [[[817.0, 369.0], [844.0, 369.0], [844.0, 431.0], [817.0, 431.0]],\n",
       "    ['...', 0.9986016750335693]],\n",
       "   [[[1071.0, 362.0], [1172.0, 362.0], [1172.0, 384.0], [1071.0, 384.0]],\n",
       "    ['p(w,|w,,1,C)', 0.840130090713501]],\n",
       "   [[[1332.0, 370.0], [1389.0, 374.0], [1387.0, 396.0], [1331.0, 392.0]],\n",
       "    ['Cross', 0.9990893602371216]],\n",
       "   [[[1063.0, 384.0], [1179.0, 381.0], [1180.0, 407.0], [1064.0, 411.0]],\n",
       "    ['p(w|wo;1,I,C)', 0.8568944931030273]],\n",
       "   [[[1326.0, 393.0], [1395.0, 397.0], [1394.0, 419.0], [1325.0, 415.0]],\n",
       "    ['Entropy', 0.9994061589241028]],\n",
       "   [[[1337.0, 417.0], [1384.0, 417.0], [1384.0, 441.0], [1337.0, 441.0]],\n",
       "    ['Loss', 0.9989813566207886]],\n",
       "   [[[1057.0, 427.0], [1186.0, 423.0], [1187.0, 456.0], [1058.0, 460.0]],\n",
       "    ['p(wq|W0:T-1,1,C)', 0.6987703442573547]],\n",
       "   [[[622.0, 437.0], [645.0, 435.0], [649.0, 490.0], [626.0, 491.0]],\n",
       "    ['LSTM', 0.9924657344818115]],\n",
       "   [[[699.0, 439.0], [720.0, 439.0], [720.0, 492.0], [699.0, 492.0]],\n",
       "    ['LSTM', 0.9978554844856262]],\n",
       "   [[[473.0, 457.0], [515.0, 462.0], [513.0, 480.0], [472.0, 476.0]],\n",
       "    ['WT-1', 0.757865309715271]],\n",
       "   [[[523.0, 456.0], [569.0, 456.0], [569.0, 474.0], [523.0, 474.0]],\n",
       "    ['beak', 0.996721625328064]],\n",
       "   [[[773.0, 455.0], [896.0, 453.0], [897.0, 481.0], [773.0, 483.0]],\n",
       "    ['p(w|Wo:r-1,I,C)', 0.8329691290855408]],\n",
       "   [[[62.0, 474.0], [202.0, 474.0], [202.0, 494.0], [62.0, 494.0]],\n",
       "    ['Target Sentence', 0.9935082793235779]],\n",
       "   [[[33.0, 496.0], [233.0, 496.0], [233.0, 516.0], [33.0, 516.0]],\n",
       "    ['\"a bright red bird with an', 0.9626122713088989]],\n",
       "   [[[74.0, 521.0], [194.0, 521.0], [194.0, 541.0], [74.0, 541.0]],\n",
       "    ['orange beak.\".', 0.9655827283859253]]],\n",
       "  'ocr': [[[699.0, 301.0], [722.0, 301.0], [722.0, 361.0], [699.0, 361.0]],\n",
       "   ['LSTM', 0.9937279224395752]]},\n",
       " '212717935-Figure1-1.png': {'caption': 'Figure 1. The mResUNet framework with decoding (red dashed box) and encoding phases (green dashed box). Each gray coloured box in these phases represents a convolution block. We change the number of filters and the map size by down sampling (red arrows) and up sampling (green arrows) the feature maps in the encoding and the decoding phases, respectively. The convolution block has four sub-stages where convolution operations are applied with different dilation rates of N = 1, 2, 3 and 4. All sub-stages have convolution, activation and batch normalization layers, and residual connections are applied between the input and output feature maps. The sub-stages of convolution blocks in decoding phase have an extra dropout layer to prevent model over-fitting. Skip connections are used to concatenate feature maps from the encoding convolution blocks to corresponding blocks in decoding phase that helps in retrieving the lost spatial information due to down sampling (see Section 2.1).',\n",
       "  'imageText': ['{',\n",
       "   '{',\n",
       "   '{{',\n",
       "   'mResUNet',\n",
       "   'Framework',\n",
       "   'Encoding',\n",
       "   'Decoding',\n",
       "   'R',\n",
       "   'esidual',\n",
       "   'C',\n",
       "   'onnection',\n",
       "   'io',\n",
       "   'n',\n",
       "   'ne',\n",
       "   'ct',\n",
       "   'l',\n",
       "   'C',\n",
       "   'on',\n",
       "   'id',\n",
       "   'ua',\n",
       "   'R',\n",
       "   'es',\n",
       "   'Concatenation',\n",
       "   'for',\n",
       "   'dilation',\n",
       "   'rates:',\n",
       "   '2,',\n",
       "   '4',\n",
       "   'Concatenation',\n",
       "   'for',\n",
       "   'dilation',\n",
       "   'rates:',\n",
       "   '2,',\n",
       "   '4',\n",
       "   'Concatenation',\n",
       "   'for',\n",
       "   'dilation',\n",
       "   'rates:',\n",
       "   '2,',\n",
       "   '4',\n",
       "   'For',\n",
       "   'each',\n",
       "   'sub-stage',\n",
       "   'dilation',\n",
       "   'rates:',\n",
       "   '4,',\n",
       "   '3,',\n",
       "   '2,',\n",
       "   '1',\n",
       "   'For',\n",
       "   'each',\n",
       "   'box,',\n",
       "   '4',\n",
       "   'sub-',\n",
       "   'stages',\n",
       "   'with',\n",
       "   'For',\n",
       "   'each',\n",
       "   'sub-stage',\n",
       "   'dilation',\n",
       "   'rates:',\n",
       "   '1,',\n",
       "   '2,',\n",
       "   '3,',\n",
       "   '4',\n",
       "   'For',\n",
       "   'each',\n",
       "   'box,',\n",
       "   '4',\n",
       "   'sub-',\n",
       "   'stages',\n",
       "   'with',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '512',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '52',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '256',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '102',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '128',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '202',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '64',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '402',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '256',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '102',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '128',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '202',\n",
       "   'NÔ¨Ålter',\n",
       "   '=',\n",
       "   '64',\n",
       "   'Map',\n",
       "   'size',\n",
       "   '=',\n",
       "   '402',\n",
       "   'Output',\n",
       "   'SZE',\n",
       "   'ProÔ¨Åle',\n",
       "   'Cluster',\n",
       "   'Mass',\n",
       "   'Input',\n",
       "   'Map',\n",
       "   '1',\n",
       "   'x',\n",
       "   '402',\n",
       "   'Addition',\n",
       "   'BatchNorm',\n",
       "   'Activation',\n",
       "   'Conv',\n",
       "   '3x3',\n",
       "   'Dropout',\n",
       "   'Input',\n",
       "   'Addition',\n",
       "   'BatchNorm',\n",
       "   'Activation',\n",
       "   'Conv',\n",
       "   '3x3',\n",
       "   '3',\n",
       "   'Input'],\n",
       "  'image_file': '212717935-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Deep Learning Model',\n",
       "    'text': 'In recent years, deep learning algorithms have been extensively used in range of astrophysical and cosmological problems (e.g. George & Huerta 2018;Mathuriya et al. 2018;Allen et al. 2019;Bottrell et al. 2019;Alexander et al. 2019;Fluri et al. 2019). Recent studies have applied deep learning (Ntampaka et al. 2019;Ho et al. 2019) and machine learning (e.g. Ntampaka et al. 2015;Armitage et al. 2019;Green et al. 2019) algorithms to estimate galaxy cluster masses using mock X-ray and velocity dispersion observations. These studies found that these techniques produce more accurate X-ray and dynamical mass estimates than conventional methods.\\nIn this work, we apply the mResUNet algorithm to extract the SZ profiles and the cluster masses from the simulated microwave sky maps. ResUNet is a feed-forward deep learning algorithm that was first introduced for segmentation of medical images (Kayalibay et al. 2017) and to extract roads from maps (Zhang et al. 2018), and later applied to a number of problems. The original algorithm was modified by Caldeira et al. (2019) to do image to image regression, i.e. get an output image that is a continous function of the input image. We implement further modifications to the network to extract small and large scale features in the map. This modified ResUNet, or mResUNet, algorithm is well suited to astrophysical problems, such as the current use case of estimating the SZ signal from an image of the sky.\\nThe mResUNet is a convolutional neural network and its basic building block is a convolution layer which performs discrete convolutions (see Gu et al. 2015, for a recent review). The aim of the convolution layer is to learn features of an input map. Convolutional neural networks assume that nearby pixels are more strongly correlated than the distant ones. The features of nearby pixels are extracted using filters that are applied to a set of neighbouring pixels. This set of neighbouring pixels is also called the receptive field. The filter applied to a set of pixels is typically a k √ó k array with k = 1, 3, 5, ..., and the size of the filter (k √ó k) is denoted as the kernel size. A filter with a given kernel-size is moved across the image from top left to bottom right and at each point in the image a convolution operation is performed to generate an output. Several such filters are used in a convolution layer to extract information about different aspects of the input image. For instance, one filter can be associated to the central region of the galaxy cluster and rest of the filters could extract information from the other parts of cluster. The filters can extract information across different length scales by using different dilation rates instead of increasing the kernel size. A dilation rate of N stretches the receptive field by k +(k ‚àí1)(N ‚àí1), thus doubling the dilation rate will increase the receptive field to 5 √ó 5 for k=3. These dilated convolutions systematically aggregate multi-scale contextual information without losing resolution (Yu & Koltun 2015).\\nThe total receptive field increases for each pixel of the input image as we stack several convolution layers in the network. An activation function is applied after each convolution layer, which is desirable to detect non-linear features and results into a highly non-linear reconstruction of input image (see Nwankpa et al. 2018, for a recent review). Each convolution layer produces a feature map for a given input image. The feature map  1. The mResUNet framework with decoding (red dashed box) and encoding phases (green dashed box). Each gray coloured box in these phases represents a convolution block. We change the number of filters and the map size by down sampling (red arrows) and up sampling (green arrows) the feature maps in the encoding and the decoding phases, respectively. The convolution block has four sub-stages where convolution operations are applied with different dilation rates of N = 1, 2, 3 and 4. All sub-stages have convolution, activation and batch normalization layers, and residual connections are applied between the input and output feature maps. The sub-stages of convolution blocks in decoding phase have an extra dropout layer to prevent model over-fitting. Skip connections are used to concatenate feature maps from the encoding convolution blocks to corresponding blocks in decoding phase that helps in retrieving the lost spatial information due to down sampling (see Section 2.1).\\n(f l ) for a convolution layer (l) is obtained by convolving the input from a previous layer (x l‚àí1 ) with a learned kernel, such that, the feature value at location (i, j) is written as\\nf i,j l = w T l x i,j l‚àí1 + b l ,(1)\\nwhere w l is the weight vector and b l is the bias term. The weights are optimized using gradient descent (e.g. Ruder 2016) that involves back-propagation from the final output, back to each layer in reverse order to update the weights. The mResUNet architecture used in this work has following main components.\\n1. We base our architecture on the encoder-decoder paradigm. This consists of a contracting path (encoder) to capture features, a symmetric expanding path (decoder) that enables precise localization and a bridge between these two. Figure 1 shows the full UNet framework, where the red and the green dashed lines point to encoding and decoding frameworks, respectively.\\n2. Each grey coloured box corresponds to a convolution block. We increase the filter size from 64 to 512 and use strides (e.g. Dumoulin & Visin 2016) to reduce the size of feature map by half whenever filter size is doubled (red arrows) during the encoding phase of the network. This process is known as down sampling by striding. For the decoding phase, we increase the size of feature map by up sampling (green arrows). Each convolution block has 4 sub-stages where convolution operations are applied with different dilation rates of N = 1, 2, 3 and 4, while keeping the stride length to unity, whenever dilation rate is not 1. This improves the performance by identifying correlations between different locations in the image (e.g. Yu & Koltun 2015;Chen et al. 2016Chen et al. , 2017.\\n3. The feature maps from two sub-stages (dilation rates N=2, 4) of first three encoding convolution blocks are cross concatenated with the corresponding maps from decoding blocks using skip connec-tions. These connections are useful to retrieve the spatial information lost due to striding operations (e.g. Drozdzal et al. 2016).\\n4. Each sub-stage of encoding and decoding convolution blocks has fixed number of layers. Among these the convolution, the activation and the batch normalization layers are present in all sub-stages. The batch normalization layer which is helpful in improving the speed, stability and performance of the network (Ioffe & Szegedy 2015). The input to these layers is always added to its output, as shown by the connection between input and addition layers. Such connections are called residual connections (He et al. 2015) and they are known to improve the performance of the network (e.g. Zhang et al. 2018;Caldeira et al. 2019).\\n5. A large feed-forward neural network when trained on a small set of data, typically performs poorly on the test data due to over-fitting. This problem can be reduced by randomly omitting some of the features during the training phase by adding dropout layers to the network (Hinton et al. 2012). We add dropout layers to the decoding phase of the network.',\n",
       "    'n_publication_ref': 28,\n",
       "    'n_figure_ref': 2},\n",
       "   {'heading': 'TRAINING AND OPTIMISATION',\n",
       "    'text': 'The mResUNet model described in Section 2.1 and Figure 1 takes images as input and outputs same sized images after passing through several convolutional blocks. This process is repeated for a number of epochs, where one epoch is when entire training data are passed through the neural network once. The data are divided into three parts: training, validation and test sets.\\nThe training dataset includes images of the microwave sky simulations of SZ clusters, the corresponding true SZ profiles and the true mass of clusters. As described in Section 2.2, both CMB maps and SZ profiles have a characteristic 20% log-normal SZ-mass scatter and all CMB maps have Gaussian random realizations of CMB. To make these simulations more realistic, we add foregrounds, 5 ¬µK-arcmin white noise and 1 beam smoothing to these maps. We normalize all maps, so that, the minimum and maximum pixel value is between -1 and 1, respectively, to improve the performance of network. This is done by dividing the image pixels by a constant factor across all cluster masses. Our training data has 400 maps for each cluster and corresponding labels (true SZ profiles and true mass of clusters). For training, we only take cluster simulations with M 200c = (1, 2, 3, 4, 5, 6, 7, 8)√ó10 14 M and leave others for testing the model. The test datasets are never used in the training phase and are kept separately to analyse the trained model. We keep 200 CMB temperature maps and corresponding labels for testing. In addition to the cluster M 200c used in training, we test our model for cluster masses that were not the part of training or validation process ,that is, clusters with M 200c = (0.5, 0.75, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 9, 10)√ó10 14 M .\\nThe maps from the training set are passed through the neural networks with a batch size of 4 and a training loss is computed as mean-squared-error (MSE) between the predicted and the true labels after each batch. Batch after batch, the weights of the network are updated using the gradient descent and the back-propagation (see Section 2.1). In this work, we use Adam optimizer (an algorithm for first-order gradient-based optimization, see Kingma & Ba 2014) with an initial learning rate of 0.001. After each epoch, the validation loss (or validation MSE) is calculated and we change the learning rate by imple-menting callbacks during the training, such that, the learning rate is reduced to half if the validation loss does not improve for five consecutive epochs. In addition, to avoid over-fitting, we set a dropout rate of 0.3 in the encoding phase of the network. We consider the network to be trained and stop the training process, if the validation loss does not improve for fifteen epochs.\\nEvery convolution block in encoding, bridging and decoding phase has a convolution layer, an activation layer and a batch normalization layer. The kernel-size of each convolution layer is set to 3 √ó 3 and we change stride length from 1 to 2, whenever filter size is doubled. All activation layers in the network have Scale Exponential Linear Unit (SELU Klambauer et al. 2017) activation functions which induce sellf-normalizing properties, such that, activations close to zero mean and unit variance converge towards zero mean and unit variance, when propagated through many network layers, even under the presence of noise and perturbations. Only for the final layer, linear (or identity) activation function is used to get same sized output images as inputs. The network has approximately 16 million parameters and is trained on a single GPU using Keras with a TensorFlow backend.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': \"Mass Estimation of Galaxy Clusters with Deep Learning I: Sunyaev-Zel'dovich Effect\",\n",
       "  'abstract': \"We present a new application of deep learning to infer the masses of galaxy clusters directly from images of the microwave sky. Effectively, this is a novel approach to determining the scaling relation between a cluster's Sunyaev-Zel'dovich (SZ) effect signal and mass. The deep learning algorithm used is mResUNet, which is a modified feed-forward deep learning algorithm that broadly combines residual learning, convolution layers with different dilation rates, image regression activation and a U-Net framework. We train and test the deep learning model using simulated images of the microwave sky that include signals from the cosmic microwave background (CMB), dusty and radio galaxies, instrumental noise as well as the cluster's own SZ signal. The simulated cluster sample covers the mass range 1√ó10 14 M < M 200c < 8√ó10 14 M at z = 0.7. The trained model estimates the cluster masses with a 1 œÉ uncertainty ‚àÜM/M ‚â§ 0.2, consistent with the input scatter on the SZ signal of 20%. We verify that the model works for realistic SZ profiles even when trained on azimuthally symmetric SZ profiles by using the Magneticum hydrodynamical simulations.\",\n",
       "  'paddleOCR': [[[[700.0, 73.0],\n",
       "     [1158.0, 73.0],\n",
       "     [1158.0, 102.0],\n",
       "     [700.0, 102.0]],\n",
       "    ['mResUNet Framework', 0.9993455410003662]],\n",
       "   [[[977.0, 141.0], [1175.0, 141.0], [1175.0, 176.0], [977.0, 176.0]],\n",
       "    ['Output SZE', 0.9997275471687317]],\n",
       "   [[[1249.0, 159.0], [1478.0, 164.0], [1477.0, 199.0], [1248.0, 195.0]],\n",
       "    ['Cluster Mass', 0.984586238861084]],\n",
       "   [[[1016.0, 180.0], [1136.0, 185.0], [1134.0, 222.0], [1015.0, 217.0]],\n",
       "    ['Profile', 0.9996896386146545]],\n",
       "   [[[543.0, 213.0], [719.0, 218.0], [718.0, 255.0], [542.0, 251.0]],\n",
       "    ['Input Map', 0.999889612197876]],\n",
       "   [[[574.0, 259.0], [692.0, 259.0], [692.0, 296.0], [574.0, 296.0]],\n",
       "    ['1 x 402', 0.9976789355278015]],\n",
       "   [[[616.0, 298.0], [641.0, 298.0], [641.0, 328.0], [616.0, 328.0]],\n",
       "    ['1', 0.7471562623977661]],\n",
       "   [[[547.0, 358.0], [711.0, 363.0], [710.0, 400.0], [546.0, 396.0]],\n",
       "    ['Encoding', 0.9998533725738525]],\n",
       "   [[[1524.0, 354.0], [1835.0, 354.0], [1835.0, 383.0], [1524.0, 383.0]],\n",
       "    ['For each box, 4 sub.', 0.9712091684341431]],\n",
       "   [[[1159.0, 366.0], [1323.0, 374.0], [1321.0, 411.0], [1158.0, 404.0]],\n",
       "    ['Decoding', 0.9993898272514343]],\n",
       "   [[[1587.0, 394.0], [1778.0, 389.0], [1779.0, 425.0], [1588.0, 429.0]],\n",
       "    ['stages with', 0.9967765212059021]],\n",
       "   [[[21.0, 419.0], [330.0, 419.0], [330.0, 448.0], [21.0, 448.0]],\n",
       "    ['For each box, 4 sub', 0.9891895651817322]],\n",
       "   [[[824.0, 435.0], [1032.0, 435.0], [1032.0, 462.0], [824.0, 462.0]],\n",
       "    ['Concatenation for.', 0.9647769927978516]],\n",
       "   [[[1579.0, 435.0], [1795.0, 435.0], [1795.0, 464.0], [1579.0, 464.0]],\n",
       "    ['dilation rates:', 0.9941463470458984]],\n",
       "   [[[86.0, 458.0], [275.0, 454.0], [276.0, 489.0], [87.0, 494.0]],\n",
       "    ['stages with', 0.9719394445419312]],\n",
       "   [[[553.0, 452.0], [708.0, 452.0], [708.0, 487.0], [553.0, 487.0]],\n",
       "    ['Nrilter = 64', 0.9796614050865173]],\n",
       "   [[[826.0, 462.0], [1032.0, 462.0], [1032.0, 491.0], [826.0, 491.0]],\n",
       "    ['dilation rates: 2, 4', 0.9844388961791992]],\n",
       "   [[[1156.0, 456.0], [1312.0, 456.0], [1312.0, 491.0], [1156.0, 491.0]],\n",
       "    ['Nrilter = 64', 0.9473485946655273]],\n",
       "   [[[1617.0, 473.0], [1753.0, 473.0], [1753.0, 508.0], [1617.0, 508.0]],\n",
       "    ['4, 3, 2, 1', 0.9880808591842651]],\n",
       "   [[[72.0, 493.0], [295.0, 498.0], [294.0, 533.0], [71.0, 528.0]],\n",
       "    ['dilation rates:', 0.9903313517570496]],\n",
       "   [[[511.0, 491.0], [744.0, 491.0], [744.0, 527.0], [511.0, 527.0]],\n",
       "    ['Map size = 402', 0.9991912245750427]],\n",
       "   [[[1114.0, 496.0], [1351.0, 493.0], [1352.0, 528.0], [1114.0, 531.0]],\n",
       "    ['Map size = 402', 0.9950515627861023]],\n",
       "   [[[114.0, 535.0], [250.0, 535.0], [250.0, 572.0], [114.0, 572.0]],\n",
       "    ['1, 2, 3, 4', 0.9262902140617371]],\n",
       "   [[[1531.0, 589.0], [1842.0, 593.0], [1841.0, 622.0], [1530.0, 618.0]],\n",
       "    ['For each sub-stage', 0.9999158382415771]],\n",
       "   [[[27.0, 651.0], [336.0, 653.0], [336.0, 688.0], [27.0, 686.0]],\n",
       "    ['For each sub-stage.', 0.9774488210678101]],\n",
       "   [[[542.0, 655.0], [715.0, 655.0], [715.0, 690.0], [542.0, 690.0]],\n",
       "    ['Nfilter = 128', 0.91664719581604]],\n",
       "   [[[824.0, 647.0], [1028.0, 647.0], [1028.0, 674.0], [824.0, 674.0]],\n",
       "    ['Concatenation for', 0.9999103546142578]],\n",
       "   [[[1152.0, 663.0], [1326.0, 663.0], [1326.0, 699.0], [1152.0, 699.0]],\n",
       "    ['Nrilter = 128', 0.9596410989761353]],\n",
       "   [[[1604.0, 657.0], [1698.0, 657.0], [1698.0, 694.0], [1604.0, 694.0]],\n",
       "    ['Input', 0.9994994401931763]],\n",
       "   [[[822.0, 674.0], [1028.0, 676.0], [1028.0, 705.0], [822.0, 703.0]],\n",
       "    ['dilation rates: 2, 4', 0.9992793202400208]],\n",
       "   [[[511.0, 694.0], [746.0, 694.0], [746.0, 730.0], [511.0, 730.0]],\n",
       "    ['Map size = 202', 0.9994481205940247]],\n",
       "   [[[1116.0, 701.0], [1356.0, 698.0], [1356.0, 736.0], [1116.0, 738.0]],\n",
       "    ['Map size = 202', 0.9994542002677917]],\n",
       "   [[[179.0, 713.0], [269.0, 713.0], [269.0, 750.0], [179.0, 750.0]],\n",
       "    ['Input', 0.9996445775032043]],\n",
       "   [[[1583.0, 748.0], [1720.0, 748.0], [1720.0, 784.0], [1583.0, 784.0]],\n",
       "    ['Dropout', 0.9998965263366699]],\n",
       "   [[[1770.0, 771.0], [1799.0, 771.0], [1799.0, 896.0], [1770.0, 896.0]],\n",
       "    ['Residual ', 0.9859462380409241]],\n",
       "   [[[61.0, 794.0], [97.0, 794.0], [97.0, 1061.0], [61.0, 1061.0]],\n",
       "    ['Residual Connection', 0.9999068379402161]],\n",
       "   [[[149.0, 808.0], [298.0, 808.0], [298.0, 844.0], [149.0, 844.0]],\n",
       "    ['Conv 3x3', 0.9998299479484558]],\n",
       "   [[[824.0, 840.0], [1030.0, 840.0], [1030.0, 866.0], [824.0, 866.0]],\n",
       "    ['Concatenation for.', 0.9668340682983398]],\n",
       "   [[[1574.0, 844.0], [1724.0, 844.0], [1724.0, 879.0], [1574.0, 879.0]],\n",
       "    ['Conv 3x3', 0.9998106956481934]],\n",
       "   [[[208.0, 862.0], [240.0, 862.0], [240.0, 910.0], [208.0, 910.0]],\n",
       "    ['+', 0.6618708968162537]],\n",
       "   [[[544.0, 856.0], [715.0, 856.0], [715.0, 893.0], [544.0, 893.0]],\n",
       "    ['Nfilter = 256', 0.9645035266876221]],\n",
       "   [[[1154.0, 858.0], [1328.0, 856.0], [1329.0, 893.0], [1154.0, 896.0]],\n",
       "    ['Nfilter = 256', 0.9527257680892944]],\n",
       "   [[[824.0, 869.0], [1028.0, 869.0], [1028.0, 896.0], [824.0, 896.0]],\n",
       "    ['dilation rates: 2, 4.', 0.9832218885421753]],\n",
       "   [[[1768.0, 881.0], [1804.0, 882.0], [1799.0, 1045.0], [1763.0, 1044.0]],\n",
       "    ['I Connection', 0.9719455242156982]],\n",
       "   [[[511.0, 896.0], [748.0, 893.0], [748.0, 931.0], [511.0, 933.0]],\n",
       "    ['Map size = 102', 0.999730110168457]],\n",
       "   [[[1125.0, 900.0], [1360.0, 900.0], [1360.0, 935.0], [1125.0, 935.0]],\n",
       "    ['Map size = 102', 0.9996284246444702]],\n",
       "   [[[147.0, 916.0], [305.0, 916.0], [305.0, 945.0], [147.0, 945.0]],\n",
       "    ['Activation', 0.9926586151123047]],\n",
       "   [[[1570.0, 947.0], [1730.0, 947.0], [1730.0, 976.0], [1570.0, 976.0]],\n",
       "    ['Activation', 0.9999138712882996]],\n",
       "   [[[128.0, 1010.0], [313.0, 1010.0], [313.0, 1045.0], [128.0, 1045.0]],\n",
       "    ['BatchNorm', 0.9998223185539246]],\n",
       "   [[[1558.0, 1036.0], [1743.0, 1036.0], [1743.0, 1072.0], [1558.0, 1072.0]],\n",
       "    ['BatchNorm', 0.999841570854187]],\n",
       "   [[[208.0, 1049.0], [242.0, 1049.0], [242.0, 1097.0], [208.0, 1097.0]],\n",
       "    ['4', 0.6153636574745178]],\n",
       "   [[[540.0, 1057.0], [712.0, 1055.0], [713.0, 1092.0], [541.0, 1095.0]],\n",
       "    ['Nfilter = 512', 0.9091458916664124]],\n",
       "   [[[147.0, 1105.0], [290.0, 1105.0], [290.0, 1140.0], [147.0, 1140.0]],\n",
       "    ['Addition', 0.9636780023574829]],\n",
       "   [[[517.0, 1101.0], [735.0, 1096.0], [736.0, 1132.0], [518.0, 1136.0]],\n",
       "    ['Map size = 52', 0.992573618888855]],\n",
       "   [[[1581.0, 1130.0], [1722.0, 1130.0], [1722.0, 1159.0], [1581.0, 1159.0]],\n",
       "    ['Addition', 0.9999043345451355]]],\n",
       "  'ocr': [[[977.0, 141.0], [1175.0, 141.0], [1175.0, 176.0], [977.0, 176.0]],\n",
       "   ['Output SZE', 0.9997275471687317]]},\n",
       " '2204.07309v1-Figure10-1.png': {'caption': 'Figure 10:Anoverviewof the batch deployment architecture and main components of Saga‚Äôs NERD stack.',\n",
       "  'imageText': ['GPU',\n",
       "   'Cluster',\n",
       "   'Compute',\n",
       "   '(Data',\n",
       "   'and',\n",
       "   'Feature',\n",
       "   'Store)',\n",
       "   'NERD',\n",
       "   'Entity',\n",
       "   'View',\n",
       "   'Model',\n",
       "   'Store',\n",
       "   'Mention',\n",
       "   'Generation',\n",
       "   'List',\n",
       "   'of',\n",
       "   '<Mention,',\n",
       "   'Context,',\n",
       "   'Predicted_entity_info>',\n",
       "   'List',\n",
       "   'of',\n",
       "   '<Mention,',\n",
       "   'Context,',\n",
       "   'Candidates>',\n",
       "   'List',\n",
       "   'of',\n",
       "   '<Text',\n",
       "   'Passage>',\n",
       "   'or',\n",
       "   '<Semi-structured',\n",
       "   'Records>',\n",
       "   'Elastic',\n",
       "   'Processing',\n",
       "   'Prepare',\n",
       "   'Output',\n",
       "   'Orchestrated',\n",
       "   'WorkÔ¨Çow',\n",
       "   'Management',\n",
       "   'Elastic',\n",
       "   'Processing',\n",
       "   'Bulk',\n",
       "   'Contextual',\n",
       "   'Entity',\n",
       "   'DisambiguationOutput',\n",
       "   'Input',\n",
       "   'Data',\n",
       "   'Preprocessing',\n",
       "   'Candidate',\n",
       "   'Retrieval'],\n",
       "  'image_file': '2204.07309v1-Figure10-1.png',\n",
       "  'sections': [{'heading': 'Entity Recognition and Disambiguation',\n",
       "    'text': 'Named entity recognition and disambiguation (NERD) is the problem of identifying text mentions of named entities in unstructured or semi-structured data and disambiguating them against entities in a KG or standardized vocabulary [12,45,58,59,59,60,62,83]. For example, given the sentence \\'We visited Hanover and Dartmouth\\' or the record <Dartmouth, located_in: Hanover> we want to resolve the mention \"Hanover\" to Hanover, New Hampshire and not to the more popular Hanover, Germany.\\nSaga provides a complete NERD stack, which is used to implement the object resolution during KG construction (see Section 2) but also powers a number of additional use cases where annotating or enriching text-based data with information from the KG is required. We use an elastic deployment for large batch jobs and a high performant low-latency variant for online workloads. Figure 10 shows a high-level diagram of the batch deployment and the main components of the NERD stack.\\nWe treat entity disambiguation as an entity linking problem [12]. A key requirement in Saga is our ability to correctly disambiguate tail (i.e., less popular) entities. In this case, one cannot rely only on string similarities between the mention and entity names in the graph but needs to reason about the context (e.g., surrounding text or other fields in a structured record) that a mention appears in. Such context can carry information about the relationships or the semantic type of the entity that the mention refers to and can be compared against information in the KG to improve the accuracy of named entity disambiguation [58,62,83]. To this end, we create a view using the Graph Engine described in Section 3 that summarizes our knowledge for each entity in the KG, i.e., its aliases, entity types, relationships, types of its neighboring entities, and reason about similarities between the context of a mention and these entity summaries. We refer to this view of entity summaries as NERD Entity View. Given a mention and the relevant context, our goal is to find if there exists any record in the NERD Entity View that is a \"match\" of the mention in the input. The first step is to identify candidate entities that are likely to be matches to the mention. Then, we compute a matching score for each of the returned candidates and identify if there is a record in the NERD Entity View that matches the input mention with high-confidence.\\nNERD Entity View. The goal of each record in the NERD entity view is to provide a comprehensive summary that can act as  a discriminative definition for each entity in the KG. Each entry in the NERD Entity View is a record with attributes that contain information about: 1) the name and aliases of the entity in different locales, 2) the different types from the KG ontology that are associated with the entity (e.g., \\'human\\', \\'music artist\\', \\'academic scholar\\' etc), 3) a text-based description of the entity if available, 4) a list of important one-hop relationships that the entity participates in, 5) the entity types of important neighbors of the entity, and 6) the entity importance scores computed by the Graph Engine (Section 3.3). This comprehensive summary of each entity in the KG provides opportunities to identify cases where information in the NERD Entity View overlaps with information in the context and hence perform more accurate disambiguation. For example, given that the NERD Entity View for Hanover, New Hampshire includes the relationship <Dartmouth College, located_in, Hanover>, we can accurately identify that the mention \"Hanover\" in the context of the sentence \\'We visited downtown Hanover after spending time at Dartmouth\\' refers to Hanover, New Hampshire and not Hanover, Germany. The NERD Entity View is computed using the Graph Engine, which guarantees the its freshness via incremental updates as new facts and entities are ingested in the KG.\\nCandidate Retrieval. Candidate retrieval can be viewed as a parallel to blocking in entity linking. In this step we rely on the similarity between the input entity mention and the name and alias fields of the records in the NERD Entity View to find likely matches. To go beyond exact matches, we use the neural string similarity functions described above. We also allow information on admissible entity types to be used to further improve precision-we make use of Entity Type information during Object Resolution in KG Construction where the attribute-value to be disambiguated is accompanied by an entity type (see Section 6). In the presence of constraints on computational resources or tight latency requirements, we rely on entity importance to prioritize candidate comparison and limit the scope of entity disambiguation to popular entities. Overall, given a limit of -candidates the goal of candidate retrieval is to optimize recall by pruning the domain of possible matches given the extreme and ever-increasing number of entities in the KG. This approach is inspired by our prior work on HoloClean [66,82] where pruning was shown to be critical for accurate data cleaning and imputation over extremely large domains.\\nContextual Entity Disambiguation. The last step of the NERD stack is responsible for determining which of the entity candidates (if any) is the most probable to be referenced in the input mention. We cast Entity Disambiguation as a classification problem over the space of available candidates with an additional rejection mechanism, i.e., we allow rejecting all input candidates as not good options. To enable classification over sets of candidates with variable input size and provide the opportunity for rejection we rely on a one versus all version of multi-class classification [34]. We also follow a neural network architecture that is similar to state-of-the-art named entity disambiguation models [62,83] and models that jointly encode graphs and text [16,78]. Specifically, the model we use to perform this classification task is a contextual, transformer-based deep neural network that leverages the Attention mechanism [75] to reason about the similarity between the input context and the different attributes in the NERD Entity View records. A diagram of our model and approach for Entity Disambiguation is shown in Figure 11. All models used in the NERD stack are trained offline via weak-supervision procedures that combine a collection of text data annotated with entity tags, manually curated query logs, and text snippets generated by applying templates over a selection of facts present in the KG. While these models are re-trained at regular intervals to ensure no accuracy degradation, entity additions are reflected by updating the NERD Entity View.',\n",
       "    'n_publication_ref': 21,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale',\n",
       "  'abstract': 'We introduce Saga, a next-generation knowledge construction and serving platform for powering knowledge-based applications at industrial scale. Saga follows a hybrid batch-incremental design to continuously integrate billions of facts about real-world entities and construct a central knowledge graph that supports multiple production use cases with diverse requirements around data freshness, accuracy, and availability. In this paper, we discuss the unique challenges associated with knowledge graph construction at industrial scale, and review the main components of Saga and how they address these challenges. Finally, we share lessons-learned from a wide array of production use cases powered by Saga.',\n",
       "  'paddleOCR': [[[[203.0, 0.0], [461.0, 0.0], [462.0, 13.0], [203.0, 14.0]],\n",
       "    ['Orchestrated Workflow Management', 0.9802472591400146]],\n",
       "   [[[347.0, 66.0], [475.0, 68.0], [475.0, 82.0], [347.0, 80.0]],\n",
       "    ['Elastic Processing', 0.9860321879386902]],\n",
       "   [[[335.0, 107.0], [394.0, 107.0], [394.0, 124.0], [335.0, 124.0]],\n",
       "    ['Mention', 0.9995686411857605]],\n",
       "   [[[458.0, 106.0], [494.0, 106.0], [494.0, 124.0], [458.0, 124.0]],\n",
       "    ['Data', 0.9987834692001343]],\n",
       "   [[[626.0, 105.0], [698.0, 107.0], [697.0, 125.0], [625.0, 122.0]],\n",
       "    ['Candidate', 0.9993215203285217]],\n",
       "   [[[878.0, 103.0], [921.0, 103.0], [921.0, 118.0], [878.0, 118.0]],\n",
       "    ['NERD', 0.9985967874526978]],\n",
       "   [[[72.0, 121.0], [111.0, 121.0], [111.0, 136.0], [72.0, 136.0]],\n",
       "    ['Input', 0.9975846409797668]],\n",
       "   [[[325.0, 123.0], [401.0, 125.0], [400.0, 143.0], [324.0, 140.0]],\n",
       "    ['Generation', 0.997494101524353]],\n",
       "   [[[427.0, 123.0], [525.0, 125.0], [524.0, 143.0], [426.0, 140.0]],\n",
       "    ['Preprocessing', 0.9988563060760498]],\n",
       "   [[[631.0, 123.0], [691.0, 126.0], [691.0, 141.0], [630.0, 138.0]],\n",
       "    ['Retrieval', 0.9977474212646484]],\n",
       "   [[[862.0, 119.0], [937.0, 119.0], [937.0, 136.0], [862.0, 136.0]],\n",
       "    ['Entity View', 0.9459674954414368]],\n",
       "   [[[819.0, 138.0], [980.0, 138.0], [980.0, 152.0], [819.0, 152.0]],\n",
       "    ['(Data and Feature Store)', 0.9564550518989563]],\n",
       "   [[[7.0, 176.0], [176.0, 179.0], [176.0, 193.0], [7.0, 190.0]],\n",
       "    ['List of <Text Passage> or', 0.923706591129303]],\n",
       "   [[[379.0, 181.0], [636.0, 181.0], [636.0, 195.0], [379.0, 195.0]],\n",
       "    ['List of <Mention,Context, Candidates>', 0.9636961221694946]],\n",
       "   [[[3.0, 196.0], [181.0, 196.0], [181.0, 209.0], [3.0, 209.0]],\n",
       "    ['<Semi-structuredRecords>', 0.9952547550201416]],\n",
       "   [[[241.0, 205.0], [371.0, 208.0], [371.0, 222.0], [241.0, 220.0]],\n",
       "    ['Elastic Processing', 0.9835038781166077]],\n",
       "   [[[279.0, 246.0], [334.0, 246.0], [334.0, 263.0], [279.0, 263.0]],\n",
       "    ['Prepare', 0.9995967149734497]],\n",
       "   [[[401.0, 243.0], [552.0, 246.0], [552.0, 264.0], [401.0, 261.0]],\n",
       "    ['Bulk Contextual Entity', 0.9980881214141846]],\n",
       "   [[[67.0, 260.0], [120.0, 260.0], [120.0, 274.0], [67.0, 274.0]],\n",
       "    ['Output', 0.9977784752845764]],\n",
       "   [[[799.0, 251.0], [844.0, 251.0], [844.0, 266.0], [799.0, 266.0]],\n",
       "    ['Model', 0.9995578527450562]],\n",
       "   [[[282.0, 264.0], [331.0, 264.0], [331.0, 279.0], [282.0, 279.0]],\n",
       "    ['Output', 0.9989325404167175]],\n",
       "   [[[423.0, 262.0], [529.0, 264.0], [528.0, 281.0], [422.0, 279.0]],\n",
       "    ['Disambiguation', 0.9657174348831177]],\n",
       "   [[[801.0, 264.0], [842.0, 267.0], [841.0, 285.0], [800.0, 282.0]],\n",
       "    ['Store', 0.9992377161979675]],\n",
       "   [[[434.0, 301.0], [525.0, 303.0], [524.0, 320.0], [434.0, 318.0]],\n",
       "    ['GPU Cluster', 0.9493156671524048]],\n",
       "   [[[6.0, 314.0], [176.0, 314.0], [176.0, 328.0], [6.0, 328.0]],\n",
       "    ['List of <Mention,Context,', 0.957980215549469]],\n",
       "   [[[445.0, 321.0], [513.0, 321.0], [513.0, 338.0], [445.0, 338.0]],\n",
       "    ['Compute', 0.9986172318458557]],\n",
       "   [[[19.0, 331.0], [165.0, 331.0], [165.0, 345.0], [19.0, 345.0]],\n",
       "    ['Predicted_entity_info>', 0.9954368472099304]]],\n",
       "  'ocr': [[[7.0, 176.0], [176.0, 179.0], [176.0, 193.0], [7.0, 190.0]],\n",
       "   ['List of <Text Passage> or', 0.923706591129303]]},\n",
       " '2205.14970v2-Figure2-1.png': {'caption': 'Figure 2: Overview of our contrastive non-autoregressive model Conna for personalized bundle creative generation.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2205.14970v2-Figure2-1.png',\n",
       "  'sections': [{'heading': 'PROPOSED MODEL',\n",
       "    'text': 'We propose a contrastive non-autoregressive model Conna for bundle creative generation. Figure 2 gives an overview of Conna, which contains a type-aware encoder and a non-autoregressive decoder to improve generation efficiency. It is optimized via a contrastive learning objective to ensure creative-level quality.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Towards Personalized Bundle Creative Generation with Contrastive Non-Autoregressive Decoding',\n",
       "  'abstract': 'Current bundle generation studies focus on generating a combination of items to improve user experience. In real-world applications, there is also a great need to produce bundle creatives that consist of mixture types of objects (e.g., items, slogans and templates) for achieving better promotion effect. We study a new problem named bundle creative generation: for given users, the goal is to generate personalized bundle creatives that the users will be interested in. To take both quality and efficiency into account, we propose a contrastive non-autoregressive model that captures user preferences with ingenious decoding objective. Experiments on large-scale realworld datasets verify that our proposed model shows significant advantages in terms of creative quality and generation speed.',\n",
       "  'paddleOCR': [[[[111.0, 10.0], [281.0, 10.0], [281.0, 31.0], [111.0, 31.0]],\n",
       "    ['Learning Objectives', 0.9998952150344849]],\n",
       "   [[[332.0, 51.0], [1005.0, 53.0], [1005.0, 70.0], [332.0, 68.0]],\n",
       "    ['Negative Creative b_: Itcmy Itcmw Itcmy Slogan Slogan Template (unclicked',\n",
       "     0.9338676929473877]],\n",
       "   [[[334.0, 105.0], [539.0, 105.0], [539.0, 120.0], [334.0, 120.0]],\n",
       "    ['Positive Creative b:', 0.9509614109992981]],\n",
       "   [[[552.0, 103.0], [999.0, 104.0], [999.0, 125.0], [552.0, 124.0]],\n",
       "    ['Itemx Itemy Itemz Slogang Slogan Templatey(clicked)',\n",
       "     0.9484071731567383]],\n",
       "   [[[920.0, 141.0], [998.0, 144.0], [997.0, 161.0], [919.0, 159.0]],\n",
       "    ['Hungarian', 0.9868000745773315]],\n",
       "   [[[7.0, 152.0], [179.0, 152.0], [179.0, 172.0], [7.0, 172.0]],\n",
       "    ['Contrastive Lct(b, b_', 0.9382084608078003]],\n",
       "   [[[207.0, 152.0], [353.0, 152.0], [353.0, 173.0], [207.0, 173.0]],\n",
       "    ['Set-based Lset (b)', 0.926325261592865]],\n",
       "   [[[922.0, 162.0], [993.0, 164.0], [993.0, 182.0], [921.0, 179.0]],\n",
       "    ['Matching', 0.999110758304596]],\n",
       "   [[[333.0, 197.0], [608.0, 197.0], [608.0, 218.0], [333.0, 218.0]],\n",
       "    ['Generated Creative b : Itemz', 0.996334433555603]],\n",
       "   [[[613.0, 199.0], [665.0, 201.0], [664.0, 217.0], [613.0, 214.0]],\n",
       "    ['Itemx', 0.9198129773139954]],\n",
       "   [[[676.0, 203.0], [722.0, 203.0], [722.0, 214.0], [676.0, 214.0]],\n",
       "    ['Itermy', 0.8967384696006775]],\n",
       "   [[[729.0, 199.0], [937.0, 200.0], [937.0, 219.0], [729.0, 218.0]],\n",
       "    ['Slogan Slogang Template y', 0.968309760093689]],\n",
       "   [[[923.0, 238.0], [994.0, 238.0], [994.0, 256.0], [923.0, 256.0]],\n",
       "    ['Predicted', 0.9993894100189209]],\n",
       "   [[[911.0, 258.0], [1005.0, 260.0], [1004.0, 278.0], [911.0, 275.0]],\n",
       "    ['Distributions', 0.9988999962806702]],\n",
       "   [[[90.0, 268.0], [314.0, 264.0], [314.0, 284.0], [91.0, 289.0]],\n",
       "    ['Object Representations HI+1', 0.9845780730247498]],\n",
       "   [[[633.0, 309.0], [656.0, 309.0], [656.0, 321.0], [633.0, 321.0]],\n",
       "    ['FFN', 0.8733785152435303]],\n",
       "   [[[750.0, 308.0], [777.0, 308.0], [777.0, 321.0], [750.0, 321.0]],\n",
       "    ['FFN', 0.9974554181098938]],\n",
       "   [[[876.0, 308.0], [899.0, 308.0], [899.0, 321.0], [876.0, 321.0]],\n",
       "    ['FFN', 0.9969214797019958]],\n",
       "   [[[164.0, 371.0], [493.0, 371.0], [493.0, 392.0], [164.0, 392.0]],\n",
       "    ['Pre-LN Self-Attention Layer (x L)', 0.9761709570884705]],\n",
       "   [[[584.0, 371.0], [896.0, 371.0], [896.0, 392.0], [584.0, 392.0]],\n",
       "    ['Cross-Attention & Self-Attention', 0.9972647428512573]],\n",
       "   [[[7.0, 440.0], [92.0, 440.0], [92.0, 458.0], [7.0, 458.0]],\n",
       "    ['Content Emb', 0.999912679195404]],\n",
       "   [[[317.0, 443.0], [332.0, 443.0], [332.0, 456.0], [317.0, 456.0]],\n",
       "    ['82', 0.7338964343070984]],\n",
       "   [[[361.0, 442.0], [386.0, 442.0], [386.0, 459.0], [361.0, 459.0]],\n",
       "    ['S|,', 0.5113083124160767]],\n",
       "   [[[920.0, 434.0], [1006.0, 434.0], [1006.0, 449.0], [920.0, 449.0]],\n",
       "    ['Type-Specific', 0.9882859587669373]],\n",
       "   [[[918.0, 453.0], [1006.0, 453.0], [1006.0, 466.0], [918.0, 466.0]],\n",
       "    ['Position Emb.', 0.9924854040145874]],\n",
       "   [[[319.0, 469.0], [331.0, 469.0], [331.0, 476.0], [319.0, 476.0]],\n",
       "    ['+', 0.9891110062599182]],\n",
       "   [[[451.0, 470.0], [456.0, 470.0], [456.0, 475.0], [451.0, 475.0]],\n",
       "    ['+', 0.9558030962944031]],\n",
       "   [[[27.0, 485.0], [94.0, 485.0], [94.0, 502.0], [27.0, 502.0]],\n",
       "    ['Type Emb.', 0.9997046589851379]],\n",
       "   [[[241.0, 484.0], [257.0, 484.0], [257.0, 497.0], [241.0, 497.0]],\n",
       "    ['(i)', 0.9659624695777893]],\n",
       "   [[[921.0, 486.0], [985.0, 486.0], [985.0, 500.0], [921.0, 500.0]],\n",
       "    ['Type Emb', 0.9735310673713684]],\n",
       "   [[[96.0, 525.0], [397.0, 518.0], [397.0, 539.0], [96.0, 547.0]],\n",
       "    ['User Clicked ItemsCandidate Slogans', 0.9946568012237549]],\n",
       "   [[[389.0, 532.0], [557.0, 517.0], [559.0, 535.0], [391.0, 549.0]],\n",
       "    ['Candidate Templates', 0.9795407652854919]],\n",
       "   [[[588.0, 526.0], [688.0, 520.0], [689.0, 538.0], [589.0, 544.0]],\n",
       "    ['Item Trigger', 0.9870932698249817]],\n",
       "   [[[190.0, 563.0], [429.0, 560.0], [430.0, 585.0], [191.0, 588.0]],\n",
       "    ['Type-Aware Encoder', 0.9998874664306641]],\n",
       "   [[[572.0, 562.0], [907.0, 564.0], [907.0, 588.0], [572.0, 586.0]],\n",
       "    ['Non-Autoregressive Decoder', 0.9998288750648499]]],\n",
       "  'ocr': [[[920.0, 434.0], [1006.0, 434.0], [1006.0, 449.0], [920.0, 449.0]],\n",
       "   ['Type-Specific', 0.9882859587669373]]},\n",
       " '2202.09450v1-Figure49-1.png': {'caption': 'Fig. 49. The architecture of m3d CNN model for hand pose estimation. Courtesy of [69]',\n",
       "  'imageText': [],\n",
       "  'image_file': '2202.09450v1-Figure49-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Modern Augmented Reality: Applications, Trends, and Future Directions',\n",
       "  'abstract': \"Augmented reality (AR) is one of the relatively old, yet trending areas in the intersection of computer vision and computer graphics with numerous applications in several areas, from gaming and entertainment, to education and healthcare. Although it has been around for nearly fifty years, it has seen a lot of interest by the research community in the recent years, mainly because of the huge success of deep learning models for various computer vision and AR applications, which made creating new generations of AR technologies possible. This work tries to provide an overview of modern augmented reality, from both application-level and technical perspective. We first give an overview of main AR applications, grouped into more than ten categories. We then give an overview of around 100 recent promising machine learning based works developed for AR systems, such as deep learning works for AR shopping (clothing, makeup), AR based image filters (such as Snapchat's lenses), AR animations, and more. In the end we discuss about some of the current challenges in AR domain, and the future directions in this area.\",\n",
       "  'paddleOCR': [[[[389.0, 18.0], [497.0, 18.0], [497.0, 39.0], [389.0, 39.0]],\n",
       "    ['x direction', 0.9882295727729797]],\n",
       "   [[[566.0, 142.0], [588.0, 142.0], [587.0, 432.0], [565.0, 432.0]],\n",
       "    ['3D Conv1+ReLU+ 3D Pool', 0.9348172545433044]],\n",
       "   [[[610.0, 141.0], [634.0, 141.0], [634.0, 432.0], [610.0, 432.0]],\n",
       "    ['3D Conv2 + ReLU + 3D Pool', 0.9644284844398499]],\n",
       "   [[[387.0, 194.0], [495.0, 192.0], [496.0, 213.0], [387.0, 216.0]],\n",
       "    ['y direction', 0.9877437353134155]],\n",
       "   [[[657.0, 191.0], [679.0, 191.0], [679.0, 381.0], [657.0, 381.0]],\n",
       "    ['3D Conv3 + ReLU', 0.9101487398147583]],\n",
       "   [[[712.0, 273.0], [733.0, 273.0], [733.0, 306.0], [712.0, 306.0]],\n",
       "    ['FC', 0.9923726320266724]],\n",
       "   [[[816.0, 269.0], [839.0, 269.0], [839.0, 296.0], [816.0, 296.0]],\n",
       "    ['8', 0.7679906487464905]],\n",
       "   [[[65.0, 344.0], [134.0, 347.0], [133.0, 374.0], [64.0, 371.0]],\n",
       "    ['Depth', 0.9995797276496887]],\n",
       "   [[[902.0, 344.0], [991.0, 344.0], [991.0, 364.0], [902.0, 364.0]],\n",
       "    ['3D Joint', 0.9377144575119019]],\n",
       "   [[[200.0, 358.0], [299.0, 358.0], [299.0, 378.0], [200.0, 378.0]],\n",
       "    ['3D Points', 0.9771195650100708]],\n",
       "   [[[66.0, 374.0], [133.0, 379.0], [131.0, 402.0], [65.0, 396.0]],\n",
       "    ['Image', 0.9989339709281921]],\n",
       "   [[[389.0, 370.0], [494.0, 369.0], [494.0, 388.0], [389.0, 390.0]],\n",
       "    ['z direction', 0.9425135254859924]],\n",
       "   [[[896.0, 373.0], [994.0, 373.0], [994.0, 393.0], [896.0, 393.0]],\n",
       "    ['Locations', 0.9954618811607361]],\n",
       "   [[[347.0, 544.0], [540.0, 544.0], [540.0, 565.0], [347.0, 565.0]],\n",
       "    ['Projective D-TSDF', 0.9691578149795532]],\n",
       "   [[[655.0, 559.0], [742.0, 559.0], [742.0, 578.0], [655.0, 578.0]],\n",
       "    ['3D CNN', 0.9256933331489563]],\n",
       "   [[[401.0, 574.0], [486.0, 574.0], [486.0, 593.0], [401.0, 593.0]],\n",
       "    ['Volumes', 0.998225212097168]]],\n",
       "  'ocr': [[[566.0, 142.0], [588.0, 142.0], [587.0, 432.0], [565.0, 432.0]],\n",
       "   ['3D Conv1+ReLU+ 3D Pool', 0.9348172545433044]]},\n",
       " '2010.15032v3-Figure3-1.png': {'caption': 'Figure 3: Azure Functions architecture.',\n",
       "  'imageText': ['Invocation',\n",
       "   'Invocation',\n",
       "   '...',\n",
       "   'Function',\n",
       "   'Instance',\n",
       "   '1.5',\n",
       "   'GB,',\n",
       "   '1',\n",
       "   'CPU',\n",
       "   'Scale',\n",
       "   'Controller',\n",
       "   'Process',\n",
       "   'Invocations',\n",
       "   'Monitor',\n",
       "   'Usage',\n",
       "   'Monitor',\n",
       "   'Events',\n",
       "   'Create',\n",
       "   'instance',\n",
       "   'rc',\n",
       "   'es',\n",
       "   't',\n",
       "   'S',\n",
       "   'ou',\n",
       "   'Ev',\n",
       "   'en'],\n",
       "  'image_file': '2010.15032v3-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Azure Functions architecture',\n",
       "    'text': 'An architecture overview of Azure Functions is shown in Figure 3. A description of it is available in its documentation [37]. In the service, a set of function instances run invocations in response to events from different sources. The scale of this set is regulated by a long-running component that monitors the state of the service: the Scale Controller.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Benchmarking Parallelism in FaaS Platforms',\n",
       "  'abstract': 'Serverless computing has seen a myriad of work exploring its potential. Some systems tackle Function-as-a-Service (FaaS) properties on automatic elasticity and scale to run highly-parallel computing jobs. However, they focus on specific platforms and convey that their ideas can be extrapolated to any FaaS runtime. An important question arises: do all FaaS platforms fit parallel computations? In this paper, we argue that not all of them provide the necessary means to host highly-parallel applications. To validate our hypothesis, we create a comparative framework and categorize the architectures of four cloud FaaS offerings, emphasizing parallel performance. We attest and extend this description with an empirical experiment that consists in plotting in deep detail the evolution of a parallel computing job on each service. The analysis of our results evinces that FaaS is not inherently good for parallel computations and architectural differences across platforms are decisive to categorize their performance. A key insight is the importance of virtualization technologies and the scheduling approach of FaaS platforms. Parallelism improves with lighter virtualization and proactive scheduling due to finer resource allocation and faster elasticity. This causes some platforms like AWS and IBM to perform well for highly-parallel computations, while others such as Azure present difficulties to achieve the required parallelism degree. Consequently, the information in this paper becomes of special interest to help users choose the most adequate infrastructure for their parallel applications.',\n",
       "  'paddleOCR': [[[[139.0, 32.0], [251.0, 34.0], [250.0, 51.0], [139.0, 49.0]],\n",
       "    ['Monitor Events', 0.9765625596046448]],\n",
       "   [[[369.0, 41.0], [591.0, 41.0], [591.0, 65.0], [369.0, 65.0]],\n",
       "    ['Scale Controller', 0.9996788501739502]],\n",
       "   [[[251.0, 144.0], [358.0, 148.0], [357.0, 166.0], [250.0, 162.0]],\n",
       "    ['Monitor Usage', 0.9848061800003052]],\n",
       "   [[[469.0, 143.0], [601.0, 147.0], [601.0, 165.0], [468.0, 162.0]],\n",
       "    ['Create instance', 0.9981753826141357]],\n",
       "   [[[38.0, 153.0], [70.0, 154.0], [67.0, 357.0], [35.0, 356.0]],\n",
       "    ['Event Sources', 0.9998896718025208]],\n",
       "   [[[369.0, 255.0], [610.0, 257.0], [610.0, 281.0], [368.0, 279.0]],\n",
       "    ['Function Instance', 0.9887145757675171]],\n",
       "   [[[419.0, 292.0], [559.0, 292.0], [559.0, 312.0], [419.0, 312.0]],\n",
       "    ['1.5 GB, 1 CPU', 0.909433126449585]],\n",
       "   [[[125.0, 355.0], [266.0, 356.0], [266.0, 373.0], [125.0, 372.0]],\n",
       "    ['Process Invocations', 0.9983592629432678]],\n",
       "   [[[403.0, 349.0], [489.0, 349.0], [489.0, 367.0], [403.0, 367.0]],\n",
       "    ['Invocation', 0.9981452226638794]],\n",
       "   [[[460.0, 415.0], [547.0, 415.0], [547.0, 433.0], [460.0, 433.0]],\n",
       "    ['Invocation', 0.9981359243392944]]],\n",
       "  'ocr': [[[38.0, 153.0], [70.0, 154.0], [67.0, 357.0], [35.0, 356.0]],\n",
       "   ['Event Sources', 0.9998896718025208]]},\n",
       " '2011.12879v4-Figure1-1.png': {'caption': 'Figure 1: From Classical Model to Heard-Of Predicate',\n",
       "  'imageText': ['Rounds',\n",
       "   'Asynchrony',\n",
       "   'Heard-Of',\n",
       "   'Predicate',\n",
       "   'Delivered',\n",
       "   'Predicate',\n",
       "   'Operational',\n",
       "   'Model'],\n",
       "  'image_file': '2011.12879v4-Figure1-1.png',\n",
       "  'sections': [{'heading': '',\n",
       "    'text': '1. Introduction 1.1. Motivation. Distributed computing studies how multiple processes can accomplish computational tasks by interacting with each other. Various means of communication are traditionally studied; we focus here on message-passing, where processes exchange messages. Yet, message-passing models still abound: they might have various degrees of synchrony (how much processes can drift of from each other in term of processing speed or communication), different kinds of faults (processes crashing, processes crashing and restarting, message loss, Figure 1: From Classical Model to Heard-Of Predicate capture nothing specific about the model -for example the rule that always allows to change rounds. Conversely, rules that are tailored to a specific model, like waiting for n messages, might block processes in other models (where there might be less than n messages received at a round). Finding a corresponding heard-of predicate for an asynchronous model thus requires a model-specific analysis, as well as some way to choose among the possible rules. This paper proposes a formalisation of this question, as well as an approach for answering it for concrete message-passing models.\\n1.2. Overview. As mentioned above, abstracting message-passing models under one formalism would significantly help with comparing results across models and formally verifying them. The Heard-Of model provides such an abstraction, but only if we can define and compute a corresponding heard-of predicate for asynchronous message-passing models, which is the hardest case due to lack of an upper bound on communication delay. We compute this heard-of predicate in two steps, as shown in Figure 1. We start with the operational model, derive a \"delivered predicate\", and then find the heard-of predicates that can be implemented by some rule for changing rounds (called a strategy) for this specific delivered predicate. Among such predicates, we propose a criterion to choose the one characterizing the asynchronous message-passing model.\\nThe first step goes from the original asynchronous message-passing model to a delivered predicate, an abstraction that we introduce. A delivered predicate captures the messages that are eventually delivered from each round r, without considering the round of delivery. This is to contrast with heard-of collections, that only capture messages tagged by r if delivered at the receiver when its local round counter is ‚â§ r. A delivered predicate makes the original model formal in the form of a delivered predicate, but it avoids dealing with the main issue for getting a heard-of predicate: asynchrony. Because the round of delivery is not considered in a delivered predicate, computing the corresponding one for our original model does not require a strategy for when to change rounds. Intuitively, the delivered predicate of a model is the heard-of predicate of the same model if it was synchronous, which are relatively straightforward to define and compute.\\nThe second step goes from the delivered predicate to the heard-of predicate corresponding to the original model. To do so, we define strategies: rules that tell when processes can change rounds. The main constraint on such strategies is to never block a process at a round forever. As mentioned above, the difficulty here comes from the fact that there are in general many different strategies for implementing rounds that don\\'t block forever. Which of these should be the corresponding heard-of predicate? Our answer relies on the following intuition: the predicate precisely capturing the asynchronous model is the predicate satisfied by the fewer heard-of collections, among the set of predicates that can be implemented on top of the original model. Formally, this translates to being included in every other heard-of predicate that can be implemented (considering predicates as sets in the standard way). Such a predicate, if it exists, intuitively constrains communication the most, by allowing fewer possibilities for which messages are heard by who at which round. This choice stems from the relevance of uncertainty for distributed computability. A distributed algorithm has to give the correct answer independently of the specific scheduling, the specific failures, and every other source of uncertainty and non-determinism specified by the model. This means that if everything that could happen according to model M 1 could also happen according to model M 2 , then every correct algorithm for M 2 is also correct for M 1 . In the Heard-Of model, this means that for two heard-of predicates HO 1 and HO 2 such that HO 1 ‚äÜ HO 2 , then every correct algorithm for HO 2 is also correct on HO 1 .\\nWith this formalization in hand, what is left is a way to compute the resulting predicate and prove that it is indeed the strongest. This problem becomes tractable by introducing operations on predicates (delivered and heard-of) and strategies. Operations capture the intuition that it\\'s often easier to build complex models by composing simple ones together. Hence if the original model can be framed as such a composition, its delivered predicate can similarly be constructed from the delivered predicates of the building blocks, thanks to the operations we define. For some families of strategies (strategies that only depend on some limited part of the local state of a process, here the messages of the current round or the messages of past and current rounds respectively), the strategy that implements the heard-of predicate corresponding to the full model can be built from the strategies of the building blocks using analogous operations on strategies that we define. Then the heard-of predicate of this built strategy is linked with the composition of the heard-of predicates implemented by the building block strategies.\\n1.3. Contributions. The contributions of this article are the following:\\n‚Ä¢ The definition of delivered predicates and strategies, in Section 3.\\n‚Ä¢ Operations on delivered predicates and strategies, to build complex predicates, in Section 4.\\n‚Ä¢ The formalization of the derivation of heard-of predicates from a delivered predicate and a strategy, in Section 5. This comes with a complete example: the asynchronous message-passing model with reliable communication and at most F permanent crashes. ‚Ä¢ The study of oblivious strategies, the strategies only looking at messages for the current round, in Section 6. We provide a technique to extract a strategy dominating the oblivious strategies of the complex predicate from the strategies of its building blocks; exact computations of the generated heard-of predicates; and a sufficient condition on the building blocks for the result of the operations to be dominated by an oblivious strategy. ‚Ä¢ The study of conservative strategies, the strategies looking at all messages from previous and current round, as well as the round number, in Section 7. We provide a technique to extract a strategy dominating the conservative strategies of the complex predicate from the strategies of its building blocks; upper bounds on the generated heard-of predicates; and a sufficient condition on the building blocks for the result of the operations to be dominated by a conservative strategy. ‚Ä¢ A preliminary exploration of strategies using messages from future rounds, and an extended example where these strategies build stronger heard-of predicates than oblivious and conservative strategies, in Section 8.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'CHARACTERIZATION AND DERIVATION OF HEARD-OF PREDICATES FOR ASYNCHRONOUS MESSAGE-PASSING MODELS',\n",
       "  'abstract': 'In distributed computing, multiple processes interact to solve a problem together. The main model of interaction is the message-passing model, where processes communicate by exchanging messages. Nevertheless, there are several models varying along important dimensions: degree of synchrony, kinds of faults, number of faults. . . This variety is compounded by the lack of a general formalism in which to abstract these models, for translating results from one to the other. One way to bring order to these models is to constrain them further, to communicate in rounds. This is the setting of the Heard-Of model, which captures many models through predicates on the messages sent in a round and received on time (at this round or before, where the round is the one of the receiver). Yet, it is not easy to define the predicate that best captures a given operational model. The question is even harder for the asynchronous case, as unbounded message delay means the implementation of rounds must depend on details of the model. This paper shows that characterising asynchronous models by heard-of predicates is indeed meaningful. This characterization relies on the introduction of delivered predicates, an intermediate abstraction between the informal operational model and the heard-of predicates. Our approach splits the problem into two steps: first extract the delivered model capturing the informal model, and then characterize the heard-of predicates that can be generated by this delivered model. For the first part, we provide examples of delivered predicates, and an approach to derive more. It uses the intuition that complex models are a composition of simpler models. We thus define operations like union, succession or repetition that make it easier to derive complex delivered predicates from simple ones while retaining expressivity. For the second part, we formalize and study strategies for when to change rounds. Intuitively, the characterizing predicate of a model is the one generated by a strategy that waits for as much messages as possible, without blocking forever.',\n",
       "  'paddleOCR': [[[[300.0, 65.0], [428.0, 65.0], [428.0, 97.0], [300.0, 97.0]],\n",
       "    ['Rounds', 0.9998981356620789]],\n",
       "   [[[739.0, 56.0], [934.0, 60.0], [934.0, 98.0], [738.0, 93.0]],\n",
       "    ['Asynchrony', 0.9999428987503052]],\n",
       "   [[[27.0, 89.0], [228.0, 87.0], [228.0, 119.0], [28.0, 121.0]],\n",
       "    ['Operational', 0.9998236298561096]],\n",
       "   [[[520.0, 87.0], [680.0, 87.0], [680.0, 119.0], [520.0, 119.0]],\n",
       "    ['Delivered', 0.9998393654823303]],\n",
       "   [[[993.0, 87.0], [1154.0, 87.0], [1154.0, 119.0], [993.0, 119.0]],\n",
       "    ['Heard-Of', 0.9997705221176147]],\n",
       "   [[[73.0, 140.0], [182.0, 140.0], [182.0, 172.0], [73.0, 172.0]],\n",
       "    ['Model', 0.9997690320014954]],\n",
       "   [[[518.0, 141.0], [681.0, 141.0], [681.0, 173.0], [518.0, 173.0]],\n",
       "    ['Predicate', 0.999923050403595]],\n",
       "   [[[991.0, 138.0], [1153.0, 143.0], [1153.0, 175.0], [990.0, 170.0]],\n",
       "    ['Predicate', 0.9999178647994995]]],\n",
       "  'ocr': [[[73.0, 140.0], [182.0, 140.0], [182.0, 172.0], [73.0, 172.0]],\n",
       "   ['Model', 0.9997690320014954]]},\n",
       " '2010.06632v1-Figure2-1.png': {'caption': 'Figure 2: Illustration of the geometry of an impulsive encounter along a nearly straight orbit, specifying the coordinate axes and radial vectors used throughout this paper.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.06632v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'INTRODUCTION',\n",
       "    'text': \"When an extended object, hereafter the subject, has a gravitational encounter with another massive body, hereafter the perturber, it induces a tidal distortion that causes a transfer of orbital energy to internal energy of the body (i.e., coherent bulk motion is transferred into random motion). Gravitational encounters therefore are a means by which two unbound objects can become bound ('tidal capture'), and ultimately merge. They also cause a heating and deformation of the subject, which can result in mass loss and even a complete disruption of the subject. Gravitational encounters thus play an important role in many areas of astrophysics, including, among others, the merging of galaxies and dark matter halos (e.g., Richstone 1975Richstone , 1976White 1978;Makino & Hut 1997;Mamon 1992Mamon , 2000, the tidal stripping, heating and harassment of subhalos, satellite galaxies and globular clusters (e.g., Moore et al. 1996;van den Bosch et al. 2018;Dutta Chowdhury et al. 2020), the heating of discs (Ostriker et al. 1972), the formation of stellar binaries by two-body tidal capture (Fabian et al. 1975;Press & Teukolsky 1977;Lee & Ostriker 1986), and the disruption of star clusters and stellar binaries (e.g., Spitzer 1958;Heggie 1975;Bahcall et al. 1985). Throughout this paper, for brevity we will refer to the constituent particles of the subject as 'stars'.\\nA fully general treatment of gravitational encounters is extremely complicated, which is why they are often studied using numerical simulations. However, in the impulsive limit, when the encounter velocity is large compared to the characteristic internal velocities of the subject, the encounter can be treated analytically. In particular, in this case, one can ignore the Figure 1: A pictorial comparison of impulsive encounters (vP œÉ) under certain conditions for the impact parameter b. In the upper-right corner of each panel we cite the paper in which the impulsive energy transfer for this case was first worked out. This paper presents the fully general case D (no constraint on b), as depicted in the lower right-hand panel. internal motion within the subject (i.e., ignore the displacements of the stars during the encounter), and simply compute the velocity change (the impulse) of a star using\\n‚àÜv = ‚àí ‚àáŒ¶P dt , (1\\n)\\nwhere Œ¶P is the potential due to the perturber. And since the encounter speed, vP, is high, one can further simplify matters by considering the perturber to be on a straight-line orbit with constant speed. The impulse increases the specific kinetic energy of the subject stars by\\n‚àÜŒµ = v ‚Ä¢ ‚àÜv + 1 2 (‚àÜv) 2 . (2\\n)\\nSince the potential energy of the stars remains invariant during (but not after) the impulse, the increase in total internal energy of the subject is given by ‚àÜEint = œÅS(r)‚àÜŒµ(r)\\nd 3 r ‚àí 1 2 MS(‚àÜvCM) 2 . (3\\n)\\nHere MS and œÅS(r) are the mass and density profile of the subject and ‚àÜvCM is the velocity impulse of the centre-of-mass of the subject.\\nIf the encounter, in addition to being impulsive, is also distant, such that the impact parameter b is much larger than the scale radii of the subject (rS) and the perturber (rP), i.e., b max(rS, rP), then the internal structure of the perturber can be ignored (it can be treated as a point mass), and its potential can be expanded as a multipole series and truncated at the quadrupole term. This 'distant tide approximation' (hereafter DTA, depicted as case A in Fig. 1) was first used by Spitzer (1958, hereafter S58) to study the disruption of star clusters by passing interstellar clouds. In particular, Spitzer showed that, Case Impact parameter ‚àÜEint Source (1)\\n(2)\\n(3) (    Spitzer (1958), , van den Bosch et al. (2018), and this paper, respectively.\\nœást = 1 2 (3J0 ‚àí J1 ‚àí I0) 2 + (2I0 ‚àí I1 ‚àí 3J0 + J1) 2 + I 2 0 , I k (b) = ‚àû 1 ¬µ k (bŒ∂) dŒ∂ Œ∂ 2 (Œ∂ 2 ‚àí 1) 1/2 , J k (b) = ‚àû 1 ¬µ k (bŒ∂) dŒ∂ Œ∂ 4 (Œ∂ 2 ‚àí 1) 1/2 (k = 0, 1), ¬µ0(\\nFigure 2: Illustration of the geometry of an impulsive encounter along a nearly straight orbit, specifying the coordinate axes and radial vectors used throughout this paper.\\nvelocity vP and impact parameter b. For sufficiently fast encounters (large vP), the deflection of the galaxies from their original orbits due to their mutual gravitational interaction is small and we can approximate the orbits as a straight line. We study the impulsive heating of one of the galaxies (the subject) by the gravitational field of the other (the perturber). Throughout this paper we always assume the perturber to be infinitely extended, while the subject is either truncated or infinitely extended. For simplicity we consider both the perturber and the subject to be spherically symmetric, with density profiles œÅP(r) and œÅS(r), respectively. The masses of the subject and the perturber are denoted by MS and MP respectively, and rS and rP are their scale radii. We take the centre of the unperturbed subject as the origin and define·∫ë to be oriented along the relative velocity vP, and≈∑ perpendicular to·∫ë and directed towards the orbit of the perturber. The position vector of a star belonging to the subject is given by r, that of the COM of the perturber is R and that of the COM of the perturber with respect to the star is RP = R ‚àí r (see Fig. 2).\",\n",
       "    'n_publication_ref': 19,\n",
       "    'n_figure_ref': 4}],\n",
       "  'title': 'A Fully General, Non-Perturbative Treatment of Impulsive Heating',\n",
       "  'abstract': 'Impulsive encounters between astrophysical objects are usually treated using the distant tide approximation (DTA) for which the impact parameter, b, is assumed to be significantly larger than the characteristic radii of the subject, r S , and the perturber, r P . The perturber potential is then expanded as a multipole series and truncated at the quadrupole term. When the perturber is more extended than the subject, this standard approach can be extended to the case where r S b < r P . However, for encounters with b of order r S or smaller, the DTA typically overpredicts the impulse, ‚àÜv, and hence the internal energy change of the subject, ‚àÜE int . This is unfortunate, as these close encounters are, from an astrophysical point of view, the most interesting, potentially leading to tidal capture, mass stripping, or tidal disruption. Another drawback of the DTA is that ‚àÜE int is proportional to the moment of inertia, which diverges unless the subject is truncated or has a density profile that falls off faster than r ‚àí5 . To overcome these shortcomings, this paper presents a fully general, non-perturbative treatment of impulsive encounters which is valid for any impact parameter, and not hampered by divergence issues, thereby negating the necessity to truncate the subject. We present analytical expressions for ‚àÜv for a variety of perturber profiles, and apply our formalism to both straight-path encounters and eccentric orbits.',\n",
       "  'paddleOCR': [[[[1120.0, 197.0],\n",
       "     [1162.0, 213.0],\n",
       "     [1150.0, 248.0],\n",
       "     [1107.0, 232.0]],\n",
       "    ['VP', 0.6177285313606262]],\n",
       "   [[[733.0, 453.0], [752.0, 453.0], [752.0, 479.0], [733.0, 479.0]],\n",
       "    ['R', 0.9983452558517456]],\n",
       "   [[[816.0, 446.0], [836.0, 446.0], [836.0, 473.0], [816.0, 473.0]],\n",
       "    ['b', 0.9289391040802002]],\n",
       "   [[[1055.0, 489.0], [1245.0, 493.0], [1245.0, 526.0], [1054.0, 522.0]],\n",
       "    ['PERTURBER', 0.9951877593994141]],\n",
       "   [[[751.0, 511.0], [770.0, 511.0], [770.0, 537.0], [751.0, 537.0]],\n",
       "    ['R', 0.9975793957710266]],\n",
       "   [[[519.0, 663.0], [659.0, 663.0], [659.0, 693.0], [519.0, 693.0]],\n",
       "    ['SUBJECT', 0.9944671988487244]]],\n",
       "  'ocr': [[[733.0, 453.0], [752.0, 453.0], [752.0, 479.0], [733.0, 479.0]],\n",
       "   ['R', 0.9983452558517456]]},\n",
       " '2102.10590v1-Figure1-1.png': {'caption': 'Fig. 1. A schematic overview of the proposed method for violence detection. The proposed pipeline has two streams consisting of CNN and SepConvLSTM modules. Background suppression and Frame difference are pre-processing modules. The output of the two streams are fused to produce robust Spatiotemporal features.',\n",
       "  'imageText': ['Fusion',\n",
       "   'Classification',\n",
       "   'SepConvLSTM',\n",
       "   'SepConvLSTM',\n",
       "   'CNN',\n",
       "   'CNN',\n",
       "   'Frame',\n",
       "   'Difference',\n",
       "   'Background',\n",
       "   'Suppression'],\n",
       "  'image_file': '2102.10590v1-Figure1-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Efficient Two-Stream Network for Violence Detection Using Separable Convolutional LSTM',\n",
       "  'abstract': '',\n",
       "  'paddleOCR': [[[[198.0, 69.0], [321.0, 69.0], [321.0, 94.0], [198.0, 94.0]],\n",
       "    ['Background', 0.9997352361679077]],\n",
       "   [[[387.0, 86.0], [437.0, 86.0], [437.0, 107.0], [387.0, 107.0]],\n",
       "    ['CNN', 0.9948741793632507]],\n",
       "   [[[501.0, 86.0], [648.0, 85.0], [648.0, 106.0], [501.0, 107.0]],\n",
       "    ['SepConvLSTM', 0.9922569394111633]],\n",
       "   [[[196.0, 101.0], [323.0, 101.0], [323.0, 125.0], [196.0, 125.0]],\n",
       "    ['Suppression', 0.9995678067207336]],\n",
       "   [[[673.0, 191.0], [921.0, 191.0], [921.0, 212.0], [673.0, 212.0]],\n",
       "    ['Fusion Classification', 0.9764959216117859]],\n",
       "   [[[226.0, 282.0], [293.0, 282.0], [293.0, 303.0], [226.0, 303.0]],\n",
       "    ['Frame', 0.998826801776886]],\n",
       "   [[[386.0, 296.0], [437.0, 296.0], [437.0, 319.0], [386.0, 319.0]],\n",
       "    ['CNN', 0.9941854476928711]],\n",
       "   [[[499.0, 297.0], [649.0, 295.0], [649.0, 319.0], [500.0, 321.0]],\n",
       "    ['SepConvLSTM', 0.9979171752929688]],\n",
       "   [[[204.0, 309.0], [314.0, 313.0], [313.0, 336.0], [203.0, 332.0]],\n",
       "    ['Difference', 0.9985901713371277]]],\n",
       "  'ocr': [[[226.0, 282.0], [293.0, 282.0], [293.0, 303.0], [226.0, 303.0]],\n",
       "   ['Frame', 0.998826801776886]]},\n",
       " '2202.02380v1-Figure10-1.png': {'caption': 'Figure 10: Visual representation of the improved crystal graph convolutional neural network (iCGCNN) crystal graph. On the left is an illustration of the Voronoi cell of Atom A, which is connected to its twelve nearest neighbors. On the right is the local environment of A. Each node and edge is embedded with vectors that contain information about the relationship between each constituent atom (vi, vj) and its neighbors (u(i,i)k ,u(i,j)k). Additionally, edge vectors contain information (e.g. solid angle, area, and volume) about the Voronoi polyhedra. Reproduced with permission from Park, C. W.; Wolverton, C. Phys. Rev. Materials 2020, 4 (6), 063801. [47]',\n",
       "  'imageText': [],\n",
       "  'image_file': '2202.02380v1-Figure10-1.png',\n",
       "  'sections': [{'heading': 'Artificial Neural Network (ANN)',\n",
       "    'text': 'The crystal graph convolutional neural network (CGCNN) model can accurately learn material properties from graphical representations of atomic crystal structures, called \"crystal graphs\". Park and On the left is an illustration of the Voronoi cell of Atom A, which is connected to its twelve nearest neighbors. On the right is the local environment of A. Each node and edge is embedded with vectors that contain information about the relationship between each constituent atom (v i , v j ) and its neighbors (u (i,i) k , u (i,j) k ). Additionally, edge vectors contain information (e.g. solid angle, area, and volume) about the Voronoi polyhedra. Reproduced with permission from Park, C. W.; Wolverton, C. Phys. Rev. Materials 2020, 4 (6), 063801. [47] Wolverton [47] designed an improved framework of the CGCNN model, called improved crystal graph convolutional neural network (iCGCNN), which incorporated Voronoi tessellated crystal structures, 3body explicit correlations of neighboring atoms, and an optimized chemical representation of interatomic bonds in the crystal graphs, all of which are absent in CGCNN (Figure 10). First, a training/testing dataset consisting of 180 000 DFT entries from the Open Quantum Materials Database [70] was created.\\nCGCNN and iCGCNN were compared in their accuracy of predicting the thermodynamic stability of inorganic materials. Then, both models were used to conduct separate ML-assisted HiTp searches to discover new stable compounds. The new framework was shown to have 20 % higher accuracy than those of CGCNN on DFT calculated thermodynamic stability and a success rate that is 2.4 times higher than CGCNN. Using iCGCNN, they were also able to identify 97 novel stable compounds from 132 600 screened ThCR 2 Si 2 -type compounds through only 757 DFT calculations which corresponds to a success rate that is 130 times higher than that of an undirected HiTp search.\\nG√≥mez-Bombarelli et al. [6] screened 40 000 organic light-emitting diode (OLED) molecules with thermally activated delayed fluorescence (TADF) character randomly selected from a library of 1.6 million software-generated candidates using an ANN combined with BO. Then, the highest-ranking molecules based on external quantum efficiency (EQE) predicted by the ANN were promoted to timedependent density functional theory (TD-DFT) simulation. After BO, 400 000 molecules were screened in total. Results from the TD-DFT simulation found thousands of emitters predicted to be highly efficient, with about 900 being extremely promising. The top candidates, chosen by humans, were then validated using experimental synthesis. G√≥mez-Bombarelli et al. [6] was able to perform an integrated high-throughput virtual screening method targeting novel TADF OLED emitters, which resulted in the discovery of new devices up to 22 % EQE, which can be applied to other areas of organic electronics.',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Data-Driven Materials Discovery and Synthesis using Machine Learning Methods',\n",
       "  'abstract': 'Experimentally  and computationally [39][40][41][42][43][44][45][46][47][48][49][50] validated machine learning (ML) articles are sorted based on size of the training data: 1-100, 101-10 000, and 10 000+ in a comprehensive set summarizing legacy and recent advances in the field. The review emphasizes the interrelated fields of synthesis, characterization, and prediction. Size range 1-100 consists mostly of Bayesian optimization (BO) articles, whereas 101-10 000 consists mostly of support vector machine (SVM) articles. The articles often use combinations of ML, feature selection (FS), adaptive design (AD), high-throughput (HiTp) techniques, and domain knowledge to enhance predictive performance and/or model interpretability. Grouping cross-validation (G-CV) techniques curb overly optimistic extrapolative predictive performance. Smaller',\n",
       "  'paddleOCR': [[[[1027.0, 13.0],\n",
       "     [1072.0, 13.0],\n",
       "     [1072.0, 60.0],\n",
       "     [1027.0, 60.0]],\n",
       "    ['Vj', 0.9547467231750488]],\n",
       "   [[[944.0, 83.0], [1177.0, 83.0], [1177.0, 121.0], [944.0, 121.0]],\n",
       "    ['Embedding of', 0.9754571914672852]],\n",
       "   [[[961.0, 126.0], [1155.0, 130.0], [1154.0, 170.0], [960.0, 166.0]],\n",
       "    ['atom typej', 0.96649169921875]],\n",
       "   [[[185.0, 149.0], [583.0, 149.0], [583.0, 186.0], [185.0, 186.0]],\n",
       "    ['Voronoi cell of atom A', 0.9382485747337341]],\n",
       "   [[[1029.0, 286.0], [1075.0, 286.0], [1075.0, 330.0], [1029.0, 330.0]],\n",
       "    ['Vi', 0.9817408919334412]],\n",
       "   [[[945.0, 344.0], [1179.0, 349.0], [1179.0, 390.0], [944.0, 386.0]],\n",
       "    ['Embedding of', 0.9669987559318542]],\n",
       "   [[[1300.0, 335.0], [1725.0, 337.0], [1725.0, 377.0], [1299.0, 375.0]],\n",
       "    ['Crystal graph of atom A', 0.948783278465271]],\n",
       "   [[[963.0, 392.0], [1157.0, 396.0], [1156.0, 435.0], [962.0, 431.0]],\n",
       "    ['atom type i', 0.9533782601356506]],\n",
       "   [[[1507.0, 473.0], [1538.0, 473.0], [1538.0, 507.0], [1507.0, 507.0]],\n",
       "    ['A', 0.999596893787384]],\n",
       "   [[[1173.0, 642.0], [1297.0, 659.0], [1291.0, 708.0], [1166.0, 691.0]],\n",
       "    ['u(i,i)k', 0.8648017048835754]],\n",
       "   [[[1048.0, 710.0], [1416.0, 714.0], [1416.0, 754.0], [1048.0, 750.0]],\n",
       "    ['Embedding of kth edge', 0.9725704193115234]],\n",
       "   [[[1101.0, 763.0], [1362.0, 763.0], [1362.0, 795.0], [1101.0, 795.0]],\n",
       "    ['connecting i&i', 0.9595142006874084]],\n",
       "   [[[1169.0, 849.0], [1298.0, 868.0], [1291.0, 917.0], [1162.0, 898.0]],\n",
       "    ['u(i,j)k', 0.7399891018867493]],\n",
       "   [[[1042.0, 917.0], [1412.0, 919.0], [1412.0, 959.0], [1042.0, 957.0]],\n",
       "    ['Embedding of kth edge', 0.9551660418510437]],\n",
       "   [[[1090.0, 966.0], [1355.0, 966.0], [1355.0, 1004.0], [1090.0, 1004.0]],\n",
       "    ['connectingi&j', 0.9833195209503174]],\n",
       "   [[[543.0, 987.0], [1035.0, 991.0], [1035.0, 1030.0], [542.0, 1027.0]],\n",
       "    ['Voronoi polyhedra subtending', 0.981890857219696]],\n",
       "   [[[616.0, 1038.0], [963.0, 1038.0], [963.0, 1070.0], [616.0, 1070.0]],\n",
       "    ['to neighboring atoms', 0.9672149419784546]]],\n",
       "  'ocr': [[[1029.0, 286.0], [1075.0, 286.0], [1075.0, 330.0], [1029.0, 330.0]],\n",
       "   ['Vi', 0.9817408919334412]]},\n",
       " '2110.15569v1-Figure1-1.png': {'caption': 'Fig. 1. The pipeline of the proposed method. The training are divided into two stages, and only a single view is required in training. In the first stage, the feature extracted from a source viewpoint image is transformed into the intrinsic representation with respect to a pre-defined reference pose, and the view transformation between reference pose and a source pose is learned. In the second stage, the view transformation between reference pose and a random pose is first learned, and then a reverse mapping strategy is introduced to further fine-tune the network. In synthesis, an intrinsic representation is obtained from a single viewpoint image without need for knowing its pose and a novel view of an arbitrary pose is synthesized from the intrinsic representation.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2110.15569v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'I. INTRODUCTION',\n",
       "    'text': 'N OVEL view synthesis (NVS) aims to generate an unknown-view from a single or multiple source views. Many methods have been developed to synthesize a novel view from multiple views [1]- [2]. Recently, methods are also explored to synthesize a novel view from a single source view [3]- [5]. The key underlying mechanism of these methods for synthesis from a single view is to learn a view transformation, either 2D or 3D, between a source view and a target view. Such a transformation is often learned from paired views in which one view is treated as a target view to serve as a supervising signal and the other view is considered as the source from which the target view is synthesized. The learned transformation allows us to synthesize a novel view from a single source view of known pose. However, camera pose information of the single source view must be provided for  and C. Yu are with the School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China (e-mail: bzliu@tju.edu.cn; jjlei@tju.edu.cn; bpeng@tju.edu.cn; cbyu@tju.edu.cn).\\nW. Li is with the Advanced Multimedia Research Lab, University of Wollongong, Wollongong, Australia (e-mail: wanqing@uow.edu.au).\\nN. Ling is with the Department of Computer Science and Engineering, Santa Clara University, Santa Clara, CA 95053 USA (e-mail: nling@scu.edu).\\nDigital Object Identifier In the first stage, the feature extracted from a source viewpoint image is transformed into the intrinsic representation with respect to a pre-defined reference pose, and the view transformation between reference pose and a source pose is learned.\\nIn the second stage, the view transformation between reference pose and a random pose is first learned, and then a reverse mapping strategy is introduced to further fine-tune the network. In synthesis, an intrinsic representation is obtained from a single viewpoint image without need for knowing its pose and a novel view of an arbitrary pose is synthesized from the intrinsic representation.\\nthe synthesis of a novel view. In other words, only the views with pose information can be chosen as input in synthesis.\\nIn a practical multi-view scenario [6]- [7], such as broadcasting of a sports event, multiple source views are captured by a set of fixed source cameras with known poses. At the same time, there are also a few moving cameras in the scene that dynamically follows the important part of the event. It is a desirable and appealing feature if a novel view can be generated from the views taken by a moving camera or a hand-held camera in the scene. Since it is usually difficult to obtain the pose information of these moving cameras or hand-held cameras in real-time, existing methods for novel view synthesis from a single view are not applicable because they must be provided with the pose information of the input single source view.\\nTo address this limitation, this paper proposes an unsupervised network that is able to synthesize a novel view from a single source viewpoint image without requiring the pose information of the source view. The key idea is to learn a view transformation between a pose and a pre-defined reference arXiv:2110.15569v1 [cs.CV] 29 Oct 2021 pose. To this end, the proposed network mainly consists (a) a specially designed token transformation module (TTM) that maps the features of any input source viewpoint image (with unknown pose information) to an intrinsic representation with respect to a reference pose, (b) a view generation module (VGM) that reconstructs an explicit occupancy volume with respect to the reference pose, rotates the volume explicitly to a target pose to generate the target view. The network is trained in an unsupervised manner. In particular, a reverse mapping strategy is introduced to improve the training. Compared to the existing methods for synthesizing novel views from a single view, the proposed unsupervised network has two advantages. First, it only requires a source viewpoint image without pose information during inference for view synthesis. Second, the network is trained using a single view, rather than paired views with different poses as most existing methods do. The pipeline of the proposed method is shown in Fig. 1.\\nIn summary, the main contributions of this paper include: 1) A new unsupervised network is proposed for novel view synthesis from a single image. Unlike existing methods, it does not require pose information of the single source view during synthesis. Therefore, choice of the single input viewpoint image in synthesis is not limited to the views captured by fixed source cameras and it can be an arbitrarily viewpoint image captured by a non-source camera.\\n2) A token transformation module is developed to learn an intrinsic representation and a view generation module is developed to synthesize novel views from the intrinsic representation.\\n3) A two-stage unsupervised training is proposed in which the network is first trained using individual view and then fined-tuned with a reverse mapping strategy as detailed in Section II-D.\\n4) Experiments compared with state-of-the-art methods on both synthetic and real datasets have demonstrated the effectiveness of the proposed network.\\nThe rest of this paper is organized as follows. Section II reviews the related works. Section III introduces the detail of the proposed method. The experimental results and analysis are presented in Section IV. Finally, Section V concludes this paper.',\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Novel View Synthesis from a Single Image via Unsupervised Learning',\n",
       "  'abstract': 'View synthesis aims to generate novel views from one or more given source views. Although existing methods have achieved promising performance, they usually require paired views of different poses to learn a pixel transformation. This paper proposes an unsupervised network to learn such a pixel transformation from a single source viewpoint. In particular, the network consists of a token transformation module (TTM) that facilities the transformation of the features extracted from a source viewpoint image into an intrinsic representation with respect to a pre-defined reference pose and a view generation module (VGM) that synthesizes an arbitrary view from the representation. The learned transformation allows us to synthesize a novel view from any single source viewpoint image of unknown pose. Experiments on the widely used view synthesis datasets have demonstrated that the proposed network is able to produce comparable results to the state-of-the-art methods despite the fact that learning is unsupervised and only a single source viewpoint image is required for generating a novel view. The code will be available soon.',\n",
       "  'paddleOCR': [[[[527.0, 46.0], [602.0, 46.0], [602.0, 74.0], [527.0, 74.0]],\n",
       "    ['Novel', 0.9997836351394653]],\n",
       "   [[[534.0, 79.0], [593.0, 82.0], [592.0, 107.0], [533.0, 104.0]],\n",
       "    ['Pose', 0.9998446702957153]],\n",
       "   [[[891.0, 78.0], [1029.0, 76.0], [1030.0, 102.0], [891.0, 105.0]],\n",
       "    ['Synthesized', 0.9999105334281921]],\n",
       "   [[[892.0, 112.0], [1028.0, 112.0], [1028.0, 135.0], [892.0, 135.0]],\n",
       "    ['Novel View', 0.9588180780410767]],\n",
       "   [[[569.0, 243.0], [742.0, 243.0], [742.0, 265.0], [569.0, 265.0]],\n",
       "    ['Reference Pose', 0.9810879826545715]],\n",
       "   [[[892.0, 308.0], [1030.0, 308.0], [1030.0, 335.0], [892.0, 335.0]],\n",
       "    ['Synthesized', 0.9998888373374939]],\n",
       "   [[[541.0, 321.0], [636.0, 380.0], [620.0, 406.0], [524.0, 347.0]],\n",
       "    ['Training', 0.9999239444732666]],\n",
       "   [[[57.0, 346.0], [140.0, 349.0], [139.0, 373.0], [56.0, 371.0]],\n",
       "    ['Source', 0.9993241429328918]],\n",
       "   [[[921.0, 343.0], [1002.0, 343.0], [1002.0, 368.0], [921.0, 368.0]],\n",
       "    ['Source', 0.9993476867675781]],\n",
       "   [[[319.0, 364.0], [418.0, 364.0], [418.0, 391.0], [319.0, 391.0]],\n",
       "    ['Intrinsic', 0.9995208382606506]],\n",
       "   [[[40.0, 381.0], [160.0, 381.0], [160.0, 408.0], [40.0, 408.0]],\n",
       "    ['Viewpoint', 0.9999088644981384]],\n",
       "   [[[924.0, 374.0], [1000.0, 374.0], [1000.0, 402.0], [924.0, 402.0]],\n",
       "    ['view /', 0.871819019317627]],\n",
       "   [[[284.0, 401.0], [453.0, 401.0], [453.0, 424.0], [284.0, 424.0]],\n",
       "    ['Representation', 0.9999417066574097]],\n",
       "   [[[61.0, 411.0], [137.0, 417.0], [135.0, 446.0], [59.0, 440.0]],\n",
       "    ['Image', 0.9995890855789185]],\n",
       "   [[[529.0, 411.0], [610.0, 411.0], [610.0, 434.0], [529.0, 434.0]],\n",
       "    ['Source', 0.9994621276855469]],\n",
       "   [[[890.0, 410.0], [1029.0, 407.0], [1030.0, 433.0], [890.0, 436.0]],\n",
       "    ['Synthesized', 0.9999182224273682]],\n",
       "   [[[532.0, 447.0], [606.0, 442.0], [608.0, 466.0], [534.0, 472.0]],\n",
       "    ['pose/', 0.9983318448066711]],\n",
       "   [[[910.0, 442.0], [1009.0, 442.0], [1009.0, 469.0], [910.0, 469.0]],\n",
       "    ['Random', 0.9992454648017883]],\n",
       "   [[[519.0, 477.0], [617.0, 477.0], [617.0, 503.0], [519.0, 503.0]],\n",
       "    ['Random', 0.9993641376495361]],\n",
       "   [[[930.0, 475.0], [994.0, 475.0], [994.0, 503.0], [930.0, 503.0]],\n",
       "    ['View', 0.9997535347938538]],\n",
       "   [[[539.0, 510.0], [600.0, 510.0], [600.0, 537.0], [539.0, 537.0]],\n",
       "    ['Pose', 0.9998201727867126]],\n",
       "   [[[33.0, 728.0], [171.0, 724.0], [172.0, 751.0], [34.0, 755.0]],\n",
       "    ['Synthesized', 0.9999222159385681]],\n",
       "   [[[319.0, 729.0], [416.0, 732.0], [415.0, 756.0], [319.0, 754.0]],\n",
       "    ['Intrinsic', 0.9998437166213989]],\n",
       "   [[[29.0, 760.0], [174.0, 760.0], [174.0, 784.0], [29.0, 784.0]],\n",
       "    ['Source View', 0.999184250831604]],\n",
       "   [[[283.0, 765.0], [453.0, 765.0], [453.0, 791.0], [283.0, 791.0]],\n",
       "    ['Representation', 0.9999256134033203]]],\n",
       "  'ocr': [[[892.0, 308.0], [1030.0, 308.0], [1030.0, 335.0], [892.0, 335.0]],\n",
       "   ['Synthesized', 0.9998888373374939]]},\n",
       " '203952961-Figure1-1.png': {'caption': 'Figure 1. Scheme of the proposed method.',\n",
       "  'imageText': [],\n",
       "  'image_file': '203952961-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Proposed method',\n",
       "    'text': 'In short, our method can be characterized as MixMatch with EfficientNet backbone. In this section we present a brief description of these 2 architectures and our contribution to their application to the contest tasks. General scheme of our approach is shown in Figure 1.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'MixMatch Domain Adaptaion: Prize-winning solution for both tracks of VisDA 2019 challenge',\n",
       "  'abstract': 'We present a domain adaptation (DA) system that can be used in multi-source and semi-supervised settings. Using the proposed method we achieved 2nd place on multisource track and 3rd place on semi-supervised track of the VisDA 2019 challenge 1 . The source code of out method is available publicly 2 .',\n",
       "  'paddleOCR': [[[[503.0, 58.0], [608.0, 63.0], [606.0, 95.0], [501.0, 90.0]],\n",
       "    ['images', 0.9995827674865723]],\n",
       "   [[[1857.0, 58.0], [1941.0, 58.0], [1941.0, 88.0], [1857.0, 88.0]],\n",
       "    ['labels', 0.9902231097221375]],\n",
       "   [[[757.0, 189.0], [877.0, 194.0], [876.0, 233.0], [756.0, 228.0]],\n",
       "    ['classify', 0.9997461438179016]],\n",
       "   [[[804.0, 263.0], [832.0, 263.0], [832.0, 295.0], [804.0, 295.0]],\n",
       "    ['A', 0.9707980751991272]],\n",
       "   [[[287.0, 308.0], [476.0, 308.0], [476.0, 336.0], [287.0, 336.0]],\n",
       "    ['augmentations', 0.9997913837432861]],\n",
       "   [[[713.0, 304.0], [918.0, 304.0], [918.0, 334.0], [713.0, 334.0]],\n",
       "    ['shared weights', 0.9997490048408508]],\n",
       "   [[[804.0, 338.0], [832.0, 338.0], [832.0, 373.0], [804.0, 373.0]],\n",
       "    ['A', 0.9789055585861206]],\n",
       "   [[[1311.0, 366.0], [1429.0, 371.0], [1427.0, 401.0], [1309.0, 396.0]],\n",
       "    ['average', 0.9997056722640991]],\n",
       "   [[[1618.0, 373.0], [1730.0, 373.0], [1730.0, 396.0], [1618.0, 396.0]],\n",
       "    ['sharpen', 0.9993857145309448]],\n",
       "   [[[757.0, 407.0], [877.0, 412.0], [876.0, 451.0], [756.0, 446.0]],\n",
       "    ['classify', 0.9998308420181274]],\n",
       "   [[[103.0, 427.0], [187.0, 427.0], [187.0, 457.0], [103.0, 457.0]],\n",
       "    ['target', 0.9999063611030579]],\n",
       "   [[[804.0, 481.0], [832.0, 481.0], [832.0, 513.0], [804.0, 513.0]],\n",
       "    ['A', 0.9246004819869995]],\n",
       "   [[[713.0, 526.0], [920.0, 526.0], [920.0, 556.0], [713.0, 556.0]],\n",
       "    ['shared weights', 0.9992002844810486]],\n",
       "   [[[1036.0, 694.0], [1122.0, 694.0], [1122.0, 724.0], [1036.0, 724.0]],\n",
       "    ['labels', 0.9864978790283203]],\n",
       "   [[[110.0, 750.0], [202.0, 750.0], [202.0, 774.0], [110.0, 774.0]],\n",
       "    ['source', 0.9994730949401855]],\n",
       "   [[[426.0, 804.0], [674.0, 802.0], [674.0, 838.0], [427.0, 841.0]],\n",
       "    [' mixup and shuffle', 0.980891764163971]],\n",
       "   [[[1775.0, 806.0], [2018.0, 806.0], [2018.0, 836.0], [1775.0, 836.0]],\n",
       "    ['mixup and shuffle', 0.9836360812187195]],\n",
       "   [[[545.0, 840.0], [567.0, 840.0], [567.0, 860.0], [545.0, 860.0]],\n",
       "    ['V', 0.5296028852462769]],\n",
       "   [[[1887.0, 851.0], [1900.0, 838.0], [1911.0, 849.0], [1898.0, 862.0]],\n",
       "    ['K', 0.7946096062660217]],\n",
       "   [[[758.0, 974.0], [875.0, 974.0], [875.0, 1013.0], [758.0, 1013.0]],\n",
       "    ['classify', 0.942518413066864]],\n",
       "   [[[1517.0, 978.0], [1588.0, 978.0], [1588.0, 1008.0], [1517.0, 1008.0]],\n",
       "    ['MSE', 0.9984727501869202]],\n",
       "   [[[804.0, 1043.0], [829.0, 1043.0], [829.0, 1075.0], [804.0, 1075.0]],\n",
       "    ['A', 0.9495221972465515]],\n",
       "   [[[713.0, 1084.0], [918.0, 1084.0], [918.0, 1114.0], [713.0, 1114.0]],\n",
       "    ['shared weights', 0.9996818900108337]],\n",
       "   [[[804.0, 1123.0], [829.0, 1123.0], [829.0, 1157.0], [804.0, 1157.0]],\n",
       "    ['A', 0.8253004550933838]],\n",
       "   [[[758.0, 1192.0], [875.0, 1192.0], [875.0, 1230.0], [758.0, 1230.0]],\n",
       "    ['classify', 0.9997949004173279]],\n",
       "   [[[1519.0, 1194.0], [1590.0, 1194.0], [1590.0, 1226.0], [1519.0, 1226.0]],\n",
       "    ['MSE', 0.9987006187438965]],\n",
       "   [[[804.0, 1261.0], [832.0, 1261.0], [832.0, 1297.0], [804.0, 1297.0]],\n",
       "    ['A', 0.9147890210151672]],\n",
       "   [[[713.0, 1299.0], [920.0, 1299.0], [920.0, 1330.0], [713.0, 1330.0]],\n",
       "    ['shared weights', 0.9998317956924438]],\n",
       "   [[[804.0, 1338.0], [829.0, 1338.0], [829.0, 1377.0], [804.0, 1377.0]],\n",
       "    ['A...', 0.8162972927093506]],\n",
       "   [[[757.0, 1404.0], [877.0, 1410.0], [876.0, 1449.0], [756.0, 1443.0]],\n",
       "    ['classify', 0.9998061656951904]],\n",
       "   [[[1508.0, 1401.0], [1590.0, 1401.0], [1590.0, 1424.0], [1508.0, 1424.0]],\n",
       "    ['cross-', 0.974064290523529]],\n",
       "   [[[1499.0, 1428.0], [1606.0, 1436.0], [1603.0, 1468.0], [1496.0, 1460.0]],\n",
       "    ['entropy', 0.9996673464775085]]],\n",
       "  'ocr': [[[713.0, 1084.0], [918.0, 1084.0], [918.0, 1114.0], [713.0, 1114.0]],\n",
       "   ['shared weights', 0.9996818900108337]]},\n",
       " '2102.13346v4-Figure1-1.png': {'caption': 'Fig. 1. Illustration of ‚ÄúKeeping the lights on‚Äù for an IDS via SMESSs and DR.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2102.13346v4-Figure1-1.png',\n",
       "  'sections': [{'heading': 'II. THE SURVIVABILITY-ORIENTED STRATEGY',\n",
       "    'text': 'A general scenario of IDS is shown in Fig. 1, where the local area customers lose the continuous supply from the normal power sources (i.e., the substation and the REG) but have backup small-capacity fossil-fuel-based generation (FFG) within the IDS. In addition, we further assume such an extreme condition that limited fuel is stored in the IDS without any supplement from outside. This scenario can be simply revised to represent any other required scenarios, such as a case where an IDS, which is normally supplied only by the REG in a remote area, loses the supply from the REG by removing the substation node. The model in the following sections can also be simply revised accordingly. Then, a two-pronged strategy to enhance the survivability of the IDS is described as follows: 1) From the IDS\\'s external point of view, SMESSs are scheduled to construct non-wires links for energy transmission from the outside \"stranded\" sources to the IDS. In addition, SMESSs can even realize a continuous power supply for the IDS, provided output of the Mods and traveling behavior of the Carrs are well scheduled. 2) From the IDS\\'s internal point of view, DR is scheduled to relieve the energy and power shortages that may arise in the operation of IDS by reducing the demand in the allowable range. Considering that rapid response to the DR request from the IDS operator is beneficial and expected under such an emergency circumstance, in our strategy, the fully dispatchable DR is used, e.g., direct load control (DLC), which can be executed directly by the operator, as in [11] and [15].',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'This work has been submitted to the IEEE for possible publication',\n",
       "  'abstract': 'Extreme circumstances in which a local distribution system is electrically isolated from the main power supply may not always be avoidable. Efforts must be made to keep the lights on for such an isolated distribution system (IDS) until reconnection to the main power source. In this paper, we propose a strategy to enhance IDS survivability utilizing the coordination of two flexible approaches, namely, separable mobile energy storage systems (SMESSs), which construct non-wires links for energy transmission between the IDS and the external live power sources, and demand response (DR), which adjusts the internal electrical demand of the IDS to provide effective operating stress alleviation. Considering the uncertainty of renewable energy generation and loads, a two-stage robust optimization (RO) model involving the joint scheduling of these two approaches is constructed. The objective is to minimize the fuel consumption and the decreased and unserved demand under the worst-case scenario to endow the IDS with extended survivability. Finally, test is conducted and the results demonstrate the effectiveness of the proposed method in enhancing the survivability of IDS.',\n",
       "  'paddleOCR': [[[[339.0, 46.0], [400.0, 46.0], [400.0, 70.0], [339.0, 70.0]],\n",
       "    ['REG', 0.9928045868873596]],\n",
       "   [[[842.0, 41.0], [974.0, 41.0], [974.0, 62.0], [842.0, 62.0]],\n",
       "    ['Mods1,2,3.', 0.8492894172668457]],\n",
       "   [[[836.0, 72.0], [898.0, 68.0], [900.0, 91.0], [837.0, 95.0]],\n",
       "    ['and 4', 0.9966721534729004]],\n",
       "   [[[546.0, 88.0], [638.0, 88.0], [638.0, 107.0], [546.0, 107.0]],\n",
       "    ['SMESSs', 0.9820790886878967]],\n",
       "   [[[296.0, 110.0], [371.0, 115.0], [369.0, 137.0], [294.0, 132.0]],\n",
       "    ['Charge', 0.9985573887825012]],\n",
       "   [[[846.0, 120.0], [967.0, 120.0], [967.0, 138.0], [846.0, 138.0]],\n",
       "    ['Carr 1and2', 0.9652988314628601]],\n",
       "   [[[57.0, 167.0], [156.0, 167.0], [156.0, 186.0], [57.0, 186.0]],\n",
       "    ['SMESSs:', 0.9484131932258606]],\n",
       "   [[[722.0, 176.0], [879.0, 181.0], [878.0, 204.0], [721.0, 199.0]],\n",
       "    ['Lines in outage', 0.9702350497245789]],\n",
       "   [[[16.0, 195.0], [197.0, 199.0], [197.0, 221.0], [15.0, 216.0]],\n",
       "    ['Non-wires energy', 0.9938228726387024]],\n",
       "   [[[672.0, 212.0], [947.0, 212.0], [947.0, 230.0], [672.0, 230.0]],\n",
       "    ['Nodes and branches of IDS', 0.9554146528244019]],\n",
       "   [[[43.0, 227.0], [172.0, 227.0], [172.0, 248.0], [43.0, 248.0]],\n",
       "    ['transmission', 0.9984998106956482]],\n",
       "   [[[901.0, 246.0], [948.0, 246.0], [948.0, 270.0], [901.0, 270.0]],\n",
       "    ['DR:', 0.9993619918823242]],\n",
       "   [[[843.0, 272.0], [1007.0, 274.0], [1006.0, 297.0], [843.0, 294.0]],\n",
       "    ['Operating stress', 0.9892147779464722]],\n",
       "   [[[869.0, 295.0], [980.0, 299.0], [979.0, 321.0], [869.0, 318.0]],\n",
       "    ['alleviation', 0.9975713491439819]],\n",
       "   [[[486.0, 306.0], [590.0, 311.0], [589.0, 333.0], [485.0, 328.0]],\n",
       "    ['Discharge', 0.9980334043502808]],\n",
       "   [[[87.0, 359.0], [176.0, 363.0], [175.0, 389.0], [86.0, 385.0]],\n",
       "    ['Trans-', 0.9983753561973572]],\n",
       "   [[[745.0, 352.0], [799.0, 352.0], [799.0, 377.0], [745.0, 377.0]],\n",
       "    ['IDS', 0.9954623579978943]],\n",
       "   [[[82.0, 397.0], [181.0, 397.0], [181.0, 423.0], [82.0, 423.0]],\n",
       "    ['mission', 0.9991862177848816]],\n",
       "   [[[83.0, 431.0], [180.0, 435.0], [179.0, 465.0], [82.0, 461.0]],\n",
       "    ['System', 0.9995834827423096]],\n",
       "   [[[660.0, 437.0], [701.0, 437.0], [701.0, 457.0], [660.0, 457.0]],\n",
       "    ['FFG', 0.9959775805473328]],\n",
       "   [[[830.0, 441.0], [1018.0, 441.0], [1018.0, 463.0], [830.0, 463.0]],\n",
       "    ['Pre-and post-DR', 0.9713678956031799]],\n",
       "   [[[833.0, 467.0], [1020.0, 467.0], [1020.0, 489.0], [833.0, 489.0]],\n",
       "    ['demand at a node', 0.9507607221603394]]],\n",
       "  'ocr': [[[16.0, 195.0], [197.0, 199.0], [197.0, 221.0], [15.0, 216.0]],\n",
       "   ['Non-wires energy', 0.9938228726387024]]},\n",
       " '2102.02111v2-Figure12-1.png': {'caption': 'Figure 12. A Recurrent Neural Network. Architecture of a basic RNN unfolded through time. At time step t, the hidden state ht is a function of the previous hidden state, ht‚àí1, and current input embedding z[at]. yt is the output produced at t.',\n",
       "  'imageText': ['#[&!]',\n",
       "   '#[&\"]',\n",
       "   '#[&#$!]',\n",
       "   '#[&#]',\n",
       "   '‚Ä¶‚Ñé#',\n",
       "   '\"#',\n",
       "   '‚Ä¶',\n",
       "   '\"!',\n",
       "   '\"\"',\n",
       "   '\"#$!',\n",
       "   '‚Ñé!',\n",
       "   '‚Ñé\"',\n",
       "   '‚Ñé#$!'],\n",
       "  'image_file': '2102.02111v2-Figure12-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis',\n",
       "  'abstract': 'Transformer-based models for transfer learning have the potential to achieve high prediction accuracies on text-based supervised learning tasks with relatively few training data instances. These models are thus likely to benefit social scientists that seek to have as accurate as possible text-based measures but only have limited resources for annotating training data. To enable social scientists to leverage these potential benefits for their research, this paper explains how these methods work, why they might be advantageous, and what their limitations are. Additionally, three Transformer-based models for transfer learning, BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019, and the Longformer (Beltagy et al. 2020), are compared to conventional machine learning algorithms on three applications. Across all evaluated tasks, textual styles, and training data set sizes, the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to textbased social science research.',\n",
       "  'paddleOCR': [[[[34.0, 52.0], [87.0, 52.0], [87.0, 96.0], [34.0, 96.0]],\n",
       "    ['Y1', 0.8685296773910522]],\n",
       "   [[[231.0, 49.0], [288.0, 51.0], [286.0, 98.0], [229.0, 96.0]],\n",
       "    ['Y2', 0.8717578649520874]],\n",
       "   [[[532.0, 52.0], [627.0, 57.0], [625.0, 98.0], [530.0, 93.0]],\n",
       "    ['Yt-1', 0.9843248128890991]],\n",
       "   [[[745.0, 57.0], [795.0, 57.0], [795.0, 101.0], [745.0, 101.0]],\n",
       "    ['Yt', 0.9206829071044922]],\n",
       "   [[[42.0, 233.0], [74.0, 233.0], [74.0, 269.0], [42.0, 269.0]],\n",
       "    ['h.', 0.7668313980102539]],\n",
       "   [[[597.0, 256.0], [620.0, 256.0], [620.0, 271.0], [597.0, 271.0]],\n",
       "    ['1', 0.7793996334075928]],\n",
       "   [[[24.0, 418.0], [125.0, 430.0], [119.0, 479.0], [18.0, 467.0]],\n",
       "    ['Z[a1]', 0.9884635210037231]],\n",
       "   [[[224.0, 418.0], [324.0, 430.0], [318.0, 478.0], [219.0, 466.0]],\n",
       "    ['Z[a2]', 0.9957830309867859]],\n",
       "   [[[521.0, 421.0], [646.0, 432.0], [642.0, 476.0], [517.0, 465.0]],\n",
       "    ['Z[at-1]', 0.9941464066505432]],\n",
       "   [[[736.0, 418.0], [833.0, 429.0], [827.0, 477.0], [730.0, 466.0]],\n",
       "    ['Z[at]', 0.9970360994338989]]],\n",
       "  'ocr': [[[532.0, 52.0], [627.0, 57.0], [625.0, 98.0], [530.0, 93.0]],\n",
       "   ['Yt-1', 0.9843248128890991]]},\n",
       " '2011.04337v1-Figure1-1.png': {'caption': 'Fig. 1: Deep CTL architecture. The illustration is given for L = 2 layers, with the first layer T1 composed of M1 = 4 filters of size 5√ó 1, and the second layer composed of M2 = 8 filters of size 3√ó 1.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2011.04337v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Deep Convolutional Transform Learning',\n",
       "    'text': 'Deep CTL consists of stacking multiple convolutional layers on top of each other to generate the features, as shown in Figure 1. To learn all the variables in an end-to-end fashion, deep CTL relies on the key property that the solution X to the CTL problem, assuming fixed filters T , can be reformulated as the simple application of an element-wise activation function, that is with œÜ the proximity operator of Œ® [41]. For example, if Œ® is the indicator function of the positive orthant, then œÜ identifies with the famous rectified linear unit (ReLU) activation function. Many other examples are provided in [41]. Consequently, deep features can be computed by stacking many such layers\\nargmin X F (T, X) = œÜ(T * S),(7)\\n(‚àÄ ‚àà {1, . . . , L ‚àí 1}) X = œÜ (T * X ‚àí1 ),(8)\\nwhere X 0 = S and œÜ a given activation function for layer . Putting all together, deep CTL amounts to minimize T1,...,T L ,X\\nF conv (T 1 , . . . , T L , X | S)(9)\\nwhere\\nF conv (T 1 , . . . , T L , X | S) = 1 2 T L * œÜ L‚àí1 (T L‚àí1 * . . . œÜ 1 (T 1 * S)) ‚àí X 2 F + Œ®(X) + L =1 (¬µ||T || 2 F ‚àíŒª log det(T )).(10)\\nThis is a direct extension of the one-layer formulation in (4).',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Experimental Evaluation',\n",
       "    'text': \"We carry out experiments on the real world problem of stock forecasting and trading. The problem of stock forecasting is a regression problem aiming at estimating the price of a stock at a future date (next day for our problem) given inputs till the current date. Stock trading is a classification problem, where the decision whether to buy or sell a stock has to be taken at each time. The two problems are related by the fact that simple logic dictates that if the price of a stock at a later date is expected to increase, the stock must be bought; and if the stock price is expected to go down, the stock must be sold. We will use the five raw inputs for both the tasks, namely open price, close price, high, low and net asset value (NAV). One could compute technical indicators based on the raw inputs [17] but, in keeping with the essence of true representation learning, we chose to stay with those raw values. Each of the five inputs is processed by a separate 1D processing pipeline. Each of the pipelines produces a flattened output (Fig. 1). The flattened outputs are then concatenated and fed into the Transform Learning layer acting as the fully connected layer (Fig. 2) for fusion. While our processing pipeline ends here (being unsupervised), the benchmark techniques are supervised and have an output node. The node is binary (buy / sell) for classification and real valued for regression. More precisely, we will compare with two state-of-the-art time series analysis models, namely TimeNet [12] and ConvTimeNet [13]. In the former, the processing individual processing pipelines are based on LSTM and in the later they use 1D CNN.\\nWe make use of a real dataset from the National Stock Exchange (NSE) of India. The dataset contains information of 150 symbols between 2014 and 2018; these stocks were chosen after filtering out stocks that had less than three years of data. The companies available in the dataset are from various sectors such as IT (e.g., TCS, INFY), automobile (e.g., HEROMOTOCO, TATAMOTORS), bank (e.g., HDFCBANK, ICICIBANK), coal and petroleum (e.g., OIL, ONGC), steel (e.g., JSWSTEEL, TATASTEEL), construction (e.g., ABIRLANUVO, ACC), public sector units (e.g., POWERGRID, GAIL). The detailed architectures for each tested techniques, namely DeConFuse, ConvTimeNet and TimeNet are presented in the Table 2. For DeConFuse, TimeNet and ConvTimeNet, we have tuned the architectures to yield the best performance and have randomly initialized the weights for each stock's training. \\nDeConFuse 5 √ó \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 layer1 : 1D Conv(1, 4, 5, 1, 2) 1 Maxpool(2, 2) 2 SELU layer2 : 1D Conv(5, 8, 3, 1, 1) 1 layer3 : Fully Connected\\nLearning Rate = 0.001, ¬µ = 0.01, = 0.0001 Optimizer Used: Adam **with parameters** (Œ≤1, Œ≤2) = (0.9, 0.999), weight decay = 5e-5, epsilon = 1e-8 \\nConvTimeNet 5√ó \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 layer1 : 1D Convolution(1,\",\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'DeConFuse : A Deep Convolutional Transform based Unsupervised Fusion Framework',\n",
       "  'abstract': 'This work proposes an unsupervised fusion framework based on deep convolutional transform learning. The great learning ability of convolutional filters for data analysis is well acknowledged. The success of convolutive features owes to convolutional neural network (CNN). However, CNN cannot perform learning tasks in an unsupervised fashion. In a recent work, we show that such shortcoming can be addressed by adopting a convolutional transform learning (CTL) approach, where convolutional filters are learnt in an unsupervised fashion. The present paper aims at (i) proposing a deep version of CTL; (ii) proposing an unsupervised fusion formulation taking advantage of the proposed deep CTL representation; (iii) developing a mathematically sounded optimization strategy for performing the learning task. We apply the proposed technique, named DeConFuse, on the problem of stock forecasting and trading. Comparison with state-of-the-art methods (based on CNN and long short-term memory network) shows the superiority of our method for performing a reliable feature extraction.',\n",
       "  'paddleOCR': [[[[1391.0, 10.0],\n",
       "     [1418.0, 10.0],\n",
       "     [1418.0, 38.0],\n",
       "     [1391.0, 38.0]],\n",
       "    ['X', 0.6561277508735657]],\n",
       "   [[[1283.0, 67.0], [1296.0, 58.0], [1306.0, 71.0], [1293.0, 81.0]],\n",
       "    ['CO', 0.5998203754425049]],\n",
       "   [[[136.0, 79.0], [213.0, 79.0], [213.0, 108.0], [136.0, 108.0]],\n",
       "    ['Input', 0.9969744682312012]],\n",
       "   [[[1293.0, 76.0], [1317.0, 63.0], [1338.0, 108.0], [1314.0, 120.0]],\n",
       "    ['filter', 0.8960739970207214]],\n",
       "   [[[330.0, 116.0], [361.0, 116.0], [359.0, 375.0], [329.0, 375.0]],\n",
       "    ['Convolutional Layer', 0.9963626861572266]],\n",
       "   [[[815.0, 114.0], [845.0, 114.0], [843.0, 375.0], [813.0, 375.0]],\n",
       "    ['Convolutional Layer', 0.9948698282241821]],\n",
       "   [[[946.0, 125.0], [976.0, 125.0], [976.0, 385.0], [946.0, 385.0]],\n",
       "    ['SELU + MAXPOOL', 0.9388381838798523]],\n",
       "   [[[1397.0, 191.0], [1424.0, 192.0], [1421.0, 306.0], [1393.0, 305.0]],\n",
       "    ['Features', 0.9928532838821411]],\n",
       "   [[[26.0, 210.0], [97.0, 216.0], [95.0, 244.0], [23.0, 238.0]],\n",
       "    ['Time', 0.9973778128623962]],\n",
       "   [[[168.0, 224.0], [195.0, 227.0], [189.0, 295.0], [162.0, 293.0]],\n",
       "    ['Data', 0.9946547150611877]],\n",
       "   [[[26.0, 245.0], [99.0, 251.0], [97.0, 279.0], [23.0, 273.0]],\n",
       "    ['Steps', 0.9947355389595032]],\n",
       "   [[[567.0, 242.0], [626.0, 242.0], [626.0, 266.0], [567.0, 266.0]],\n",
       "    ['5x1', 0.9232986569404602]],\n",
       "   [[[1176.0, 258.0], [1234.0, 258.0], [1234.0, 282.0], [1176.0, 282.0]],\n",
       "    ['3x1', 0.9302588105201721]]],\n",
       "  'ocr': [[[1397.0, 191.0], [1424.0, 192.0], [1421.0, 306.0], [1393.0, 305.0]],\n",
       "   ['Features', 0.9928532838821411]]},\n",
       " '2202.08098v1-Figure3-1.png': {'caption': 'Figure 3. Flowchart of proposed network (LA-Net). The convolutional layers marked in the same colors in the decomposition network and the high-frequency pathway share the weights. More experiments about weight sharing can be found in the supplementary materials.',\n",
       "  'imageText': ['Ilight',\n",
       "   '64C',\n",
       "   'CR',\n",
       "   '3232',\n",
       "   '32',\n",
       "   '64',\n",
       "   '64',\n",
       "   '128',\n",
       "   '128128',\n",
       "   '32',\n",
       "   'Ilight',\n",
       "   '64',\n",
       "   'CR32',\n",
       "   '128',\n",
       "   '128',\n",
       "   '128',\n",
       "   '64',\n",
       "   '64',\n",
       "   '32',\n",
       "   '32',\n",
       "   '32',\n",
       "   'Ilow',\n",
       "   'N-R',\n",
       "   'curves',\n",
       "   '+',\n",
       "   'C',\n",
       "   'Color',\n",
       "   'Recovery',\n",
       "   'Concatenation',\n",
       "   'Pixel-wise',\n",
       "   'Addition',\n",
       "   'U-shaped',\n",
       "   'Net',\n",
       "   'CR',\n",
       "   'Ground',\n",
       "   'Truth',\n",
       "   'Input',\n",
       "   'Scene',\n",
       "   '1X1',\n",
       "   'conv',\n",
       "   'skip',\n",
       "   'connections',\n",
       "   'Ihigh',\n",
       "   'Iin',\n",
       "   'Thigh',\n",
       "   'Ilow',\n",
       "   'Tlow',\n",
       "   'Ldc=Ldc-gt',\n",
       "   '+',\n",
       "   'Ldc-in',\n",
       "   'T',\n",
       "   'Ienh',\n",
       "   'LU',\n",
       "   '+P',\n",
       "   'Re',\n",
       "   'Co',\n",
       "   'nv',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '3',\n",
       "   '3',\n",
       "   '64',\n",
       "   '64',\n",
       "   '64',\n",
       "   '3',\n",
       "   '64',\n",
       "   '64',\n",
       "   '64',\n",
       "   '64',\n",
       "   'C',\n",
       "   'CR',\n",
       "   'Ilight',\n",
       "   'Idetail',\n",
       "   'Iout',\n",
       "   'Lcom',\n",
       "   'Llight',\n",
       "   'Ldetail',\n",
       "   'f(n16,œÉ16)',\n",
       "   'f(n2,œÉ2)',\n",
       "   'f(n1,œÉ1)',\n",
       "   '+',\n",
       "   'Res-Block',\n",
       "   'Decomposition',\n",
       "   'High-frequency',\n",
       "   'Pathway',\n",
       "   'Low-frequency',\n",
       "   'Pathway',\n",
       "   '+B',\n",
       "   'N',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '+B',\n",
       "   'N',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '+B',\n",
       "   'N',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '+B',\n",
       "   'N',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '+B',\n",
       "   'N',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   'Res-Block',\n",
       "   'Res-Block',\n",
       "   'Res-Block',\n",
       "   'Res-Block',\n",
       "   'Res-Block',\n",
       "   'd',\n",
       "   'gm',\n",
       "   'oi',\n",
       "   'v+',\n",
       "   'Si',\n",
       "   'C',\n",
       "   'on',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   'eL',\n",
       "   'U',\n",
       "   'v+',\n",
       "   'PR',\n",
       "   'C',\n",
       "   'on',\n",
       "   '+',\n",
       "   '+',\n",
       "   '+',\n",
       "   '+',\n",
       "   '+',\n",
       "   'Light',\n",
       "   'Adaptation',\n",
       "   'Fusion'],\n",
       "  'image_file': '2202.08098v1-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Proposed Model',\n",
       "    'text': 'According to the description in Section 3.1, we propose a new network for image enhancement with the two-pathway and visual adaptation mechanisms. The pipeline of the proposed method is shown in Fig. 3. Specifically, the input image is first decomposed into low-and high-frequency components with a small convolutional network. Then, light adaptation is handled in the low-frequency pathway with a unified sub-network inspired by visual adaptation. Noise suppression and detail enhancement are achieved in the high-frequency pathway by introducing residual-based blocks that can prevent the disappearance of gradients, especially low values in the high-frequency pathway.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Noise Suppression and Detail Enhancement',\n",
       "    'text': 'To enhance details and suppress potential noises, we designed a sub-network to process the high-frequency information (I high ). The structure of the sub-network is shown in Fig. 3. Specifically, the basic residual-block (i.e.,Res-Block) is used in the proposed sub-network, which is aimed at avoiding the possible vanishing of gradients in backpropagation, which, as usual, results in small pixel values in the images of details. A loss function used in the lowfrequency pathway is defined as\\nL detail = I detail ‚àí T high 2 2 . (7\\n)\\nThe proposed network is targeted to achieve noise suppression for low-light enhancement tasks and realize detail enhancement or preservation when facing some noise-free input scenes, such as HDR scenes.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Learning to Adapt to Light',\n",
       "  'abstract': 'Light adaptation or brightness correction is a key step in improving the contrast and visual appeal of an image. There are multiple light-related tasks (for example, low-light enhancement and exposure correction) and previous studies have mainly investigated these tasks individually. However, it is interesting to consider whether these light-related tasks can be executed by a unified model, especially considering that our visual system adapts to external light in such way. In this study, we propose a biologically inspired method to handle light-related image-enhancement tasks with a unified network (called LA-Net). First, a frequency-based decomposition module is designed to decouple the common and characteristic sub-problems of light-related tasks into two pathways. Then, a new module is built inspired by biological visual adaptation to achieve unified light adaptation in the low-frequency pathway. In addition, noise suppression or detail enhancement is achieved effectively in the high-frequency pathway regardless of the light levels. Extensive experiments on three tasks-low-light enhancement, exposure correction, and tone mapping-demonstrate that the proposed method almost obtains state-of-the-art performance compared with recent methods designed for these individual tasks.',\n",
       "  'paddleOCR': [[[[1522.0, 23.0],\n",
       "     [1582.0, 33.0],\n",
       "     [1576.0, 66.0],\n",
       "     [1516.0, 56.0]],\n",
       "    ['Ilight', 0.9252579808235168]],\n",
       "   [[[880.0, 53.0], [976.0, 53.0], [976.0, 78.0], [880.0, 78.0]],\n",
       "    ['f(ni', 0.8831473588943481]],\n",
       "   [[[1100.0, 56.0], [1255.0, 56.0], [1255.0, 78.0], [1100.0, 78.0]],\n",
       "    ['skip connections', 0.997255802154541]],\n",
       "   [[[885.0, 98.0], [972.0, 98.0], [972.0, 129.0], [885.0, 129.0]],\n",
       "    ['f(n2,02)', 0.8838244080543518]],\n",
       "   [[[1682.0, 124.0], [1798.0, 132.0], [1796.0, 163.0], [1680.0, 155.0]],\n",
       "    ['yyihJ <...', 0.5505411028862]],\n",
       "   [[[1303.0, 136.0], [1347.0, 136.0], [1347.0, 153.0], [1303.0, 153.0]],\n",
       "    ['conv', 0.9125509858131409]],\n",
       "   [[[1404.0, 149.0], [1429.0, 149.0], [1429.0, 164.0], [1404.0, 164.0]],\n",
       "    ['R', 0.9934661984443665]],\n",
       "   [[[928.0, 160.0], [944.0, 160.0], [944.0, 189.0], [928.0, 189.0]],\n",
       "    ['OO0', 0.6310155987739563]],\n",
       "   [[[1308.0, 164.0], [1342.0, 164.0], [1342.0, 187.0], [1308.0, 187.0]],\n",
       "    ['1X1', 0.9880185723304749]],\n",
       "   [[[883.0, 235.0], [983.0, 235.0], [983.0, 260.0], [883.0, 260.0]],\n",
       "    ['f(n16,616)', 0.8670375943183899]],\n",
       "   [[[69.0, 253.0], [217.0, 253.0], [217.0, 278.0], [69.0, 278.0]],\n",
       "    ['Input Scene', 0.9815356731414795]],\n",
       "   [[[862.0, 273.0], [1008.0, 273.0], [1008.0, 298.0], [862.0, 298.0]],\n",
       "    ['Light Adaptation', 0.999934196472168]],\n",
       "   [[[1155.0, 278.0], [1216.0, 278.0], [1216.0, 302.0], [1155.0, 302.0]],\n",
       "    ['Fusion', 0.999860942363739]],\n",
       "   [[[940.0, 318.0], [1251.0, 320.0], [1251.0, 351.0], [940.0, 349.0]],\n",
       "    ['Low-frequency Pathway', 0.9997743964195251]],\n",
       "   [[[570.0, 366.0], [791.0, 371.0], [791.0, 400.0], [569.0, 395.0]],\n",
       "    ['Ldc=Ldc-gt + Ldc-in', 0.9501746296882629]],\n",
       "   [[[1947.0, 362.0], [2047.0, 367.0], [2045.0, 398.0], [1945.0, 393.0]],\n",
       "    ['... Lcom', 0.8705090284347534]],\n",
       "   [[[919.0, 462.0], [1013.0, 462.0], [1013.0, 484.0], [919.0, 484.0]],\n",
       "    ['Res-Block', 0.9985215663909912]],\n",
       "   [[[284.0, 498.0], [480.0, 498.0], [480.0, 529.0], [284.0, 529.0]],\n",
       "    ['Decomposition', 0.9995317459106445]],\n",
       "   [[[990.0, 489.0], [1086.0, 489.0], [1086.0, 513.0], [990.0, 513.0]],\n",
       "    ['Res-Block', 0.9719099402427673]],\n",
       "   [[[1820.0, 489.0], [1861.0, 489.0], [1861.0, 518.0], [1820.0, 518.0]],\n",
       "    ['Iout', 0.9996839761734009]],\n",
       "   [[[37.0, 524.0], [208.0, 524.0], [208.0, 549.0], [37.0, 549.0]],\n",
       "    ['Ground Truth.', 0.9532811641693115]],\n",
       "   [[[1077.0, 518.0], [1171.0, 518.0], [1171.0, 542.0], [1077.0, 542.0]],\n",
       "    ['Res-Block', 0.9978945255279541]],\n",
       "   [[[1015.0, 544.0], [1040.0, 544.0], [1040.0, 589.0], [1015.0, 589.0]],\n",
       "    ['J+BN', 0.9991956949234009]],\n",
       "   [[[1150.0, 551.0], [1246.0, 551.0], [1246.0, 575.0], [1150.0, 575.0]],\n",
       "    ['Res-Block', 0.9982227683067322]],\n",
       "   [[[1020.0, 582.0], [1036.0, 582.0], [1036.0, 646.0], [1020.0, 646.0]],\n",
       "    ['+PReLU', 0.9983876347541809]],\n",
       "   [[[1237.0, 580.0], [1328.0, 580.0], [1328.0, 602.0], [1237.0, 602.0]],\n",
       "    ['Res-Block', 0.9993249177932739]],\n",
       "   [[[85.0, 591.0], [337.0, 598.0], [335.0, 636.0], [84.0, 628.0]],\n",
       "    ['(CR) Color Recovery', 0.9520004391670227]],\n",
       "   [[[1349.0, 582.0], [1374.0, 582.0], [1374.0, 602.0], [1349.0, 602.0]],\n",
       "    ['+', 0.991402804851532]],\n",
       "   [[[1684.0, 597.0], [1802.0, 602.0], [1801.0, 634.0], [1682.0, 628.0]],\n",
       "    ['....> Ldetail', 0.9084941148757935]],\n",
       "   [[[1212.0, 624.0], [1296.0, 624.0], [1296.0, 646.0], [1212.0, 646.0]],\n",
       "    ['Res-Blocl', 0.9959624409675598]],\n",
       "   [[[123.0, 646.0], [318.0, 646.0], [318.0, 678.0], [123.0, 678.0]],\n",
       "    [') Concatenation', 0.9671443700790405]],\n",
       "   [[[949.0, 660.0], [974.0, 660.0], [974.0, 680.0], [949.0, 680.0]],\n",
       "    ['64', 0.9999156594276428]],\n",
       "   [[[551.0, 669.0], [599.0, 669.0], [599.0, 693.0], [551.0, 693.0]],\n",
       "    ['high', 0.9987136721611023]],\n",
       "   [[[96.0, 695.0], [391.0, 695.0], [391.0, 726.0], [96.0, 726.0]],\n",
       "    ['+) Pixel-wise Addition', 0.9773134589195251]],\n",
       "   [[[1532.0, 693.0], [1601.0, 693.0], [1601.0, 726.0], [1532.0, 726.0]],\n",
       "    ['Idetail', 0.998626172542572]],\n",
       "   [[[1073.0, 723.0], [1093.0, 704.0], [1111.0, 722.0], [1092.0, 741.0]],\n",
       "    ['64', 0.9254834651947021]],\n",
       "   [[[81.0, 737.0], [316.0, 745.0], [315.0, 776.0], [80.0, 768.0]],\n",
       "    ['SU-shaped Net', 0.9753202795982361]],\n",
       "   [[[929.0, 744.0], [1246.0, 749.0], [1246.0, 780.0], [928.0, 775.0]],\n",
       "    ['High-frequency Pathway', 0.9998748302459717]]],\n",
       "  'ocr': [[[1100.0, 56.0], [1255.0, 56.0], [1255.0, 78.0], [1100.0, 78.0]],\n",
       "   ['skip connections', 0.997255802154541]]},\n",
       " '2103.06819v3-Figure1-1.png': {'caption': 'Fig. 1: Gradient transformer attack process.',\n",
       "  'imageText': ['Patient‚Äôs',\n",
       "   'name',\n",
       "   'is',\n",
       "   'David.',\n",
       "   'Age:',\n",
       "   '50',\n",
       "   'Gender:',\n",
       "   'male',\n",
       "   'Patient‚Äôs',\n",
       "   'name',\n",
       "   'is',\n",
       "   'David.',\n",
       "   'Age:',\n",
       "   '50',\n",
       "   'Gender:',\n",
       "   'male',\n",
       "   'Patient‚Äôs',\n",
       "   'name:',\n",
       "   '??',\n",
       "   'Age:',\n",
       "   '??',\n",
       "   'Gender:??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   '??',\n",
       "   '...',\n",
       "   '??',\n",
       "   'UpdateGTA',\n",
       "   'adversary',\n",
       "   'Comparison',\n",
       "   '(‚àáùëä‚Ä≤,',\n",
       "   '‚àáùëä)',\n",
       "   '‚àáùëä‚Ä≤Transformer',\n",
       "   'Model',\n",
       "   'ùêπ(ùëã,ùëä)',\n",
       "   'ùêπ(ùëã,ùëä)',\n",
       "   'Transformer',\n",
       "   'Server',\n",
       "   'Transformer',\n",
       "   'Model',\n",
       "   '‚àáùëä'],\n",
       "  'image_file': '2103.06819v3-Figure1-1.png',\n",
       "  'sections': [{'heading': 'A. Privacy leakage problem',\n",
       "    'text': 'Privacy leakage is studied in the training phase and prediction phase. Privacy attack from gradient and model inversion (MI) attack [14] aims at the training phase by constructing the features of the training data by using the correlation between the training data and the model output. The authors in [14] showed that it is possible to infer individual genomic data via access to a linear model for personalized medicine. Recent works extend MI attack to recover features of training data of Deep Neural Networks (DNNs). Privacy attack from gradients is different from previous MI attack. It reconstructs the training data exploiting their gradients in a machine learning model. The process of privacy leakage from gradients is shown at Figure 1.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'B. Federated learning',\n",
       "    'text': 'Instead of uploading all the data to a centralized server and jointly training them, federated learning enables training on a large corpus of decentralized data on edge devices and only collects the local models or gradients for global synchronization Fig. 1: Gradient transformer attack process. on a central server [1], [2], [15]. As a distributed machine learning approach, MPL enables edge devices at geographically different locations to collaboratively learn an ML model while storing all data locally [1], [2]. Koneƒçn√Ω et al. proposed Federated Averaging (FedAvg), which trains models using relatively few rounds of communication [16]. To date, as the most comprehensive survey paper on MPL, Kairouz et al. [17] summarized the pioneer worked from google [18]- [20] and listed some open problems that are currently motivated by the real-world setting, crossing ML models to data types.',\n",
       "    'n_publication_ref': 9,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'TAG: Transformer Attack from Gradient',\n",
       "  'abstract': 'Although federated learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training images (gradient leakage) to a third-party in computer vision. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the Transformer-based language models and propose a gradient attack algorithm, TAG, to reconstruct the local training data. We develop a set of metrics to evaluate the effectiveness of the proposed attack algorithm quantitatively. Experimental results on Transformer, TinyBERT4, TinyBERT6, BERTBASE, and BERTLARGE using GLUE benchmark show that TAG works well on more weight distributions in reconstructing training data and achieves 1.5√ó recover rate and 2.5√ó ROUGE-2 over prior methods without the need of ground truth label. TAG can obtain up to 90% data by attacking gradients in CoLA dataset. In addition, TAG has a stronger adversary on large models, small dictionary size, and small input length. We hope the proposed TAG will shed some light on the privacy leakage problem in Transformer-based NLP models.',\n",
       "  'paddleOCR': [[[[100.0, 29.0], [223.0, 32.0], [222.0, 52.0], [100.0, 49.0]],\n",
       "    ['GTA adversary', 0.9978474974632263]],\n",
       "   [[[525.0, 26.0], [592.0, 26.0], [592.0, 49.0], [525.0, 49.0]],\n",
       "    ['Update', 0.999891459941864]],\n",
       "   [[[451.0, 102.0], [738.0, 102.0], [738.0, 128.0], [451.0, 128.0]],\n",
       "    ['Transformer Model', 0.994634747505188]],\n",
       "   [[[103.0, 120.0], [171.0, 120.0], [171.0, 140.0], [103.0, 140.0]],\n",
       "    ['?? ... ??', 0.9053568243980408]],\n",
       "   [[[790.0, 118.0], [831.0, 118.0], [831.0, 139.0], [790.0, 139.0]],\n",
       "    ['MA', 0.8627722859382629]],\n",
       "   [[[877.0, 116.0], [1024.0, 118.0], [1023.0, 145.0], [877.0, 142.0]],\n",
       "    ['Comparison', 0.9999402761459351]],\n",
       "   [[[104.0, 142.0], [171.0, 145.0], [170.0, 168.0], [103.0, 165.0]],\n",
       "    ['?? ... ??', 0.9727995991706848]],\n",
       "   [[[541.0, 143.0], [669.0, 143.0], [669.0, 178.0], [541.0, 178.0]],\n",
       "    ['F(X,W)', 0.9968116879463196]],\n",
       "   [[[889.0, 151.0], [1015.0, 151.0], [1015.0, 177.0], [889.0, 177.0]],\n",
       "    [\"(MA'M)\", 0.6854844093322754]],\n",
       "   [[[102.0, 170.0], [172.0, 170.0], [172.0, 192.0], [102.0, 192.0]],\n",
       "    ['?? ... ??', 0.9953821897506714]],\n",
       "   [[[890.0, 214.0], [921.0, 214.0], [921.0, 232.0], [890.0, 232.0]],\n",
       "    ['MA', 0.970944881439209]],\n",
       "   [[[315.0, 242.0], [475.0, 242.0], [475.0, 257.0], [315.0, 257.0]],\n",
       "    ['Transformer Server', 0.9761409759521484]],\n",
       "   [[[350.0, 275.0], [619.0, 275.0], [619.0, 296.0], [350.0, 296.0]],\n",
       "    [\"Patient's name is David.\", 0.9727994799613953]],\n",
       "   [[[347.0, 308.0], [437.0, 305.0], [437.0, 331.0], [348.0, 334.0]],\n",
       "    ['Age: 50', 0.9646992683410645]],\n",
       "   [[[850.0, 304.0], [1017.0, 305.0], [1017.0, 327.0], [850.0, 326.0]],\n",
       "    ['Transformer Model.', 0.9608215689659119]],\n",
       "   [[[349.0, 342.0], [506.0, 342.0], [506.0, 365.0], [349.0, 365.0]],\n",
       "    ['Gender: male', 0.9732189178466797]],\n",
       "   [[[896.0, 335.0], [970.0, 335.0], [970.0, 359.0], [896.0, 359.0]],\n",
       "    ['F(X,W)', 0.9983468651771545]],\n",
       "   [[[118.0, 440.0], [207.0, 440.0], [207.0, 466.0], [118.0, 466.0]],\n",
       "    ['?? ... ??', 0.9107224345207214]],\n",
       "   [[[365.0, 442.0], [571.0, 442.0], [571.0, 464.0], [365.0, 464.0]],\n",
       "    [\"Patient's name: ??\", 0.9928681254386902]],\n",
       "   [[[748.0, 446.0], [1015.0, 446.0], [1015.0, 468.0], [748.0, 468.0]],\n",
       "    [\"Patient's name is David.\", 0.9992340207099915]],\n",
       "   [[[118.0, 474.0], [207.0, 474.0], [207.0, 500.0], [118.0, 500.0]],\n",
       "    ['?? ... ??', 0.9056044816970825]],\n",
       "   [[[363.0, 476.0], [449.0, 472.0], [450.0, 499.0], [365.0, 504.0]],\n",
       "    ['Age: ??', 0.9991362690925598]],\n",
       "   [[[745.0, 479.0], [832.0, 475.0], [834.0, 503.0], [746.0, 507.0]],\n",
       "    ['Age: 50', 0.9962398409843445]],\n",
       "   [[[118.0, 506.0], [207.0, 506.0], [207.0, 534.0], [118.0, 534.0]],\n",
       "    ['?? ... ??', 0.9504784345626831]],\n",
       "   [[[364.0, 508.0], [484.0, 508.0], [484.0, 534.0], [364.0, 534.0]],\n",
       "    ['Gender:??', 0.9993573427200317]],\n",
       "   [[[748.0, 514.0], [901.0, 514.0], [901.0, 537.0], [748.0, 537.0]],\n",
       "    ['Gender: male', 0.9997448921203613]]],\n",
       "  'ocr': [[[877.0, 116.0], [1024.0, 118.0], [1023.0, 145.0], [877.0, 142.0]],\n",
       "   ['Comparison', 0.9999402761459351]]},\n",
       " '2101.07295v5-Figure17-1.png': {'caption': 'Figure 17: Architecture for the visual feature analysis. We freeze the weights of the feature extractor up to the pooling layer before the FC layer that outputs the class predictions. We train the VF classifier by optimizing the parameters œÜt of the FC layer F (t) (blue branch, top). The VF targets are obtained by binarizing the feature representation of the batch model. We utilize the binary cross entropy loss on each element of the predicted VF outputs YÃÇ (t) and the ground truth VF targets Y (B). Please refer to the text for more details.',\n",
       "  'imageText': ['At',\n",
       "   'F',\n",
       "   't',\n",
       "   'YÃÇ',\n",
       "   't',\n",
       "   'Y',\n",
       "   'T',\n",
       "   'x',\n",
       "   'œÜ',\n",
       "   'Input',\n",
       "   'VF',\n",
       "   'Classifier',\n",
       "   'Training',\n",
       "   'Class',\n",
       "   'Prediction',\n",
       "   'FC',\n",
       "   'Layer',\n",
       "   'Outputs',\n",
       "   'VF',\n",
       "   'Targets',\n",
       "   'BCE',\n",
       "   'Loss',\n",
       "   'Feature',\n",
       "   'Extractor',\n",
       "   'FC',\n",
       "   'Layer',\n",
       "   'Class',\n",
       "   'Outputs',\n",
       "   'Avg',\n",
       "   'Pool',\n",
       "   'Activation',\n",
       "   'Map'],\n",
       "  'image_file': '2101.07295v5-Figure17-1.png',\n",
       "  'sections': [{'heading': 'G.2. Visual Feature Analysis',\n",
       "    'text': 'The architecture for training the visual feature analysis approach is illustrated in Fig. 17. Given an input image, we first obtain the visual feature (VF) targets Y (B) . This is done by binarizing the output of the average pooling layer A (B) using 1{a\\n(B) i > Œ∏} of the batch model, where a i is each activation and Œ∏ is the threshold 12 . For the experiments conducted in this section, we utilized threshold Œ∏ = 1. The VF target with value 1 indicates that the visual feature is active and 0 otherwise. Our goal is to train a set of N binary classifiers where N is the number of visual features. After obtaining the feature representation learned at each learning exposure, we then freeze the weights of the feature extractor and train the VF classifier by optimizing the parameters œÜ t of the FC layer F (t) to produce the VF prediction≈∂ (t) (blue branch in Fig. 17). Note that F (t) is different from the FC layer that outputs the class prediction (gray branch in Fig. 17). We use binary cross entropy loss on each element of the predicted VF outputs≈∂ (t) and the ground truth VF targets Y (B) . The intuition is that the accuracy of the VF classifiers measures the extent to which the current learned representation captures information related to the final representation.',\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'The Surprising Positive Knowledge Transfer in Continual 3D Object Shape Reconstruction',\n",
       "  'abstract': 'Continual learning has been extensively studied for classification tasks with methods developed to primarily avoid catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we present a set of continual 3D object shape reconstruction tasks, including complete 3D shape reconstruction from different input modalities, as well as visible surface (2.5D) reconstruction which, surprisingly demonstrate positive knowledge (backward and forward) transfer when training with solely standard SGD and without additional heuristics. We provide evidence that continuously updated representation learning of single-view 3D shape reconstruction improves the performance on learned and novel categories over time. We provide a novel analysis of knowledge transfer ability by looking at the output distribution shift across sequential learning tasks. Finally, we show that the robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. The codebase, dataset and pretrained models released with this article can be found at https://github.com/rehg-lab/CLRec',\n",
       "  'paddleOCR': [[[[718.0, 0.0], [874.0, 2.0], [874.0, 16.0], [717.0, 13.0]],\n",
       "    ['VF Classifier Training', 0.9850134253501892]],\n",
       "   [[[599.0, 18.0], [741.0, 20.0], [741.0, 36.0], [599.0, 34.0]],\n",
       "    ['FC Layer Outputs', 0.9688268899917603]],\n",
       "   [[[851.0, 17.0], [937.0, 21.0], [937.0, 38.0], [851.0, 34.0]],\n",
       "    ['VF Targets', 0.9904254674911499]],\n",
       "   [[[10.0, 54.0], [67.0, 54.0], [67.0, 71.0], [10.0, 71.0]],\n",
       "    ['Input x', 0.9535293579101562]],\n",
       "   [[[154.0, 53.0], [281.0, 53.0], [281.0, 67.0], [154.0, 67.0]],\n",
       "    ['Feature Extractor', 0.9788516759872437]],\n",
       "   [[[364.0, 97.0], [474.0, 100.0], [473.0, 117.0], [363.0, 114.0]],\n",
       "    ['Activation Map', 0.9885269403457642]],\n",
       "   [[[763.0, 92.0], [832.0, 94.0], [831.0, 111.0], [763.0, 109.0]],\n",
       "    ['BCE LoSS', 0.8400542140007019]],\n",
       "   [[[488.0, 134.0], [552.0, 134.0], [552.0, 151.0], [488.0, 151.0]],\n",
       "    ['Avg Pool', 0.9944524765014648]],\n",
       "   [[[540.0, 183.0], [560.0, 183.0], [560.0, 200.0], [540.0, 200.0]],\n",
       "    ['At', 0.9860789179801941]],\n",
       "   [[[627.0, 200.0], [742.0, 200.0], [742.0, 214.0], [627.0, 214.0]],\n",
       "    ['Class Prediction', 0.999718189239502]],\n",
       "   [[[597.0, 225.0], [771.0, 228.0], [771.0, 245.0], [597.0, 241.0]],\n",
       "    ['FC Layer Class Outputs', 0.9862821102142334]]],\n",
       "  'ocr': [[[597.0, 225.0], [771.0, 228.0], [771.0, 245.0], [597.0, 241.0]],\n",
       "   ['FC Layer Class Outputs', 0.9862821102142334]]},\n",
       " '2011.05139v1-Figure1-1.png': {'caption': 'Figure 1: Illustration of Inception module. It was restricted to filter sizes 1 √ó 1, 3 √ó 3, and 5 √ó 5. Subsequently, the outputs were concatenated into a single vector that is the input for the next stage. Adding of an alternative parallel pooling path was found to be beneficial. Applying filters of 1 √ó 1 convolution makes possible to reduce the volume before the expensive 3 √ó 3 and 5 √ó 5 convolutions [31].',\n",
       "  'imageText': [],\n",
       "  'image_file': '2011.05139v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Methodology',\n",
       "    'text': 'To extract visual features, GoogLeNet [31] or Inception-V3 [32] were applied as base models. GoogLeNet [31] is a 22 layer deep CNN and was the winner of ILSVRC 2014 with a top 5 error rate of 6.7 %. Depth and width of the network was increased but not simply following the general method of stacking the layers on each other. A new level of organization was introduced codenamed Inception module (see Figure 1). In GoogLeNet [31] not everything happens sequentially like in previous CNN models, pieces of the network work in parallel. Inspired by a neuroscience model in [36] where for handling multiple scales a series of Gabor filters were used with a two layer deep model. But contrary to the beforementioned model all layers are learned and not fixed. In GoogLeNet [31] architecture Inception layers are introduced and repeated many times. Subsequent improvements of GoogLeNet [31] have been called Inception-vN where N refers to the version number put out by Google. Inception-V2 [32] was refined by the introduction of batch normalization [37]. Inception-V3 [32] was improved by factorization ideas. Factorization into smaller convolutions means for example replacing a 5 √ó 5 convolution by a multi-layer network with fewer parameters but with the same input size and output depth.\\nWe chose the features of Inception modules for the following reasons. The main motivation behind the construction of Inception modules is that salient parts of images may very extremely. This means that the region of interest can occupy very different image regions both in terms of size and location. That is why, determining the convolutional kernel size in a CNN is very difficult. Namely, a larger kernel size is required for visual information that is distributed rather globally. On the other hand, a smaller kernel size is better for visual information that is distributed more locally. As already mentioned, the creators of Inception modules reflected to this challenge by the introduction of multiple filters with multiple sizes on the same level. Furthermore, visual distortions have a similar nature. Namely, the distortion distribution is strongly influenced by image content [38]. Figure 2: The pipeline of the proposed method. An input image is run through on an ImageNet database pretrained CNN body (GoogLeNet and Inception-V3 are considered in this study) which carries out all its defined operations. Furthermore, global average pooling (GAP) layers are attached to each Inception module to extract resolution independent deep features at different abstraction levels. The feature vectors obtained from the Inception modules are concatenated and an SVR with radial basis function is applied to predict perceptual image quality.',\n",
       "    'n_publication_ref': 11,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'MULTI-POOLED INCEPTION FEATURES FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT',\n",
       "  'abstract': 'Image quality assessment (IQA) is an important element of a broad spectrum of applications ranging from automatic video streaming to display technology. Furthermore, the measurement of image quality requires a balanced investigation of image content and features. Our proposed approach extracts visual features by attaching global average pooling (GAP) layers to multiple Inception modules of on an ImageNet database pretrained convolutional neural network (CNN). In contrast to previous methods, we do not take patches from the input image. Instead, the input image is treated as a whole and is run through a pretrained CNN body to extract resolution-independent, multi-level deep features. As a consequence, our method can be easily generalized to any input image size and pretrained CNNs. Thus, we present a detailed parameter study with respect to the CNN base architectures and the effectiveness of different deep features. We demonstrate that our best proposal -called MultiGAP-NRIQA -is able to provide state-of-the-art results on three benchmark IQA databases. Furthermore, these results were also confirmed in a cross database test using the LIVE In the Wild Image Quality Challenge database.',\n",
       "  'paddleOCR': [[[[485.0, 44.0], [564.0, 47.0], [563.0, 79.0], [483.0, 75.0]],\n",
       "    ['Filter', 0.9972281455993652]],\n",
       "   [[[425.0, 88.0], [628.0, 88.0], [628.0, 112.0], [425.0, 112.0]],\n",
       "    ['concatenation', 0.999886155128479]],\n",
       "   [[[338.0, 240.0], [554.0, 240.0], [554.0, 264.0], [338.0, 264.0]],\n",
       "    ['3x3 convolutions', 0.9985817670822144]],\n",
       "   [[[638.0, 240.0], [856.0, 240.0], [856.0, 264.0], [638.0, 264.0]],\n",
       "    ['5x5 convolutions', 0.999550461769104]],\n",
       "   [[[941.0, 240.0], [1158.0, 240.0], [1158.0, 264.0], [941.0, 264.0]],\n",
       "    ['1x1 convolutions', 0.9958502054214478]],\n",
       "   [[[47.0, 324.0], [265.0, 325.0], [265.0, 350.0], [46.0, 348.0]],\n",
       "    ['1x1 convolutions', 0.9997970461845398]],\n",
       "   [[[337.0, 397.0], [554.0, 397.0], [554.0, 422.0], [337.0, 422.0]],\n",
       "    ['1x1 convolutions', 0.9998009204864502]],\n",
       "   [[[633.0, 392.0], [849.0, 392.0], [849.0, 417.0], [633.0, 417.0]],\n",
       "    ['1x1 convolutions', 0.9998480081558228]],\n",
       "   [[[974.0, 393.0], [1125.0, 396.0], [1124.0, 427.0], [973.0, 424.0]],\n",
       "    ['3x3 pooling', 0.9992986917495728]],\n",
       "   [[[416.0, 551.0], [622.0, 551.0], [622.0, 581.0], [416.0, 581.0]],\n",
       "    ['Previous layer', 0.9878378510475159]]],\n",
       "  'ocr': [[[638.0, 240.0], [856.0, 240.0], [856.0, 264.0], [638.0, 264.0]],\n",
       "   ['5x5 convolutions', 0.999550461769104]]},\n",
       " '812374-Figure2-1.png': {'caption': 'Figure 2: Overview of our training pipeline. We use an annotated image collection to estimate camera viewpoints which we then use alongwith object silhouettes to learn 3D shape models. Our learnt shape models, as illustrated in the rightmost figure are capable of deforming to capture intra-class shape variation.',\n",
       "  'imageText': [],\n",
       "  'image_file': '812374-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Introduction',\n",
       "    'text': 'Consider the car in Figure 1. As humans, not only can we infer at a glance that the image contains a car, we also construct a rich internal representation of it such as its location and 3D pose. Moreover, we have a guess of its 3D shape, even though we might never have have seen this particular car. We can do this because we don\\'t experience the image of this car tabula rasa, but in the context of our \"remembrance of things past\". Previously seen cars enable us to develop a notion of the 3D shape of cars, which we can project to this particular instance. We also specialize our representation to this particular instance (e.g. any custom decorations it might have), signalling that both top-down and bottom-up cues influence our percept [26].\\nA key component in such a process would be a mechanism to build 3D shape models from past visual experiences. We have developed an algorithm that can build category-specific shape models from just images with 2D annotations (segmentation masks and a small set of keypoints) present in modern computer vision datasets (e.g. * Authors contributed equally Figure 1: Automatic object reconstruction from a single image obtained by our system. Our method leverages estimated instance segmentations and predicted viewpoints to generate a full 3D mesh and high frequency 2.5D depth maps.\\nPASCAL VOC [15]). These models are then used to guide the top down 3D shape reconstruction of novel 2D car images. We complement our top-down shape inference algorithm with a bottom-up module that further refines our shape estimate for a particular instance. Finally, building upon the rapid recent progress in recognition modules [2,11,17,20,34] (object detection, segmentation and pose estimation), we demonstrate that our learnt models are robust when applied \"in the wild\" enabling fully automatic reconstructions with just images as inputs.\\nThe recent method of Vicente et al. [36] reconstructs 3D models from similar annotations as we do but it has a different focus: it aims to reconstruct a fully annotated image set while making strong assumptions about the quality of the segmentations it fits to and is hence inappropriate for reconstruction in an unconstrained setting. Our approach can work in such settings, partly because it uses explicit 3D shape models. Our work also has connections to that of Kemelmacher-Shlizerman et al. [23,32] which aims to learn morphable models for faces from 2D images, but we focus on richer shapes in unconstrained settings, at the expense of lower resolution reconstructions.\\nIn the history of computer vision, model-based object Figure 2: Overview of our training pipeline. We use an annotated image collection to estimate camera viewpoints which we then use alongwith object silhouettes to learn 3D shape models. Our learnt shape models, as illustrated in the rightmost figure are capable of deforming to capture intra-class shape variation.\\nreconstruction from a single image has reflected varying preferences on model representations. Generalized cylinders [27] resulted in very compact descriptions for certain classes of shapes, and can be used for category level descriptions, but the fitting problem for general shapes in challenging. Polyhedral models [18,40], which trace back to the early work of Roberts [29], and CAD models [25,31] provide crude approximations of shape and given a set of point correspondences can be quite effective for determining instance viewpoints. Here we pursue more expressive basis shape models [1,7,42] which establish a balance between the two extremes as they can deform but only along class-specific modes of variation. In contrast to previous work (e.g. [42]), we fit them to automatic figure-ground object segmentations.\\nOur paper is organized as follows: in Section 2 we describe our model learning pipeline where we estimate camera viewpoints for all training objects (Section 2.1) followed by our shape model formulation (Section 2.2) to learn 3D models. Section 3 describes our testing pipeline where we use our learnt models to reconstruct novel instances without assuming any annotations. We evaluate our reconstructions under various settings in Section 4 and provide sample reconstructions in the wild.',\n",
       "    'n_publication_ref': 20,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Category-Specific Object Reconstruction from a Single Image',\n",
       "  'abstract': 'Object reconstruction from a single image -in the wild -is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.',\n",
       "  'paddleOCR': [[[[1721.0, 110.0],\n",
       "     [1738.0, 110.0],\n",
       "     [1738.0, 144.0],\n",
       "     [1721.0, 144.0]],\n",
       "    ['...', 0.9523673057556152]],\n",
       "   [[[52.0, 378.0], [442.0, 378.0], [442.0, 406.0], [52.0, 406.0]],\n",
       "    ['Annotated Image Collection', 0.9841442704200745]],\n",
       "   [[[619.0, 374.0], [926.0, 378.0], [926.0, 408.0], [618.0, 404.0]],\n",
       "    ['Viewpoint Estimation.', 0.9812405109405518]],\n",
       "   [[[1133.0, 378.0], [1517.0, 378.0], [1517.0, 408.0], [1133.0, 408.0]],\n",
       "    ['Learning 3D Shape Models', 0.999384880065918]],\n",
       "   [[[1702.0, 372.0], [1954.0, 374.0], [1954.0, 410.0], [1702.0, 408.0]],\n",
       "    ['Mean Shapes and', 0.9847239255905151]],\n",
       "   [[[1208.0, 419.0], [1441.0, 419.0], [1441.0, 448.0], [1208.0, 448.0]],\n",
       "    ['from Silhouettes', 0.9852156639099121]],\n",
       "   [[[1689.0, 419.0], [1971.0, 419.0], [1971.0, 448.0], [1689.0, 448.0]],\n",
       "    ['Deformation Modes.', 0.9734756946563721]]],\n",
       "  'ocr': [[[1702.0, 372.0], [1954.0, 374.0], [1954.0, 410.0], [1702.0, 408.0]],\n",
       "   ['Mean Shapes and', 0.9847239255905151]]},\n",
       " '2012.09966v2-Figure6-1.png': {'caption': 'Figure 6: The LSTM-based models. pr denotes the example prefix size and RTi denotes the representation vector of the i-th trial. The left part is the LSTM-TR model, the right part is the LSTM-CR model, and the complete figure presents the joint LSTM-TRCR model.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2012.09966v2-Figure6-1.png',\n",
       "  'sections': [{'heading': 'Deep Neural Network Modeling',\n",
       "    'text': \"DNNs have proven effective for many text classification tasks (Kim, 2014;Ziser & Reichart, 2018). In this part of the paper, we provide a high level description as well as more specific details of our DNN models.\\nIn our DNN models, each trial in the prefix is represented using its behavioral features (as described in Section 5.1). These features are concatenated to the trial's textual features (either its T DN N or its T HC features, or their concatenation, as described in Section 5.1). In contrast, since the suffix trials' behavioral features are not known, each trial in the suffix is represented only with its textual features.\\nThe LSTM Models These models belong to the family of Recurrent Neural Networks (RNNs), which can process variable length sequences. We hypothesize that since our data involve multiple trials, and based on our analysis described in Section 4.2 where we show a sequential effect in the decision-making process, a sequential model could capture signals that non-sequential models cannot.\\nLSTM is an RNN variant designed to handle long-distance dependencies, while avoiding the vanishing gradients problem. It has shown very useful in sequence modeling tasks in NLP, such as language modeling (Sundermeyer, Schl√ºter, & Ney, 2012), speech recognition (Greff, Srivastava, Koutn√≠k, Steunebrink, & Schmidhuber, 2016) and machine translation (Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao, Macherey, et al., 2016). We describe our LSTM models below, focusing on their labels, input vectors and architectures.\\nWe have considered various LSTM-based models and multiple approaches for mapping the input v pr as these models' input. Each input v pr is represented with a sequence of feature vectors, such that each feature vector represents one trial (either a prefix or a suffix trial; the feature vectors of each prefix and suffix trial are described above). We next describe the best model version based on our development data results, an illustration of the architecture is provided in Figure 6.\\nLSTM-CR. This is the LSTM model that predicts the hotel choice rate in the suffix. Figure 6 (right) provides a description of this architecture. The LSTM is sequentially fed with the prefix and suffix trials' representations, one trial at a time. The suffix trials' hidden vectors are fed into a dot product attention layer, followed by two linear layers with a dropout layer and a ReLU activation function, in order to predict the hotel choice rate in the suffix trials. The model applies the mean squared error (MSE) loss as implemented in the PyTorch.nn module:\\n4 M SE = 1 batch batch i=1 (≈∑ CR i ‚àí y CR i ) 2\\nwhere batch is the size of the training batch (in the stochastic optimization process), and≈∑ CR i and y CR i are the predicted and the gold hotel choice rates in the i-th example of the batch, respectively.\\nLSTM-TR. This is the LSTM model that predicts the decision in each suffix trial. The LSTM-TR architecture is described in the left side of Figure 6. The result of this model can also be averaged in order to get the hotel choice rate in the suffix trials.\\nThe LSTM is sequentially fed with the prefix and suffix trials' representations, one trial at a time. Each hidden state of the suffix trials is fed into a dropout layer followed by a linear layer with a ReLU activation function, in order to predict the label for each suffix trial. The loss function of this model is the sequence cross-entropy (SCE), as implemented in the AllenNLP software package:\\n5 SCE = 1 batch batch i=1 10 j=pr+1 ‚àí(y T R ti ‚Ä¢ log(p ti ) + (1 ‚àí y T R ti ) ‚Ä¢ log(1 ‚àí p ijs )) sf\\nwhere batch is the size of the training batch (in the stochastic optimization process), pr is the prefix size, sf is the suffix size, p ti is the predicted probability that the t-th trial of the i-th example of the batch is hotel, a ti is the decision in the t-th trial of the i-th example of the batch and y T R ti ‚àà {0, 1} is the choice of the t-th example of the batch in the i-th trial, such that y T R ti = 1 if a ti = hotel 0 otherwise .\",\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 3},\n",
       "   {'heading': 'LSTM-TRCR.',\n",
       "    'text': \"This model jointly learns to predict the decisions made by the decision maker in each trial, and the hotel choice rate. The LSTM-TRCR architecture, a combination of the above LSTM-TR and the LSTM-CR, is described in Figure 6. Since the choice rate and the trial labels of each example are strongly related, such that the hotel choice rate label is an average of the trial labels, we augment the above losses with a loss term that is aimed to minimize the squared distance between the predicted choice rate and the average of the individual trial predictions. For this purpose, we calculated the averaged trial predictions using the argmax value of a softmax layer that is fed with the sequence of trial prediction. Formally, given the above notation, and defining≈∑ T R ti ‚àà [0, 1] to be the prediction of the t-th trial of the i-th example, we define the mean squared trial-choice rate error (MSTRCRE):\\nM ST RCRE = 1 batch batch i=1 (≈∑ CR i ‚àí 1 sf 10 j=pr+1≈∑ T R ti ) 2\\nFinally, we define the trial-choice rate loss (TRCRL) as the weighted average of three losses, MSE, SCE and MSTRCRE:\\nT RCRL = Œ± ‚Ä¢ M SE + Œ≤ ‚Ä¢ SCE + Œ≥ ‚Ä¢ M ST RCRE\\nwhere Œ±, Œ≤ and Œ≥ are hyper-parameters.\\nThe Transformer Models . Another neural network model that has proven to be especially effective for many natural language processing tasks is the Transformer (Vaswani et al., 2017). The Transformer has shown very useful in various NLP tasks, including machine translation (Vaswani et al., 2017, Shaw, Uszkoreit, & Vaswani, 2018 and speech recognition (Dong, Xu, & Xu, 2018), among many others. The Transformer is a sequence-to-sequence model that consists of an encoder and a decoder. In our case, the encoder maps the prefix trials' input sequence to a sequence of continuous representations. Given these representations and an input sequence of the suffix trials, the decoder then generates an output sequence of the suffix trials' representations fed to our model's next layers to generate the output predictions.\\nBelow we describe our Transformer models: Their labels, input vectors and architectures. By implementing Transformer-based models, we aim to model each input v pr as two sequences: A sequence of the prefix trials and a sequence of the suffix trials, so that to model our task as a translation of the prefix trials to the decisions in the suffix trials. The representations of both the prefix and suffix trials are described above. Since the model's input consists of two sequences, we did not feed it with examples with pr = 0. See Figure 7 for an illustration of the model architecture.\\nTransformer-CR. This is the Transformer model that predicts the hotel choice rate in the suffix, and its architecture is described in the right side of Figure 7. The Transformer is fed with two sequences as described above, and its output is a sequence of sf hidden vectors. These hidden vectors are fed into a dot product attention layer, followed by two linear layers with a dropout layer and a ReLU activation function, in order to predict the hotel choice rate in the suffix trials. The loss function of this model is the MSE loss described above.\\nTransformer-TR. The Transformer model that predicts the decision in each suffix trial, and its architecture is described in the left side of Figure 7. The output of this model can also be averaged to get the hotel choice rate in the suffix trials. The Transformer is fed with two sequences as described above, and its output is a sequence of sf hidden vectors. Each hidden vector is fed into a linear layer with a dropout layer and a ReLU activation function in order to predict the label for each suffix trial. The loss function of this model is the SCE loss described above.\\nTransformer-TRCR. This is the Transformer model that jointly predicts the per-trial decision and the overall hotel choice rate. This model is a combination of the two models described above: The Transformer-TR and the Transformer-CR, and its architecture is described in Figure 7. The loss function of this model is the TRCRL loss described above.\\nFigure 7: The Transformer-based models. pr denotes the prefix size of the sample, R T i denotes the representation vector of trial i, and h context is randomly initialized and learned jointly with the attention weights during the training process. The left part is the Transformer-TR model, the right part is the Transformer-CR model, and the entire figure stands for the joint Transformer-TRCR model. \",\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 6}],\n",
       "  'title': 'Predicting Decisions in Language Based Persuasion Games',\n",
       "  'abstract': \"Sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. However, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. This paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions. For this purpose, we conduct an online repeated interaction experiment. At each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. While the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. The expert's payoff, in turn, depends on the number of times the decision-maker chooses the hotel. We consider a number of modeling approaches for this setup, differing from each other in the model type (deep neural network (DNN) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (DNN-based vs. hand-crafted). Our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. 1\",\n",
       "  'paddleOCR': [[[[1431.0, 74.0],\n",
       "     [1683.0, 78.0],\n",
       "     [1683.0, 115.0],\n",
       "     [1430.0, 111.0]],\n",
       "    ['Hotel Choice', 0.9998505711555481]],\n",
       "   [[[1412.0, 130.0], [1709.0, 130.0], [1709.0, 171.0], [1412.0, 171.0]],\n",
       "    ['Rate Prediction', 0.9998750686645508]],\n",
       "   [[[1487.0, 293.0], [1637.0, 303.0], [1634.0, 348.0], [1484.0, 338.0]],\n",
       "    ['Linear', 0.9994543194770813]],\n",
       "   [[[1455.0, 351.0], [1675.0, 356.0], [1674.0, 405.0], [1454.0, 401.0]],\n",
       "    ['Classifier', 0.9986183047294617]],\n",
       "   [[[1485.0, 538.0], [1633.0, 545.0], [1630.0, 591.0], [1483.0, 583.0]],\n",
       "    ['Linear', 0.9995908141136169]],\n",
       "   [[[1502.0, 602.0], [1632.0, 602.0], [1632.0, 647.0], [1502.0, 647.0]],\n",
       "    ['Layer', 0.9997930526733398]],\n",
       "   [[[487.0, 742.0], [579.0, 742.0], [579.0, 781.0], [487.0, 781.0]],\n",
       "    ['Trial', 0.9995339512825012]],\n",
       "   [[[952.0, 742.0], [1041.0, 742.0], [1041.0, 781.0], [952.0, 781.0]],\n",
       "    ['Trial', 0.9996930360794067]],\n",
       "   [[[1454.0, 771.0], [1648.0, 787.0], [1643.0, 852.0], [1449.0, 837.0]],\n",
       "    ['hsample', 0.995503306388855]],\n",
       "   [[[448.0, 796.0], [615.0, 796.0], [615.0, 832.0], [448.0, 832.0]],\n",
       "    ['Decision', 0.997123122215271]],\n",
       "   [[[915.0, 796.0], [1082.0, 796.0], [1082.0, 832.0], [915.0, 832.0]],\n",
       "    ['Decision', 0.9993340373039246]],\n",
       "   [[[433.0, 840.0], [632.0, 845.0], [631.0, 888.0], [432.0, 884.0]],\n",
       "    ['Prediction', 0.999240517616272]],\n",
       "   [[[897.0, 842.0], [1097.0, 847.0], [1096.0, 888.0], [896.0, 884.0]],\n",
       "    ['Prediction', 0.9996107816696167]],\n",
       "   [[[1388.0, 966.0], [1675.0, 966.0], [1675.0, 1022.0], [1388.0, 1022.0]],\n",
       "    ['Attention', 0.9997265934944153]],\n",
       "   [[[515.0, 997.0], [1027.0, 997.0], [1027.0, 1046.0], [515.0, 1046.0]],\n",
       "    ['Linear Classifier', 0.99867182970047]],\n",
       "   [[[1230.0, 1040.0], [1378.0, 1060.0], [1371.0, 1112.0], [1223.0, 1092.0]],\n",
       "    ['hcontext', 0.9320359230041504]],\n",
       "   [[[162.0, 1183.0], [251.0, 1199.0], [241.0, 1258.0], [151.0, 1243.0]],\n",
       "    ['hT1', 0.9292769432067871]],\n",
       "   [[[498.0, 1184.0], [636.0, 1200.0], [628.0, 1262.0], [491.0, 1246.0]],\n",
       "    ['hTpr+1', 0.9861032366752625]],\n",
       "   [[[901.0, 1184.0], [1015.0, 1198.0], [1007.0, 1260.0], [893.0, 1246.0]],\n",
       "    ['hT10', 0.9303905367851257]],\n",
       "   [[[1315.0, 1201.0], [1471.0, 1212.0], [1466.0, 1282.0], [1311.0, 1271.0]],\n",
       "    ['hTpr+1', 0.9928048253059387]],\n",
       "   [[[1638.0, 1200.0], [1761.0, 1217.0], [1752.0, 1279.0], [1630.0, 1262.0]],\n",
       "    ['hT10', 0.9410464763641357]],\n",
       "   [[[471.0, 1390.0], [656.0, 1390.0], [656.0, 1447.0], [471.0, 1447.0]],\n",
       "    ['LSTM', 0.9961736798286438]],\n",
       "   [[[147.0, 1602.0], [244.0, 1613.0], [237.0, 1673.0], [140.0, 1661.0]],\n",
       "    ['RT1', 0.9844338297843933]],\n",
       "   [[[496.0, 1599.0], [640.0, 1620.0], [631.0, 1682.0], [487.0, 1661.0]],\n",
       "    ['RTpr+1', 0.9992257952690125]],\n",
       "   [[[904.0, 1601.0], [1025.0, 1616.0], [1018.0, 1675.0], [897.0, 1661.0]],\n",
       "    ['RT10', 0.9791087508201599]]],\n",
       "  'ocr': [[[904.0, 1601.0],\n",
       "    [1025.0, 1616.0],\n",
       "    [1018.0, 1675.0],\n",
       "    [897.0, 1661.0]],\n",
       "   ['RT10', 0.9791087508201599]]},\n",
       " '2207.09387v1-Figure1-1.png': {'caption': 'Fig. 1: An illustration of the quantized FL model over wireless network.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2207.09387v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'I. INTRODUCTION',\n",
       "    'text': 'Federated learning (FL) is an emerging paradigm that enables distributed learning among wireless devices [2]. In FL, a central server (e.g., a base station (BS)) and multiple mobile devices collaborate to train a shared machine learning model without sharing raw data. Many FL works employ deep neural networks (DNNs), whose size constantly grows to match the increasing demand for higher accuracy [3]. Such DNN architectures can have tens of millions of parameters and billions of multiply-accumulate (MAC) operations. Moreover, to achieve fast convergence, these networks typically represent data in 32 bits of full precision level, which may consume significant energy due to high computational complexity and memory requirements [4].\\nAdditionally, a large DNN can induce a significant communication overhead [5]. Under such practical constraints, it may be challenging to deploy FL over resource-constrained Internet of Things (IoT) devices due to its large energy cost. To design an energy-efficient, green FL scheme, one can reduce the precision level to decrease the energy consumption during the local training and communication phase. However, a low precision level can jeopardize the convergence rate by introducing quantization errors. Therefore, finding the optimal precision level that balances energy efficiency and convergence rate while meeting desired FL accuracy constraints will be a major challenge for the practical deployment of green FL over wireless networks.\\nSeveral works have studied the energy efficiency of FL from a system-level perspective [6]- [11]. The work in [6] investigated the energy efficiency of FL algorithms in terms of the carbon footprint compared to centralized learning. In [7], the authors formulated a joint minimization problem for energy consumption and training time by optimizing heterogeneous computing and wireless resources. The work in [8] developed an approach to minimize the total energy consumption by controlling a target accuracy during local training based on a derived convergence rate. The authors in [9] proposed a sum energy minimization problem by considering joint bandwidth and workload allocation of heterogeneous devices. In [10], the authors studied a joint optimization problem whose goal is to minimize the energy consumption and the training time while achieving a target accuracy. The work in [11] developed a resource management scheme by leveraging the information of loss functions of each device to maximize the accuracy under constrained communication and computation resources. However, these works [6]- [11] did not consider the energy efficiency of their DNN structure during training. Since mobile devices have limited computing and memory resources, deploying an energy-efficient DNN will be necessary for green FL.\\nTo further improve FL energy efficiency, model compression methods such as quantization were studied in [12]- [15]. The work in [12] proposed a quantization scheme for both uplink and downlink transmission in FL and analyzed the impact of the quantization on the convergence rate. In [13], the authors proposed an FL scheme with periodic averaging and quantized model uploading to improve the communication efficiency. The authors in [14] and [15] considered a novel FL setting, in which each device trains a ternary/binary neural network so as to alleviate the communication overhead by uploading ternary/binary parameters to the server. However, the works in [12] and [13] only considered the communication efficiency while there can be a large energy consumption in training. Although the works in [14] and [15], considered ternary/binarized neural networks during local training, they did not optimize the quantization levels of the neural network to balance the tradeoff between energy efficiency and convergence rate. To the best of our knowledge, there is no work that jointly considers the tradeoff between energy efficiency and convergence rate while controlling the optimal precision level for green FL over wireless networks.\\nThe main contribution of this paper is a novel green, energy-efficient quantized FL framework that can represent data with a finite precision level in both local training and uplink transmission.\\nIn our FL model, all devices train their quantized neural network (QNN), whose weights and activations are quantized with a finite precision level, so as to decrease energy consumption for computation and memory access. After training, each device calculates the training result and transmits its quantized version to the BS. The BS then aggregates the received information to generate a new global model and transmits it back to the devices. To quantify the energy consumption, we propose a rigorous energy model for the local training based on the physical structure of a processing chip. We also derive the energy model for the uplink transmission with quantization. Although a low precision level can save the energy consumption per iteration, it decreases the convergence rate because of quantization errors. Thus, there is a need for a new approach to analyze the tradeoff between energy efficiency and convergence rate by optimizing the precision levels while meeting target accuracy constraints. To this end, we formulate a multiobjective optimization problem by controlling the precision levels to minimize the total energy Fig. 1: An illustration of the quantized FL model over wireless network.\\nconsumption and the number of communication rounds while ensuring convergence with a target accuracy. We also incorporate two additional control variables: the number of local iterations and the number of selected devices at each communication round, which have a significant impact on both the energy consumption and the convergence time. To solve this problem, we first analytically derive the convergence rate of our FL framework with respect to the control variables. Then, we use the normal boundary inspection (NBI) method to obtain the Pareto boundary of our multi-objective optimization problem. To balance the tradeoff between the two objectives, we present and analyze two practical operating points: the Nash bargaining solution (NBS) and the sum minimizing solution (SUM) points. Based on these two operating points and the derived convergence rate, we provide design insights into the proposed FL framework.\\nFor instance, the total energy consumption until convergence initially decreases as the precision level increases, however, after a certain threshold, higher precision will mean higher energy costs. Meanwhile, the convergence rate will always improve with a higher precision. We also provide the impacts of system parameters such as the number of devices and model size on the performance of the proposed FL. Simulation results show that our FL model can reduce the energy consumption by up to 52% compared to a baseline that represents data in full precision.\\nThe rest of this paper is organized as follows. Section II presents the system model. In Section III, we describe the studied problem. Section III-D introduces NBS. Section IV provides simulation results. Finally, conclusions are drawn in Section V.',\n",
       "    'n_publication_ref': 24,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'II. SYSTEM MODEL',\n",
       "    'text': 'Consider an FL system having N devices connected to a BS as shown in Fig. 1. Each device k has its own local dataset D k = {x kl , y kl }, where l = 1, . . . , D k . For example, {x kl , y kl } can be an input-output pair for image classification, where x kl is an input vector and y kl is the corresponding output. We define a loss function f (w k , x kl , y kl ) to quantify the performance of a machine learning (ML) model with parameters w k ‚àà R d over {x kl , y kl }, where d is the number of parameters. Since device k has D k data samples, its local loss function can be given by\\nF k (w k ) = 1 D k D k l=1 f (w k , x kl , y kl ).\\nThe FL process aims to find the global parameters w that can solve the following optimization problem:\\nmin w 1 ,...,w N F (w) = N k=1 D k D F k (w k ) = 1 D N k=1 D k l=1 f (w k , x kl , y kl ) (1) s.t. w 1 = w 2 = ‚Ä¢ ‚Ä¢ ‚Ä¢ = w N = w,(2)\\nwhere D = N k=1 D k is the total size of the entire dataset D = ‚à™ N k=1 D k . We assume that the local datasets are identically distributed in order to guarantee that the expected stochastic gradient from D k equals to the one from D for all k ‚àà {1, . . . , N} [14], [16].\\nSolving problem (2) typically requires an iterative process between the BS and devices.\\nHowever, in practical systems, such as IoT systems, these devices are resource-constrained, particularly when it comes to computing and energy. Hence, we propose to manage the precision level of parameters used in our FL algorithm to reduce the energy consumption for computation, memory access, and transmission. As such, we adopt a QNN architecture whose weights and activations are quantized in fixed-point format rather than conventional 32-bit floating-point format [17]. During the training time, a QNN can reduce the energy consumption for MAC operation and memory access due to quantized weights and activations.',\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design',\n",
       "  'abstract': 'The practical deployment of federated learning (FL) over wireless networks requires balancing energy efficiency and convergence time due to the limited available resources of devices. Prior art on FL often trains deep neural networks (DNNs) to achieve high accuracy and fast convergence using 32 bits of precision level. However, such scenarios will be impractical for resource-constrained devices since DNNs typically have high computational complexity and memory requirements. Thus, there is a need to reduce the precision level in DNNs to reduce the energy expenditure. In this paper, a greenquantized FL framework, which represents data with a finite precision level in both local training and uplink transmission, is proposed. Here, the finite precision level is captured through the use of quantized neural networks (QNNs) that quantize weights and activations in fixed-precision format. In the considered FL model, each device trains its QNN and transmits a quantized training result to the base station. Energy models for the local training and the transmission with quantization are rigorously derived. To minimize the energy consumption and the number of communication rounds simultaneously, a multi-objective optimization problem is formulated with respect to the number of local iterations, the number of selected devices, and the precision levels for both local training and transmission while ensuring convergence under a target accuracy constraint. To solve this problem, the convergence rate of the proposed FL system is analytically derived with respect to the system control variables. Then, the Pareto boundary of the problem is characterized to provide efficient solutions using the normal boundary inspection method. Design insights on balancing the tradeoff between the two objectives are M. Kim and W. Saad are with the Wireless@VT Group,',\n",
       "  'paddleOCR': [[[[675.0, 21.0], [850.0, 21.0], [850.0, 49.0], [675.0, 49.0]],\n",
       "    ['Base Station', 0.9624914526939392]],\n",
       "   [[[1188.0, 103.0], [1285.0, 103.0], [1285.0, 134.0], [1188.0, 134.0]],\n",
       "    ['Global', 0.9995366930961609]],\n",
       "   [[[1188.0, 146.0], [1285.0, 146.0], [1285.0, 176.0], [1188.0, 176.0]],\n",
       "    ['Model', 0.9994813799858093]],\n",
       "   [[[905.0, 184.0], [1056.0, 187.0], [1055.0, 221.0], [904.0, 218.0]],\n",
       "    ['Averaging', 0.9998967051506042]],\n",
       "   [[[1314.0, 187.0], [1351.0, 197.0], [1345.0, 220.0], [1308.0, 210.0]],\n",
       "    ['Wt', 0.9529904127120972]],\n",
       "   [[[381.0, 272.0], [431.0, 267.0], [435.0, 295.0], [384.0, 301.0]],\n",
       "    ['Q,k', 0.9794378280639648]],\n",
       "   [[[502.0, 541.0], [544.0, 535.0], [547.0, 557.0], [506.0, 563.0]],\n",
       "    ['Q,1', 0.9237315058708191]],\n",
       "   [[[1054.0, 549.0], [1111.0, 526.0], [1122.0, 553.0], [1065.0, 576.0]],\n",
       "    ['de,R', 0.6067154407501221]],\n",
       "   [[[195.0, 794.0], [274.0, 794.0], [274.0, 824.0], [195.0, 824.0]],\n",
       "    ['Local', 0.9993802309036255]],\n",
       "   [[[302.0, 798.0], [426.0, 798.0], [426.0, 828.0], [302.0, 828.0]],\n",
       "    ['Device 1', 0.9997785091400146]],\n",
       "   [[[702.0, 800.0], [826.0, 800.0], [826.0, 829.0], [702.0, 829.0]],\n",
       "    ['Device 2', 0.9996728897094727]],\n",
       "   [[[1045.0, 798.0], [1171.0, 798.0], [1171.0, 828.0], [1045.0, 828.0]],\n",
       "    ['Device K', 0.9996393322944641]],\n",
       "   [[[197.0, 836.0], [270.0, 836.0], [270.0, 867.0], [197.0, 867.0]],\n",
       "    ['Data', 0.9998818039894104]]],\n",
       "  'ocr': [[[675.0, 21.0], [850.0, 21.0], [850.0, 49.0], [675.0, 49.0]],\n",
       "   ['Base Station', 0.9624914526939392]]},\n",
       " '2202.10337v1-Figure2-1.png': {'caption': 'Figure 2: Diagram of the classification of knowledge discovery algorithms.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2202.10337v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Knowledge Discovery',\n",
       "    'text': \"The goal of knowledge discovery is to extract undiscovered knowledge from data and push the boundaries of human intelligence forward. In early days, researchers obtained the equation structure by theoretical derivation and then determined the coefficients via regression methods [Hosmer Jr et al., 2013], such as the discovery of the law of gravity and Maxwell's equations. Because many real-world problems, such as turbulence in fluid dynamics, are too complicated to be solved using first-principle models, researchers have developed simulation approaches [Griebel et al., 1998;Zhang, 2001]. Nevertheless, simulations fail to reveal the full internal structure of complex systems and lack interpretability [Bongard and Lipson, 2007]. With the development of machine learning, neural networks are utilized as approximators to handle knowledge discovery problems, such as DeepONet [Lu et al., 2021]. Although the theory demonstrates that the neural network can approximate any function and its derivative [Hornik et al., 1990], but its essence is a surrogate model (i.e., unexplainable black box), and no explicit knowledge is obtained. Researchers have also attempted to use physicsinformed neural network (PINN) to determine the governing equations [Raissi et al., 2019], however such approach requires the explicit form of the governing equation, which is essentially an inverse problem rather than knowledge discovery.\\nThe real knowledge discovery method is capable to directly extract the governing equation that best matches the data with transfer ability when the equation structure is unknown. The core of knowledge discovery is determining the structure and coefficients of the governing equation. The complexity of the equation structure is the first criterion for evaluating knowledge discovery methods. The second evaluation dimension is the complexity of the equation coefficients (Figure 2).  [Rudy et al., 2017]. SGTR combines group sparse coding and solves the problem of parametric PDEs . Besides, different norm minimizations as sparsity constraints can be used in sparse regression algorithms [Donoho and Elad, 2003;Hoyer, 2004]. For the noisy observations in practice, high quality data can be generated by low-rank denoising  and neural network fitting [Rao et al., 2022;Xu et al., 2020]. In addition to selecting candidate terms, closed library methods can also be used to automatically determine physical processes [Chang and , deepening our understanding of the nature of physics. Since the candidate sets of the closed library methods are preset, prior information can be easily embedded. For instance, Rao et al. [2022] utilize the specially-designed kernels to encode known terms. Especially in PDE-Net, each Œ¥t block corresponds to a time step, which establishes the connection between governing equation and the network [Long et al., 2018]. There are also many variants of PDE-Net, most of which rely on the preset overcomplete library. Furthermore, PINN-SR is proposed by combining PINN with sparse regression to embed domain knowledge into the knowledge discovery model [Chen et al., 2021c].\",\n",
       "    'n_publication_ref': 15,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Challenges of knowledge discovery',\n",
       "    'text': 'The representation of equations is the core issue in knowledge discovery. The closed library methods directly include all possible terms, although they are easy to implement, they have a restricted range of applications. In the expandable library methods, the representation of PDEs is realized by representing the function terms as gene segments or kernels, so that the algorithm can find equations with complex interaction terms. The open-form equation methods, which can deal with governing equations with fractional structures and compound functions, employ symbolic mathematics to represent any form of governing equations, but the computational cost is high. In the future, more efficient and comprehensive equation representation approaches should be investigated.\\nThere are five research gaps and future opportunities in knowledge discovery, including:\\n‚Ä¢ In order to optimize the equation via efficient gradientbased methods, a more appropriate embedding approach for equations is required (similar to the word vector [Le and Mikolov, 2014]). The edit distance does not infer performance in equations (e.g., if the fourth derivative is the solution, the third derivative is not necessarily better than the second derivative).\\n‚Ä¢ Governing equations are essentially necessary conditions, but sufficient conditions are found in many cases, which leads to overfitting. Future studies might look towards discovering equations from multiple experiments [Tod et al., 2021] to extract commonalities (i.e., necessary conditions).\\n‚Ä¢ The governing equations for complex systems, such as turbulence, are not only complex, but even a set of PDEs. Algorithms for mining equations with complex coefficients and structures are required (top right corner of Figure 2).\\n‚Ä¢ The precision of derivatives is important for mining PDEs. Gradients calculated by difference are not robust to noise [Rudy et al., 2017]. Anti-noise methods include utilizing automatic differentiation in neural networks, using neural networks to generate high-quality data [Rao et al., 2022;Xu et al., 2020], and applying weak forms of equations [Xu et al., 2021a]. PINN-SR and R-DLGA prove that the robustness can be improved by embedding domain knowledge, which is worth exploring in the future.\\n‚Ä¢ The goal of knowledge discovery is to find a balance between accuracy and parsimony of equations. As the library goes from closed to open, it is a process of gaining precision while diminishing simplicity. The openform equation methods make it easy to find equivalent forms of equations. How to simplify the equations is a big challenge.\\nThe knowledge, experience and physical mechanisms accumulated by human beings are valuable assets, but most current machine learning models fail to properly exploit them, which is a waste and limits the application of machine learning. Pure data-driven models not only have high data requirements, but also might produce predictions that violate the physical mechanism [Raissi et al., 2019;. By integrating domain knowledge in machine learning models, it is possible to break down the barriers between datadriven and knowledge-driven models.',\n",
       "    'n_publication_ref': 7,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Discussion and Conclusion',\n",
       "    'text': 'We systematically review studies on the integration of knowledge and data from the perspectives of knowledge discovery and knowledge embedding. On the one hand, this study evaluates and categorizes knowledge discovery algorithms based on the complexity of the structure and coefficients of the uncovered equations, as shown in Figure 2. On the other hand, this study summarizes the methods of embedding domain knowledge in the modeling process, and discusses the difference of soft constraints and hard constraints, as shown in Figure 5.\\nIn addition, we propose five research gaps and future opportunities for knowledge discovery and knowledge embedding, respectively. Suggestions for knowledge discovery include: building a more appropriate embedding approach to optimize with gradient-based methods, finding necessary conditions through multiple experiments, handling governing equations with both complex structures and complex coefficients, improving the accuracy of gradient computations, and simplifying equations found by symbolic mathematical methods. Regarding knowledge embedding, the research opportunities are: exploring approaches to embed complex governing equations, attempting to use network structures such as graph neural networks to handle irregular fields, implementing adaptive hyperparameters in soft constraints, focusing on noisy and scarce real-world data, and utilizing tools such as auto machine learning to lower the threshold for applying knowledge embedding models. Furthermore, as illustrated in Figure 1, this study establishes a closed loop between knowledge discovery and knowledge embedding, realizing mutual promotion between domain knowledge (i.e., science) and machine learning models (i.e., engineering).',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Integration of knowledge and data in machine learning',\n",
       "  'abstract': \"Scientific research's duty and goal is to comprehend and explore the world, as well as to modify it based on experience and knowledge. Knowledge embedding and knowledge discovery are two significant methods of integrating knowledge and data. Through knowledge embedding, the barriers between knowledge and data can be broken, and machine learning models with physical common sense can be formed. Meanwhile, humans' understanding of the world is always limited, and knowledge discovery takes advantage of machine learning to extract new knowledge from observations. Not only may knowledge discovery help researchers better grasp the nature of physics, but it can also help them conduct knowledge embedding research. A closed loop of knowledge generation and usage are formed by combining knowledge embedding with knowledge discovery, which can improve the robustness and accuracy of the model and uncover unknown scientific principles. This study not only summarizes and analyzes the existing literature, but also proposes research gaps and future opportunities.\",\n",
       "  'paddleOCR': [[[[147.0, 15.0], [283.0, 18.0], [282.0, 42.0], [146.0, 39.0]],\n",
       "    ['Closed library', 0.9995266795158386]],\n",
       "   [[[356.0, 19.0], [535.0, 19.0], [535.0, 40.0], [356.0, 40.0]],\n",
       "    ['Expandable library', 0.9997241497039795]],\n",
       "   [[[583.0, 19.0], [775.0, 19.0], [775.0, 40.0], [583.0, 40.0]],\n",
       "    ['Open-form equation', 0.9996389746665955]],\n",
       "   [[[789.0, 48.0], [923.0, 48.0], [923.0, 69.0], [789.0, 69.0]],\n",
       "    [' Inexpressible', 0.9831424355506897]],\n",
       "   [[[181.0, 59.0], [258.0, 59.0], [258.0, 77.0], [181.0, 77.0]],\n",
       "    ['KO-PDE', 0.9954758286476135]],\n",
       "   [[[787.0, 74.0], [923.0, 78.0], [922.0, 103.0], [787.0, 99.0]],\n",
       "    ['! by equations', 0.9976334571838379]],\n",
       "   [[[158.0, 84.0], [282.0, 84.0], [282.0, 101.0], [158.0, 101.0]],\n",
       "    ['[Luo et al., 2021]', 0.9925395250320435]],\n",
       "   [[[175.0, 118.0], [263.0, 118.0], [263.0, 135.0], [175.0, 135.0]],\n",
       "    ['PDE-NET', 0.9958184957504272]],\n",
       "   [[[152.0, 142.0], [286.0, 142.0], [286.0, 159.0], [152.0, 159.0]],\n",
       "    ['[Long et al., 2018]', 0.9984562397003174]],\n",
       "   [[[26.0, 154.0], [49.0, 154.0], [49.0, 474.0], [26.0, 474.0]],\n",
       "    ['Equation coefficients complexity.', 0.9823957681655884]],\n",
       "   [[[89.0, 171.0], [110.0, 171.0], [109.0, 496.0], [88.0, 496.0]],\n",
       "    ['First principle with regression method', 0.9999593496322632]],\n",
       "   [[[191.0, 168.0], [248.0, 170.0], [247.0, 191.0], [190.0, 189.0]],\n",
       "    ['SGTR', 0.9976989030838013]],\n",
       "   [[[375.0, 170.0], [513.0, 168.0], [514.0, 188.0], [375.0, 190.0]],\n",
       "    ['Stepwise DLGA', 0.9995166063308716]],\n",
       "   [[[804.0, 172.0], [917.0, 172.0], [917.0, 193.0], [804.0, 193.0]],\n",
       "    ['Expressible', 0.9998409152030945]],\n",
       "   [[[152.0, 195.0], [285.0, 195.0], [285.0, 212.0], [152.0, 212.0]],\n",
       "    ['[Rudy et al., 2019]', 0.9591353535652161]],\n",
       "   [[[385.0, 192.0], [506.0, 192.0], [506.0, 212.0], [385.0, 212.0]],\n",
       "    ['[Xu et al., 2021]', 0.9677690863609314]],\n",
       "   [[[789.0, 198.0], [925.0, 200.0], [925.0, 225.0], [789.0, 222.0]],\n",
       "    [' by equations', 0.996354341506958]],\n",
       "   [[[188.0, 223.0], [250.0, 223.0], [250.0, 245.0], [188.0, 245.0]],\n",
       "    ['DLrSR', 0.9998542666435242]],\n",
       "   [[[385.0, 225.0], [504.0, 225.0], [504.0, 243.0], [385.0, 243.0]],\n",
       "    ['PDE-NET 2.0', 0.997704267501831]],\n",
       "   [[[161.0, 248.0], [275.0, 248.0], [275.0, 268.0], [161.0, 268.0]],\n",
       "    ['[Li et al., 2020]', 0.9996589422225952]],\n",
       "   [[[377.0, 249.0], [510.0, 247.0], [511.0, 266.0], [377.0, 267.0]],\n",
       "    ['[Long et al., 2019]', 0.9501757621765137]],\n",
       "   [[[177.0, 282.0], [260.0, 284.0], [259.0, 302.0], [177.0, 300.0]],\n",
       "    ['PINN-SR', 0.997538149356842]],\n",
       "   [[[149.0, 307.0], [290.0, 307.0], [290.0, 324.0], [149.0, 324.0]],\n",
       "    ['[Chen et al., 2021c]', 0.9676372408866882]],\n",
       "   [[[187.0, 336.0], [250.0, 339.0], [249.0, 361.0], [186.0, 357.0]],\n",
       "    ['SINDy', 0.9983094334602356]],\n",
       "   [[[142.0, 362.0], [295.0, 362.0], [295.0, 379.0], [142.0, 379.0]],\n",
       "    ['[Brunton et al., 2016]', 0.9606209397315979]],\n",
       "   [[[171.0, 395.0], [267.0, 395.0], [267.0, 414.0], [171.0, 414.0]],\n",
       "    ['PDE-FIND', 0.9966624975204468]],\n",
       "   [[[404.0, 395.0], [484.0, 395.0], [484.0, 414.0], [404.0, 414.0]],\n",
       "    ['R-DLGA', 0.9979126453399658]],\n",
       "   [[[658.0, 396.0], [702.0, 396.0], [702.0, 416.0], [658.0, 416.0]],\n",
       "    ['ARE', 0.9991284012794495]],\n",
       "   [[[816.0, 393.0], [906.0, 396.0], [905.0, 417.0], [816.0, 414.0]],\n",
       "    ['Constant', 0.9997934699058533]],\n",
       "   [[[152.0, 420.0], [286.0, 420.0], [286.0, 437.0], [152.0, 437.0]],\n",
       "    ['[Rudy et al., 2017]', 0.9878060221672058]],\n",
       "   [[[365.0, 420.0], [525.0, 420.0], [525.0, 437.0], [365.0, 437.0]],\n",
       "    ['[Xu and Zhang, 2021]', 0.9885290861129761]],\n",
       "   [[[592.0, 422.0], [768.0, 422.0], [768.0, 436.0], [592.0, 436.0]],\n",
       "    ['[Bongard and Lipson, 2007]', 0.9712496399879456]],\n",
       "   [[[808.0, 424.0], [919.0, 424.0], [919.0, 444.0], [808.0, 444.0]],\n",
       "    ['coefficients', 0.9994832873344421]],\n",
       "   [[[178.0, 454.0], [260.0, 454.0], [260.0, 472.0], [178.0, 472.0]],\n",
       "    ['PeRCNN', 0.9976034760475159]],\n",
       "   [[[417.0, 452.0], [472.0, 452.0], [472.0, 473.0], [417.0, 473.0]],\n",
       "    ['EPDE', 0.9986913204193115]],\n",
       "   [[[657.0, 452.0], [702.0, 452.0], [702.0, 474.0], [657.0, 474.0]],\n",
       "    ['SGA', 0.998288631439209]],\n",
       "   [[[158.0, 478.0], [281.0, 478.0], [281.0, 495.0], [158.0, 495.0]],\n",
       "    ['[Rao et al., 2022]', 0.9657042026519775]],\n",
       "   [[[361.0, 478.0], [527.0, 478.0], [527.0, 495.0], [361.0, 495.0]],\n",
       "    ['[Maslyaev et al., 2019]', 0.9828150272369385]],\n",
       "   [[[609.0, 478.0], [750.0, 478.0], [750.0, 495.0], [609.0, 495.0]],\n",
       "    ['[Chen et al., 2021b]', 0.95745450258255]],\n",
       "   [[[181.0, 512.0], [258.0, 512.0], [258.0, 530.0], [181.0, 530.0]],\n",
       "    ['DL-PDE', 0.9960386753082275]],\n",
       "   [[[413.0, 510.0], [475.0, 510.0], [475.0, 531.0], [413.0, 531.0]],\n",
       "    ['DLGA', 0.9994426369667053]],\n",
       "   [[[593.0, 513.0], [765.0, 514.0], [765.0, 532.0], [592.0, 531.0]],\n",
       "    ['Symbolic regression', 0.9976919293403625]],\n",
       "   [[[159.0, 533.0], [278.0, 533.0], [278.0, 553.0], [159.0, 553.0]],\n",
       "    ['[Xu et al., 2019]', 0.998049259185791]],\n",
       "   [[[385.0, 534.0], [505.0, 534.0], [505.0, 554.0], [385.0, 554.0]],\n",
       "    ['[Xu et al., 2020]', 0.9465909004211426]],\n",
       "   [[[590.0, 536.0], [768.0, 536.0], [768.0, 553.0], [590.0, 553.0]],\n",
       "    ['[Schmidt and Lipson, 2009]', 0.9886417984962463]],\n",
       "   [[[277.0, 594.0], [565.0, 594.0], [565.0, 614.0], [277.0, 614.0]],\n",
       "    ['Equation structure complexity', 0.9998637437820435]]],\n",
       "  'ocr': [[[808.0, 424.0], [919.0, 424.0], [919.0, 444.0], [808.0, 444.0]],\n",
       "   ['coefficients', 0.9994832873344421]]},\n",
       " '2011.11761v2-Figure5-1.png': {'caption': 'Figure 5: Flowchart for computing the quantities of interest from the hyperparameters. Blue blocks refer to the models, green blocks refer to the random input parameters and fields of the computational models and red blocks refer to the random output quantities of interest of the computational models.',\n",
       "  'imageText': ['Random',\n",
       "   'vector',\n",
       "   'of',\n",
       "   'quantities',\n",
       "   'of',\n",
       "   'interest',\n",
       "   'Q',\n",
       "   '=',\n",
       "   '(Q1,',\n",
       "   '.',\n",
       "   '.',\n",
       "   '.',\n",
       "   ',',\n",
       "   'Q9)',\n",
       "   'Random',\n",
       "   'vector',\n",
       "   'LeÔ¨Ä',\n",
       "   '=',\n",
       "   '(Q4,',\n",
       "   '.',\n",
       "   '.',\n",
       "   '.',\n",
       "   ',',\n",
       "   'Q9)',\n",
       "   '(Q1,',\n",
       "   'Q2,',\n",
       "   'Q3)',\n",
       "   'Random',\n",
       "   'vector',\n",
       "   '(DŒµ,',\n",
       "   'LŒµ1,',\n",
       "   'L',\n",
       "   'Œµ',\n",
       "   '2)',\n",
       "   '=',\n",
       "   'MEFF',\n",
       "   'Homogenization',\n",
       "   'Computational',\n",
       "   'Model',\n",
       "   'Mechanical',\n",
       "   'Model',\n",
       "   'MHFCMM',\n",
       "   'High',\n",
       "   'Fidelity',\n",
       "   'Computational',\n",
       "   '[Smeso]',\n",
       "   'Random',\n",
       "   'compliance',\n",
       "   'Ô¨Åeld',\n",
       "   'G',\n",
       "   'Prior',\n",
       "   'Stochastic',\n",
       "   'Model',\n",
       "   'Gaussian',\n",
       "   'random',\n",
       "   'Ô¨Åeld',\n",
       "   'U',\n",
       "   '=',\n",
       "   'U(H2)',\n",
       "   'H',\n",
       "   '=',\n",
       "   '(H1,',\n",
       "   'H2,',\n",
       "   'H3,',\n",
       "   'H4)',\n",
       "   'Random',\n",
       "   'vector',\n",
       "   'of',\n",
       "   'hyperparameters'],\n",
       "  'image_file': '2011.11761v2-Figure5-1.png',\n",
       "  'sections': [{'heading': 'Construction of the initial database',\n",
       "    'text': 'In order to construct an ad hoc database for training an ANN that can be used for the statistical identification of the prior stochastic model of [S meso ], the unknown vector-valued hyperparameter h = (Œ¥, ‚Ñì, Œ∫, ¬µ) is modeled as a random vector H = (D, L, K, M ) = (H 1 , H 2 , H 3 , H 4 ) with statistically independent random components H 1 = D, H 2 = L, H 3 = K and H 4 = M . Hence, mappings M HFCMM , M EFF and G respectively defined in (2), ( 3) and ( 4) allow for defining the random vector of quantities of interest Q = (Q 1 , . . . , Q n ) with values in R n with n = 9, given random vector H = (H 1 , . . . , H m ) with values in\\nH = H 1 √ó. . .√óH m ‚äÇ R m with m = 4, such that (Q 1 , Q 2 , Q 3 ) = M HFCMM G(U ; H 1 , H 3 , H 4 ) with U = U (H 2 ),(5a)\\n(Q 4 , . . . , Q 9 ) = M EFF G(U ; H 1 , H 3 , H 4 ) with U = U (H 2 ). (5b\\n)\\nThe probabilistic model of random vector H is constructed by using the MaxEnt principle [33,34,35,36,37,38,39,32] with the following algebraically independent constraints to be satisfied: (i) the components H 1 , H 2 , H 3 and H 4 of H are mutually statistically independent random variables, (ii) the support of the probability density function of H is a known bounded hypercube H ad ‚äÇ H. Then, the MaxEnt principle leads to a uniform R m -valued random variable H with compact support H ad and mutually statistically independent components. Note that in all the following, the reduced admissible set\\nH ad = [0.25 , 0.65]√ó[20 , 250]√ó[8.5 , 17]√ó[2.15 , 5.00] in [‚àí]√ó[¬µm]√ó[GPa]√ó[GPa]\\nhas been chosen sufficiently large so that the database can cover a large enough and realistic range of values of the hyperparameters for the application presented in Section 11 corresponding to a random heterogeneous microstructure made up of a biological tissue (bovine cortical bone) and by considering the results obtained in [48]. Furthermore, in practice, the bounds of admissible set H ad may be a posteriori considered as incorrect if any component of output vector h * is close to the corresponding bounds of H ad , which is not the case for the numerical examples presented in this paper.\\nThe required numerical database should contain a set of network input and target (desired network output) vectors, where the input vectors define data regarding the random vector Q of quantities of interest, and the target vectors define data regarding the random vector H of hyperparameters. Such a database has been numerically simulated and constructed by using the random generator defined by (5). For each realization h (i) = (h\\n(i) 1 , . . . , h (i) m\\n) of uniformly distributed random vector H = (H 1 , . . . , H m ), a realization of homogeneous normalized Gaussian random field U is generated using mapping U , then the corresponding realization of random compliance field [S meso ] is generated using mapping G, and finally the associated realization q\\n(i) = (q (i) 1 , . . . , q (i) n ) of random vector Q = (Q 1 , . . . , Q n\\n) is numerically simulated using mappings M HFCMM and M EFF . The construction of the database is then straightforward and it consists of N d independent realizations x (1) , . . . , x (N d ) of random vector X = (Q, H). Hence, each element of the database can be written as x (i) = (q (i) , h (i) ) for i = 1, . . . , N d . Figure 5 provides a schematic representation of the key steps allowing the computation of the quantities of interest from the hyperparameters. Hereinafter, this database will be referred as the initial database.',\n",
       "    'n_publication_ref': 10,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Random vector of hyperparameters',\n",
       "    'text': 'H = (H 1 , H 2 , H 3 , H 4 ) Gaussian random field U = U(H 2 ) Prior Stochastic Model G Random compliance field [S meso ] High Fidelity Computational Mechanical Model M HFCMM Homogenization Computational Model M EFF Random vector (D Œµ , L Œµ 1 , L Œµ 2 ) = (Q 1 , Q 2 , Q 3 ) Random vector L eff = (Q 4 , . . . , Q 9 )\\nRandom vector of quantities of interest Q = (Q 1 , . . . , Q 9 ) Figure 5: Flowchart for computing the quantities of interest from the hyperparameters. Blue blocks refer to the models, green blocks refer to the random input parameters and fields of the computational models and red blocks refer to the random output quantities of interest of the computational models.\\nalternative approach is proposed in the present work and consists in designing an ANN that can predict another probable value h * of random vector H given Q = q obs with the available database for which the inputs will be the N d independent realizations q (1) , . . . , q (N d ) of random vector Q, and the corresponding targets will be the N d independent realizations h (1) , . . . , h (N d ) of random vector H. Indeed, ANNs are known for being particularly well-suited for addressing and solving function approximation and nonlinear regression problems. The statistical inverse problem related to the statistical identification of h * can then be viewed as a function approximation problem and solved by using an ANN trained from the available database. The solution h * of the statistical inverse problem can be simply defined as the output vector h out of the trained ANN for the given input vector q obs . Within the framework of ML techniques based on ANNs, the network input data of the initial database will refer to the N d independent realizations q (1) , . . . , q (N d ) of random vector Q and the network target data of the initial database will refer to the N d independent realizations h (1) , . . . , h (N d ) of random vector H. Nevertheless, it should be noted that since in (5a) and (5b), mapping U is random for any input argument, then the mapping between Q and H is random too. As a consequence, the supervised training of an ANN with the initial database cannot be efficient since a trained ANN is a deterministic mapping between its inputs and outputs. This is the reason why Q is substituted by another network input vector Q such that the mapping between Q and H is (almost) deterministic. It would then make it possible to efficiently train an artificial neural network. In the next section, Q k is defined as the conditional mathematical expectation of Q k given H. In practice, an observationq obs k of Q k is not available and cannot be deduced from a unique observation q obs k of Q k since it would be equivalent to solve the statistical inverse problem. Nevertheless, we propose to calculate h * as the output of the trained ANN with observation q obs = (q obs 1 , . . . , q obs 9 ) as input (instead ofq obs = (q obs 1 , . . . ,q obs 9 )).',\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'A robust solution of a statistical inverse problem in multiscale computational mechanics using an artificial neural network',\n",
       "  'abstract': 'This work addresses the inverse identification of apparent elastic properties of random heterogeneous materials using machine learning based on artificial neural networks. The proposed neural network-based identification method requires the construction of a database from which an artificial neural network can be trained to learn the nonlinear relationship between the hyperparameters of a prior stochastic model of the random compliance field and some relevant quantities of interest of an ad hoc multiscale computational model. An initial database made up with input and target data is first generated from the computational model, from which a processed database is deduced by conditioning the input data with respect to the target data using the nonparametric statistics. Two-and three-layer feedforward artificial neural networks are then trained from each of the initial and processed databases to construct an algebraic representation of the nonlinear mapping between the hyperparameters (network outputs) and the quantities of interest (network inputs). The performances of the trained artificial neural networks are analyzed in terms of mean squared error, linear regression fit and probability distribution between network outputs and targets for both databases. An ad hoc probabilistic model of the input random vector is finally proposed in order to take into account uncertainties on the network input and to perform a robustness analysis of the network output with respect to the input uncertainties level. The capability of the proposed neural network-based identification method to efficiently solve the underlying statistical inverse problem is illustrated through two numerical examples developed within the framework of 2D plane stress linear elasticity, namely a first validation example on synthetic data obtained through computational simulations and a second application example on real experimental data obtained through a physical experiment monitored by digital image correlation on a real heterogeneous biological material (beef cortical bone).',\n",
       "  'paddleOCR': [[[[1112.0, 8.0],\n",
       "     [1357.0, 13.0],\n",
       "     [1357.0, 57.0],\n",
       "     [1111.0, 52.0]],\n",
       "    ['High Fidelity', 0.9999246597290039]],\n",
       "   [[[1603.0, 31.0], [1879.0, 36.0], [1879.0, 73.0], [1603.0, 69.0]],\n",
       "    ['Random vector', 0.9889941811561584]],\n",
       "   [[[1101.0, 63.0], [1371.0, 63.0], [1371.0, 98.0], [1101.0, 98.0]],\n",
       "    ['Computational', 0.9998632669448853]],\n",
       "   [[[1605.0, 75.0], [1879.0, 80.0], [1878.0, 130.0], [1604.0, 125.0]],\n",
       "    ['(Dc,Lj,L) =', 0.9241254329681396]],\n",
       "   [[[1068.0, 115.0], [1400.0, 115.0], [1400.0, 151.0], [1068.0, 151.0]],\n",
       "    ['Mechanical Model', 0.9991340637207031]],\n",
       "   [[[1631.0, 134.0], [1854.0, 134.0], [1854.0, 176.0], [1631.0, 176.0]],\n",
       "    ['(Q1,Q2,Q3)', 0.9977948069572449]],\n",
       "   [[[1137.0, 158.0], [1330.0, 139.0], [1334.0, 183.0], [1141.0, 202.0]],\n",
       "    ['MHFCMM', 0.9946625828742981]],\n",
       "   [[[66.0, 282.0], [393.0, 282.0], [393.0, 318.0], [66.0, 318.0]],\n",
       "    ['Random vector of', 0.9999402761459351]],\n",
       "   [[[681.0, 282.0], [783.0, 287.0], [781.0, 325.0], [679.0, 319.0]],\n",
       "    ['Prior', 0.9998825788497925]],\n",
       "   [[[1161.0, 282.0], [1314.0, 282.0], [1314.0, 320.0], [1161.0, 320.0]],\n",
       "    ['Random', 0.9999544024467468]],\n",
       "   [[[1580.0, 282.0], [1908.0, 282.0], [1908.0, 318.0], [1580.0, 318.0]],\n",
       "    ['Random vector of', 0.9999427795410156]],\n",
       "   [[[76.0, 334.0], [383.0, 334.0], [383.0, 370.0], [76.0, 370.0]],\n",
       "    ['hyperparameters', 0.9998306035995483]],\n",
       "   [[[575.0, 334.0], [886.0, 334.0], [886.0, 370.0], [575.0, 370.0]],\n",
       "    ['Stochastic Model', 0.9999091625213623]],\n",
       "   [[[1085.0, 331.0], [1385.0, 326.0], [1386.0, 368.0], [1085.0, 372.0]],\n",
       "    ['compliance field', 0.9992722272872925]],\n",
       "   [[[1551.0, 333.0], [1930.0, 326.0], [1930.0, 368.0], [1552.0, 374.0]],\n",
       "    ['quantities of interest', 0.9945995211601257]],\n",
       "   [[[25.0, 378.0], [426.0, 383.0], [425.0, 425.0], [24.0, 420.0]],\n",
       "    ['H =(H1, H2, H3, H4)', 0.9083695411682129]],\n",
       "   [[[708.0, 380.0], [753.0, 380.0], [753.0, 429.0], [708.0, 429.0]],\n",
       "    ['9', 0.8621965050697327]],\n",
       "   [[[1170.0, 375.0], [1297.0, 365.0], [1301.0, 418.0], [1174.0, 428.0]],\n",
       "    ['[Smeso]', 0.9935800433158875]],\n",
       "   [[[1574.0, 383.0], [1910.0, 383.0], [1910.0, 424.0], [1574.0, 424.0]],\n",
       "    ['Q = (Q1,...,Q9)', 0.9455819725990295]],\n",
       "   [[[143.0, 512.0], [311.0, 512.0], [311.0, 548.0], [143.0, 548.0]],\n",
       "    ['Gaussian', 0.9999147653579712]],\n",
       "   [[[1089.0, 518.0], [1384.0, 518.0], [1384.0, 554.0], [1089.0, 554.0]],\n",
       "    ['Homogenization', 0.9999517798423767]],\n",
       "   [[[1603.0, 533.0], [1881.0, 537.0], [1881.0, 575.0], [1603.0, 571.0]],\n",
       "    ['Random vector', 0.9998865723609924]],\n",
       "   [[[108.0, 560.0], [342.0, 558.0], [342.0, 596.0], [109.0, 598.0]],\n",
       "    ['random field', 0.9995220303535461]],\n",
       "   [[[1036.0, 564.0], [1437.0, 564.0], [1437.0, 606.0], [1036.0, 606.0]],\n",
       "    ['Computational Model', 0.9999118447303772]],\n",
       "   [[[1561.0, 575.0], [1916.0, 584.0], [1915.0, 634.0], [1559.0, 625.0]],\n",
       "    ['Leff = (Q4,..., Q9)', 0.9133868217468262]],\n",
       "   [[[103.0, 604.0], [344.0, 611.0], [343.0, 655.0], [102.0, 648.0]],\n",
       "    ['U = U(H2)', 0.9886132478713989]],\n",
       "   [[[1164.0, 614.0], [1297.0, 594.0], [1304.0, 642.0], [1171.0, 663.0]],\n",
       "    ['MEFF', 0.9976193904876709]]],\n",
       "  'ocr': [[[1085.0, 331.0], [1385.0, 326.0], [1386.0, 368.0], [1085.0, 372.0]],\n",
       "   ['compliance field', 0.9992722272872925]]},\n",
       " '2010.15485v1-Figure1-1.png': {'caption': 'Fig. 1. Schematic view of the Belle II data acquisition system.',\n",
       "  'imageText': ['datalink',\n",
       "   'over',\n",
       "   'fibre',\n",
       "   'network',\n",
       "   'data',\n",
       "   'path',\n",
       "   'timing',\n",
       "   'distribution',\n",
       "   '(Region-of-Interest',\n",
       "   'datalink)',\n",
       "   '(SVD',\n",
       "   'RoI)',\n",
       "   'HLT',\n",
       "   'RoI',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'tx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'trig.',\n",
       "   'dist.',\n",
       "   'readout',\n",
       "   'PC',\n",
       "   'readout',\n",
       "   'PC',\n",
       "   'data',\n",
       "   'concentrator',\n",
       "   '20201022',\n",
       "   'version',\n",
       "   'on',\n",
       "   'detector',\n",
       "   'electronics-hut',\n",
       "   'computer',\n",
       "   'room',\n",
       "   'PC',\n",
       "   'based',\n",
       "   'event',\n",
       "   'builder',\n",
       "   '0',\n",
       "   'HLT',\n",
       "   'decisiondispatcher',\n",
       "   '~200',\n",
       "   'COPPERs',\n",
       "   '80kch',\n",
       "   '240kch',\n",
       "   '8Mch',\n",
       "   'ARICH',\n",
       "   'and',\n",
       "   'others',\n",
       "   'CDC,TOP,ECL',\n",
       "   'ECL,TRG',\n",
       "   'SVD',\n",
       "   'PXD',\n",
       "   'D',\n",
       "   'etecto',\n",
       "   'r',\n",
       "   'sig',\n",
       "   'n',\n",
       "   'al',\n",
       "   'RAID',\n",
       "   '10',\n",
       "   'HLT',\n",
       "   'farms',\n",
       "   'units',\n",
       "   '~400',\n",
       "   'CPU',\n",
       "   'cores',\n",
       "   '/',\n",
       "   'unit',\n",
       "   'even',\n",
       "   't',\n",
       "   'b',\n",
       "   'u',\n",
       "   'ild',\n",
       "   'er',\n",
       "   '2',\n",
       "   'rk',\n",
       "   'sw',\n",
       "   'itch',\n",
       "   'b',\n",
       "   'ased',\n",
       "   '10GbE',\n",
       "   'netw',\n",
       "   'o',\n",
       "   'GbE',\n",
       "   'Aurora-based',\n",
       "   'datalink',\n",
       "   'n',\n",
       "   'etw',\n",
       "   'o',\n",
       "   'rk',\n",
       "   'sw',\n",
       "   'itch',\n",
       "   'b',\n",
       "   'ased',\n",
       "   'even',\n",
       "   't',\n",
       "   'b',\n",
       "   'u',\n",
       "   'ild',\n",
       "   'er',\n",
       "   '1',\n",
       "   'Belle2link',\n",
       "   '(custom',\n",
       "   'datalink)',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'tx',\n",
       "   'F/E',\n",
       "   'elec',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'tx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'rx',\n",
       "   'rx',\n",
       "   'COPPER',\n",
       "   'ATCA',\n",
       "   'based',\n",
       "   'FPGA',\n",
       "   'processor',\n",
       "   'PC',\n",
       "   'or',\n",
       "   'ATCA',\n",
       "   'based',\n",
       "   'processor',\n",
       "   'PC',\n",
       "   'or',\n",
       "   'ATCA',\n",
       "   'based',\n",
       "   'processor',\n",
       "   'PC',\n",
       "   'or',\n",
       "   'ATCA',\n",
       "   'based',\n",
       "   'processor'],\n",
       "  'image_file': '2010.15485v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'II. UNIFIED READOUT SYSTEM',\n",
       "    'text': 'In order to read out the events from the seven subdetectors, we adopt a highly unified readout system [3] [4], including a unified trigger timing distribution (TTD) system for the entire Belle II detector, a unified high speed data link system called \"Belle2link\" which is used by all subdetectors except PXD, and a common backend system called \"COPPER\" to receive the Belle2link data. Every subdetector frontend electronics (FEE) device has an FPGA in which the unified firmware components of TTD receiver and Belle2link transmitter are embedded.\\nThe system aims for taking data at 30 kHz trigger rate with a dead-time fraction of about 1% from the frontend readout system. The read-out data are sent to the backend data acquisition system comprised of the event builder, high level trigger and storage system. The schematic view of the Belle data acquisition system is given in Fig. 1.  ',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Performance of the Unified Readout System of Belle II',\n",
       "  'abstract': 'Belle II experiment at the SuperKEKB collider at KEK, Tsukuba, Japan has successfully started the data taking with the full detector in March 2019. Belle II is a luminosity frontier experiment of the new generation to search for physics beyond the Standard Model of elementary particles, from precision measurements of a huge number of B and charm mesons and tau leptons. In order to read out the events at a high rate from the seven subdetectors of Belle II, we adopt a highly unified readout system, including a unified trigger timing distribution system (TTD), a unified high speed data link system (Belle2link), and a common backend system to receive Belle2link data. Each subdetector frontend readout system has a field-programmable gate array (FPGA) in which unified firmware components of the TTD receiver and Belle2link transmitter are embedded. The system is designed for data taking at a trigger rate up to 30 kHz with a dead-time fraction of about 1% in the frontend readout system. The trigger rate during the nominal operation is still much lower than our design. However, the background level is already high due to the initial vacuum condition and other accelerator parameters, and it is the most limiting factor of the accelerator and detector operation. Hence the occupancy and the stress to the frontend electronics are rather severe, and they cause various kind of instabilities. We present the performance of the system, including the achieved trigger rate, dead-time fraction, stability, and discuss the experiences gained during the operation.',\n",
       "  'paddleOCR': [[[[40.0, 0.0], [77.0, 0.0], [77.0, 15.0], [40.0, 15.0]],\n",
       "    ['PXD', 0.9980080723762512]],\n",
       "   [[[45.0, 14.0], [78.0, 14.0], [78.0, 26.0], [45.0, 26.0]],\n",
       "    ['8Mch', 0.997207760810852]],\n",
       "   [[[184.0, 14.0], [316.0, 14.0], [316.0, 25.0], [184.0, 25.0]],\n",
       "    ['Aurora-ba$ed datalink', 0.9684564471244812]],\n",
       "   [[[977.0, 10.0], [1063.0, 10.0], [1063.0, 23.0], [977.0, 23.0]],\n",
       "    ['20201022 version', 0.9998258352279663]],\n",
       "   [[[138.0, 36.0], [160.0, 36.0], [160.0, 56.0], [138.0, 56.0]],\n",
       "    ['tx', 0.8069976568222046]],\n",
       "   [[[761.0, 35.0], [836.0, 35.0], [836.0, 47.0], [761.0, 47.0]],\n",
       "    ['ATCA based', 0.9996268153190613]],\n",
       "   [[[761.0, 50.0], [797.0, 50.0], [797.0, 62.0], [761.0, 62.0]],\n",
       "    ['FPGA', 0.9969749450683594]],\n",
       "   [[[940.0, 42.0], [1059.0, 42.0], [1059.0, 57.0], [940.0, 57.0]],\n",
       "    [' datalink over fibre', 0.9919475317001343]],\n",
       "   [[[110.0, 62.0], [163.0, 62.0], [163.0, 74.0], [110.0, 74.0]],\n",
       "    ['F/E elec', 0.9747871160507202]],\n",
       "   [[[571.0, 56.0], [600.0, 53.0], [602.0, 66.0], [572.0, 69.0]],\n",
       "    ['data', 0.99619460105896]],\n",
       "   [[[571.0, 72.0], [654.0, 72.0], [654.0, 84.0], [571.0, 84.0]],\n",
       "    ['concentrator', 0.9982696175575256]],\n",
       "   [[[674.0, 71.0], [731.0, 71.0], [731.0, 83.0], [674.0, 83.0]],\n",
       "    ['(SVD Rol)', 0.9224473237991333]],\n",
       "   [[[759.0, 67.0], [823.0, 67.0], [823.0, 79.0], [759.0, 79.0]],\n",
       "    ['processor', 0.9990997314453125]],\n",
       "   [[[940.0, 65.0], [1059.0, 65.0], [1059.0, 79.0], [940.0, 79.0]],\n",
       "    ['network data path', 0.9664003849029541]],\n",
       "   [[[301.0, 77.0], [394.0, 77.0], [394.0, 88.0], [301.0, 88.0]],\n",
       "    ['-200 COPPERs', 0.9284853339195251]],\n",
       "   [[[403.0, 76.0], [558.0, 76.0], [558.0, 87.0], [403.0, 87.0]],\n",
       "    ['(Region-of-Interest datalink)', 0.9763951897621155]],\n",
       "   [[[943.0, 86.0], [1056.0, 86.0], [1056.0, 97.0], [943.0, 97.0]],\n",
       "    ['timing distribution', 0.9972672462463379]],\n",
       "   [[[438.0, 98.0], [498.0, 98.0], [498.0, 109.0], [438.0, 109.0]],\n",
       "    ['PC based', 0.9648160934448242]],\n",
       "   [[[437.0, 110.0], [530.0, 109.0], [530.0, 121.0], [437.0, 122.0]],\n",
       "    ['event builder 0', 0.9549954533576965]],\n",
       "   [[[747.0, 102.0], [829.0, 102.0], [829.0, 114.0], [747.0, 114.0]],\n",
       "    ['HLT decision', 0.9961796402931213]],\n",
       "   [[[40.0, 113.0], [78.0, 113.0], [78.0, 130.0], [40.0, 130.0]],\n",
       "    ['SVD', 0.9963187575340271]],\n",
       "   [[[332.0, 113.0], [386.0, 113.0], [386.0, 125.0], [332.0, 125.0]],\n",
       "    ['COPPER', 0.9977972507476807]],\n",
       "   [[[8.0, 123.0], [23.0, 123.0], [23.0, 137.0], [8.0, 137.0]],\n",
       "    ['0', 0.5838689208030701]],\n",
       "   [[[403.0, 118.0], [431.0, 118.0], [431.0, 130.0], [403.0, 130.0]],\n",
       "    ['GbE', 0.9973635673522949]],\n",
       "   [[[750.0, 115.0], [814.0, 114.0], [814.0, 126.0], [750.0, 127.0]],\n",
       "    ['dispatcher', 0.9979785680770874]],\n",
       "   [[[6.0, 134.0], [25.0, 134.0], [25.0, 209.0], [6.0, 209.0]],\n",
       "    ['etector', 0.9966269135475159]],\n",
       "   [[[44.0, 128.0], [87.0, 128.0], [87.0, 140.0], [44.0, 140.0]],\n",
       "    ['240kch', 0.9972532391548157]],\n",
       "   [[[114.0, 135.0], [162.0, 135.0], [162.0, 147.0], [114.0, 147.0]],\n",
       "    ['F/E elec', 0.9624451994895935]],\n",
       "   [[[338.0, 135.0], [358.0, 135.0], [358.0, 156.0], [338.0, 156.0]],\n",
       "    ['rx', 0.9859007596969604]],\n",
       "   [[[459.0, 135.0], [509.0, 135.0], [509.0, 146.0], [459.0, 146.0]],\n",
       "    ['readout', 0.997279703617096]],\n",
       "   [[[138.0, 155.0], [161.0, 155.0], [161.0, 173.0], [138.0, 173.0]],\n",
       "    ['tx', 0.7622942924499512]],\n",
       "   [[[336.0, 151.0], [360.0, 151.0], [360.0, 176.0], [336.0, 176.0]],\n",
       "    ['rx', 0.5551733374595642]],\n",
       "   [[[459.0, 149.0], [480.0, 149.0], [480.0, 161.0], [459.0, 161.0]],\n",
       "    ['PC', 0.9948461055755615]],\n",
       "   [[[579.0, 161.0], [598.0, 161.0], [602.0, 278.0], [583.0, 279.0]],\n",
       "    ['event builder', 0.9978117346763611]],\n",
       "   [[[600.0, 160.0], [619.0, 160.0], [620.0, 350.0], [601.0, 351.0]],\n",
       "    ['network switch based', 0.9992456436157227]],\n",
       "   [[[636.0, 160.0], [677.0, 160.0], [677.0, 172.0], [636.0, 172.0]],\n",
       "    ['10GbE', 0.9755028486251831]],\n",
       "   [[[861.0, 162.0], [877.0, 162.0], [878.0, 280.0], [862.0, 280.0]],\n",
       "    ['event builder', 0.9829949736595154]],\n",
       "   [[[884.0, 163.0], [899.0, 163.0], [899.0, 348.0], [884.0, 348.0]],\n",
       "    ['network switch based', 0.9676874876022339]],\n",
       "   [[[350.0, 183.0], [366.0, 183.0], [366.0, 228.0], [350.0, 228.0]],\n",
       "    ['000', 0.5932895541191101]],\n",
       "   [[[40.0, 197.0], [97.0, 197.0], [97.0, 212.0], [40.0, 212.0]],\n",
       "    ['ARICH', 0.9983649253845215]],\n",
       "   [[[474.0, 195.0], [484.0, 195.0], [484.0, 207.0], [474.0, 207.0]],\n",
       "    ['.', 0.5683708190917969]],\n",
       "   [[[4.0, 206.0], [25.0, 208.0], [22.0, 248.0], [1.0, 246.0]],\n",
       "    ['sigr', 0.9962524175643921]],\n",
       "   [[[49.0, 211.0], [87.0, 211.0], [87.0, 223.0], [49.0, 223.0]],\n",
       "    ['80kch', 0.9940060377120972]],\n",
       "   [[[125.0, 206.0], [137.0, 206.0], [137.0, 217.0], [125.0, 217.0]],\n",
       "    ['0', 0.5587189793586731]],\n",
       "   [[[36.0, 232.0], [112.0, 232.0], [112.0, 244.0], [36.0, 244.0]],\n",
       "    ['and others', 0.9561640620231628]],\n",
       "   [[[36.0, 245.0], [116.0, 245.0], [116.0, 257.0], [36.0, 257.0]],\n",
       "    ['CDC,TOP,ECI', 0.9225504398345947]],\n",
       "   [[[174.0, 243.0], [225.0, 243.0], [225.0, 254.0], [174.0, 254.0]],\n",
       "    ['Belle2lin', 0.9896705150604248]],\n",
       "   [[[35.0, 256.0], [92.0, 257.0], [91.0, 269.0], [34.0, 268.0]],\n",
       "    ['ECL,TRG', 0.9674714207649231]],\n",
       "   [[[174.0, 257.0], [277.0, 257.0], [277.0, 271.0], [174.0, 271.0]],\n",
       "    ['(custom dacalink', 0.939953088760376]],\n",
       "   [[[332.0, 259.0], [386.0, 259.0], [386.0, 271.0], [332.0, 271.0]],\n",
       "    ['COPPER', 0.9979991316795349]],\n",
       "   [[[115.0, 280.0], [162.0, 280.0], [162.0, 292.0], [115.0, 292.0]],\n",
       "    ['F/E elec', 0.9722775220870972]],\n",
       "   [[[338.0, 281.0], [358.0, 281.0], [358.0, 299.0], [338.0, 299.0]],\n",
       "    ['rx', 0.9900065660476685]],\n",
       "   [[[460.0, 274.0], [508.0, 274.0], [508.0, 285.0], [460.0, 285.0]],\n",
       "    ['readout', 0.9961032271385193]],\n",
       "   [[[460.0, 288.0], [480.0, 288.0], [480.0, 300.0], [460.0, 300.0]],\n",
       "    ['PC', 0.9941362142562866]],\n",
       "   [[[139.0, 300.0], [160.0, 300.0], [160.0, 319.0], [139.0, 319.0]],\n",
       "    ['tx', 0.9378437995910645]],\n",
       "   [[[338.0, 300.0], [358.0, 300.0], [358.0, 319.0], [338.0, 319.0]],\n",
       "    ['rx', 0.9794067740440369]],\n",
       "   [[[677.0, 358.0], [810.0, 359.0], [810.0, 375.0], [677.0, 373.0]],\n",
       "    ['10 HLT farms units', 0.9994531273841858]],\n",
       "   [[[1015.0, 357.0], [1059.0, 357.0], [1059.0, 373.0], [1015.0, 373.0]],\n",
       "    ['RAID', 0.9984927773475647]],\n",
       "   [[[334.0, 367.0], [359.0, 367.0], [359.0, 380.0], [334.0, 380.0]],\n",
       "    ['trig.', 0.9581928253173828]],\n",
       "   [[[335.0, 382.0], [361.0, 382.0], [361.0, 395.0], [335.0, 395.0]],\n",
       "    ['dist.', 0.992436408996582]],\n",
       "   [[[677.0, 373.0], [828.0, 374.0], [828.0, 389.0], [677.0, 387.0]],\n",
       "    ['~400 CPU cores / unit', 0.9852942228317261]],\n",
       "   [[[1.0, 399.0], [102.0, 396.0], [103.0, 415.0], [1.0, 417.0]],\n",
       "    ['on detector', 0.9907171130180359]],\n",
       "   [[[527.0, 395.0], [655.0, 395.0], [655.0, 410.0], [527.0, 410.0]],\n",
       "    ['electronics-hut', 0.9846716523170471]],\n",
       "   [[[934.0, 396.0], [1063.0, 396.0], [1063.0, 410.0], [934.0, 410.0]],\n",
       "    ['computer room', 0.9565830826759338]]],\n",
       "  'ocr': [[[45.0, 14.0], [78.0, 14.0], [78.0, 26.0], [45.0, 26.0]],\n",
       "   ['8Mch', 0.997207760810852]]},\n",
       " '2202.10169v2-Figure7-1.png': {'caption': 'Figure 7: ClearML stack architecture',\n",
       "  'imageText': [],\n",
       "  'image_file': '2202.10169v2-Figure7-1.png',\n",
       "  'sections': [{'heading': 'DataRobot',\n",
       "    'text': 'The DataRobot MLOps platform supplies a single place to deploy, monitor, manage models in productions regardless of how they were created, when and where they were deployed 15 . It has a model registry to store and manage all production deployed models. As shown in Figure 6, from ML development to consumption, DataRobot facilitates ML life cycle stages. It also supports many programming languages, libraries, development environments and maintains code repositories. However, individual users are required to purchase licenses for each instance to embedded usages. Figure 6: DataRobot tool architecture 16 3.5 Allegro.ai (ClearML)\\nAllegro.ai provides open-source MLOps tools to deliver products efficiently 17 . ClearML is a product of Allegro.ai that enables a single place to experiment, orchestrate, deploy and build data store 18 . The main stages of ClearML are named as experiment, orchestrate, DataOps, hyper-datasets, deploy, and remote. Figure 7 shows the model architecture and supports customizability. In addition, ClearML supports a set of modules. For instance, the ClearML python package integrates the codebase with the framework. ClearML Server consists of controlling features for MLOps while storing experiments, models, and workflow data. ClearML agent provides orchestration, reproducibility, scalability functionalities. ClearML session module provides remote instances of Jupyter Notebooks and VSCode.',\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'MACHINE LEARNING OPERATIONS: A SURVEY ON MLOPS TOOL SUPPORT *',\n",
       "  'abstract': 'Machine Learning (ML) has become a fast-growing, trending approach in solution development in practice. Deep Learning (DL) which is a subset of ML, learns using deep neural networks to simulate the human brain. It trains machines to learn techniques and processes individually using computer algorithms, which is also considered to be a role of Artificial Intelligence (AI). In this paper, we study current technical issues related to software development and delivery in organizations that work on ML projects. Therefore, the importance of the Machine Learning Operations (MLOps) concept, which can deliver appropriate solutions for such concerns, is discussed. We investigate commercially available MLOps tool support in software development. The comparison between MLOps tools analyzes the performance of each system and its use cases. Moreover, we examine the features and usability of MLOps tools to identify the most appropriate tool support for given scenarios. Finally, we recognize that there is a shortage in the availability of a fully functional MLOps platform on which processes can be automated by reducing human intervention.',\n",
       "  'paddleOCR': [[[[410.0, 44.0], [627.0, 44.0], [627.0, 70.0], [410.0, 70.0]],\n",
       "    ['CLEARIML', 0.9948500394821167]],\n",
       "   [[[347.0, 196.0], [551.0, 196.0], [551.0, 238.0], [347.0, 238.0]],\n",
       "    ['ClearML', 0.9980537295341492]],\n",
       "   [[[613.0, 294.0], [648.0, 325.0], [630.0, 345.0], [595.0, 315.0]],\n",
       "    ['(0))', 0.8295696973800659]],\n",
       "   [[[850.0, 288.0], [926.0, 299.0], [920.0, 339.0], [844.0, 328.0]],\n",
       "    ['App', 0.9981898665428162]],\n",
       "   [[[347.0, 464.0], [695.0, 467.0], [695.0, 516.0], [346.0, 514.0]],\n",
       "    ['MLOps Engine', 0.9356837868690491]],\n",
       "   [[[812.0, 565.0], [922.0, 565.0], [922.0, 596.0], [812.0, 596.0]],\n",
       "    ['Meta+', 0.996263861656189]],\n",
       "   [[[117.0, 585.0], [266.0, 585.0], [266.0, 616.0], [117.0, 616.0]],\n",
       "    ['Execution', 0.9977583289146423]],\n",
       "   [[[321.0, 586.0], [517.0, 586.0], [517.0, 616.0], [321.0, 616.0]],\n",
       "    ['Orchestration', 0.9982496500015259]],\n",
       "   [[[589.0, 580.0], [708.0, 587.0], [706.0, 621.0], [587.0, 614.0]],\n",
       "    ['Serving', 0.9986189603805542]],\n",
       "   [[[796.0, 610.0], [938.0, 610.0], [938.0, 640.0], [796.0, 640.0]],\n",
       "    ['Datastore', 0.9985959529876709]],\n",
       "   [[[371.0, 750.0], [709.0, 750.0], [709.0, 795.0], [371.0, 795.0]],\n",
       "    ['Hybrid Cluster', 0.9804918169975281]],\n",
       "   [[[144.0, 867.0], [312.0, 869.0], [311.0, 899.0], [143.0, 896.0]],\n",
       "    ['Kubernetes', 0.9991647005081177]],\n",
       "   [[[457.0, 863.0], [594.0, 867.0], [593.0, 897.0], [456.0, 893.0]],\n",
       "    ['On-Prem', 0.9992436170578003]],\n",
       "   [[[787.0, 864.0], [880.0, 864.0], [880.0, 895.0], [787.0, 895.0]],\n",
       "    ['Cloud', 0.9990534782409668]]],\n",
       "  'ocr': [[[850.0, 288.0], [926.0, 299.0], [920.0, 339.0], [844.0, 328.0]],\n",
       "   ['App', 0.9981898665428162]]},\n",
       " '2204.08763v1-Figure2-1.png': {'caption': 'Figure 2. Illustration of our FR-IQA network. It adopts a dual-branch structure for feature extraction, i.e., one for reference and another for distortion. The feature extraction network performs feature extraction on reference and distortion images at three scales. The distance calculation module generates the difference map between the above two features. The spatial attention module gives greater weight on more informative regions to obtain the calibrated difference map, which is then fed into score prediction network to predict the final score.',\n",
       "  'imageText': ['Feature',\n",
       "   'Extraction',\n",
       "   'Network',\n",
       "   'Score',\n",
       "   'Prediction',\n",
       "   'Network',\n",
       "   '3',\n",
       "   '√ó',\n",
       "   '3',\n",
       "   'Conv',\n",
       "   '+',\n",
       "   'ReLU',\n",
       "   '1',\n",
       "   '√ó',\n",
       "   '1',\n",
       "   'Conv',\n",
       "   '+',\n",
       "   'ReLU',\n",
       "   'L2',\n",
       "   'Pooling',\n",
       "   'DAB',\n",
       "   'Dual',\n",
       "   'Attention',\n",
       "   'Block',\n",
       "   'GAP(S)',\n",
       "   'Spatial-wiseGlobal',\n",
       "   'Average',\n",
       "   'Pooling',\n",
       "   'Score',\n",
       "   'AVG',\n",
       "   'Prediction',\n",
       "   'GAP(S)',\n",
       "   'GAP(S)',\n",
       "   'GAP(S)',\n",
       "   'Spatial',\n",
       "   'Attention',\n",
       "   'LocalSW',\n",
       "   'Distance',\n",
       "   'DAB',\n",
       "   'DAB',\n",
       "   'DAB',\n",
       "   'DAB',\n",
       "   'DAB',\n",
       "   'DAB',\n",
       "   'fsDiff',\n",
       "   'f3Dis',\n",
       "   'fsDist',\n",
       "   'f2Dis',\n",
       "   'IDis',\n",
       "   'f',\n",
       "   '1',\n",
       "   'Dis',\n",
       "   'f3Ref',\n",
       "   'f2Ref',\n",
       "   'f1Ref',\n",
       "   'IRef'],\n",
       "  'image_file': '2204.08763v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'FR-IQA Network Structure',\n",
       "    'text': 'As shown in Fig. 2, our proposed FR-IQA consists of a feature extraction network and a score prediction network. The feature extraction network adopts a Siamese (i.e., dualbranch) structure, which respectively takes the reference image and the distorted image as the input. It is based on VGG16 [52] consisting of three different scales, i.e., s = 1, 2 and 3. And we further modify the VGG16 network from two aspects. First, all max pooling layers in VGG are replaced with L 2 pooling [25] to avoid aliasing when downsampling by a factor of two. Second, to increase the fitting ability, dual attention blocks (DAB) used in [67]  Ref and distortion feature f s Dis (s = 1, 2, 3), respectively. Then, local sliced Wasserstein (LocalSW) distance is presented to produce distance map f s Dist , and a spatial attention module is deployed for reweighting distance map to generate calibrated difference map f s Diff for each scale s. As shown in Fig. 2, the score prediction network has three branches, where each branch involves two 1√ó1 convolutional layers and a spatial-wise global averaging pooling layer. f s Diff is fed to the s-th branch to generate the score at scale s, and the scores at all scales are averaged to produce the final score.\\nIn the following, we elaborate more on the LocalSW distance and difference map calibration.\\nLocalSW Distance. Given the reference feature f s Ref and distortion feature f s Dis , one direct solution is the element-wise difference, i.e., |f s Ref ‚àí f s Dis |. Here | ‚Ä¢ | denotes element-wise absolute value. However, GAN-based restoration is prone to producing results being spatially distorted and misaligned with the reference image, while the element-wise difference is not robust to spatial misalign-  ment. Instead, we suggest local sliced Wasserstein (Lo-calSW) distance which measures the difference by comparing the distributions of feature maps. Previously sliced Wasserstein loss [12,24] has been proposed to calculate the global sliced Wasserstein distance. Considering that the misalignment between f s Ref and f s Dis is usually local and within a small range, we adopt LocalSW distance by dividing f s Ref and f s Dis (‚àà R H√óW √óC ) into J non-overlapped patches with resolution p √ó p, i.e., J = (H/p) √ó (W/p). Furthermore, we compute the LocalSW distance for all slices and all patches to form the LocalSW distance map\\nf s Dist ‚àà R H p √ó W p √óm . Spatial Attention for Difference Map Calibra- tion.\\nObviously, the contribution of image region to visual quality is spatially varying. Informative regions have more influences and should be emphasized more when predicting the final score. In learning-based FR-IQA, ASNA [3] computes spatial and channel attention based on decoder feature to improve MOS estimation. Actually, the importance of local region should be determined by the reference image instead of decoder feature and distance map. Thus, we adopt a much simple design by computing spatial attention based on reference feature while applying it on distance map to generate calibrated difference map. As show in Fig. 4, the spatial attention module takes reference feature f s Ref at scale s as input. Then, we use two 3 √ó 3 convolutional layers followed by global average pooling and max pooling along the channel dimension to form a feature map f s M . Finally, a 1 √ó 1 convolutional layer followed by sigmoid activation Table 1. Summary of five IQA databases, i.e., LIVE [47], CSIQ [33], TID2013 [45], KADID-10k [35] and PIPAL [19]. DMOS is inversely proportional to MOS. and local average pooling is deployed to generate spatial weighting map\\nf s W ‚àà R H p √ó W p\\n, where the size of the local average pooling region is set to p √ó p. Calibrated difference map f s Diff can then be obtained by using f s W for reweighting each channel of distance map f s\\nDist in an element-wise manner, while final score can be predicted by feeding f s Diff into score prediction network.',\n",
       "    'n_publication_ref': 11,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment',\n",
       "  'abstract': 'Full-reference (FR) image quality assessment (IQA) evaluates the visual quality of a distorted image by measuring its perceptual difference with pristine-quality reference, and has been widely used in low-level vision tasks. Pairwise labeled data with mean opinion score (MOS) are required in training FR-IQA model, but is time-consuming and cumbersome to collect. In contrast, unlabeled data can be easily collected from an image degradation or restoration process, making it encouraging to exploit unlabeled training data to boost FR-IQA performance. Moreover, due to the distribution inconsistency between labeled and unlabeled data, outliers may occur in unlabeled data, further increasing the training difficulty. In this paper, we suggest to incorporate semi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled data while mitigating the adverse effect of outliers. Particularly, by treating all labeled data as positive samples, PU learning is leveraged to identify negative samples (i.e., outliers) from unlabeled data. Semi-supervised learning (SSL) is further deployed to exploit positive unlabeled data by dynamically generating pseudo-MOS. We adopt a dual-branch network including reference and distortion branches. Furthermore, spatial attention is introduced in the reference branch to concentrate more on the informative regions, and sliced Wasserstein distance is used for robust difference map computation to address the misalignment issues caused by images recovered by GAN models. Extensive experiments show that our method performs favorably against state-of-the-arts on the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ. The source code and model are available at https://github.com/happycaoyue/JSPL.',\n",
       "  'paddleOCR': [[[[577.0, 4.0], [858.0, 4.0], [858.0, 18.0], [577.0, 18.0]],\n",
       "    ['-eature Extraction Network', 0.9405072927474976]],\n",
       "   [[[1576.0, 4.0], [1648.0, 4.0], [1648.0, 18.0], [1576.0, 18.0]],\n",
       "    ['ScoreP', 0.9171769618988037]],\n",
       "   [[[1693.0, 4.0], [1832.0, 4.0], [1832.0, 18.0], [1693.0, 18.0]],\n",
       "    ['ction Network', 0.9629231691360474]],\n",
       "   [[[1661.0, 128.0], [1750.0, 133.0], [1748.0, 162.0], [1660.0, 156.0]],\n",
       "    ['GAP(S)', 0.9945867657661438]],\n",
       "   [[[641.0, 175.0], [691.0, 175.0], [691.0, 198.0], [641.0, 198.0]],\n",
       "    ['DAB', 0.9986154437065125]],\n",
       "   [[[864.0, 175.0], [904.0, 175.0], [904.0, 192.0], [864.0, 192.0]],\n",
       "    ['DAB', 0.9969024658203125]],\n",
       "   [[[889.0, 263.0], [974.0, 263.0], [974.0, 285.0], [889.0, 285.0]],\n",
       "    ['LocalSW', 0.9305373430252075]],\n",
       "   [[[1540.0, 261.0], [1625.0, 261.0], [1625.0, 289.0], [1540.0, 289.0]],\n",
       "    ['GAP(S)', 0.9961050152778625]],\n",
       "   [[[1865.0, 255.0], [1964.0, 255.0], [1964.0, 275.0], [1865.0, 275.0]],\n",
       "    ['Prediction', 0.9987063407897949]],\n",
       "   [[[1116.0, 265.0], [1188.0, 260.0], [1189.0, 283.0], [1118.0, 288.0]],\n",
       "    [' Spatial', 0.9662498831748962]],\n",
       "   [[[1749.0, 267.0], [1789.0, 267.0], [1789.0, 283.0], [1749.0, 283.0]],\n",
       "    ['AVC', 0.8228031992912292]],\n",
       "   [[[889.0, 283.0], [974.0, 283.0], [974.0, 306.0], [889.0, 306.0]],\n",
       "    ['Distance', 0.9997883439064026]],\n",
       "   [[[1110.0, 285.0], [1201.0, 285.0], [1201.0, 306.0], [1110.0, 306.0]],\n",
       "    ['Attention', 0.9996878504753113]],\n",
       "   [[[1890.0, 279.0], [1940.0, 279.0], [1940.0, 294.0], [1890.0, 294.0]],\n",
       "    ['Score', 0.9992210268974304]],\n",
       "   [[[1013.0, 318.0], [1052.0, 318.0], [1052.0, 334.0], [1013.0, 334.0]],\n",
       "    ['Dist', 0.9856275320053101]],\n",
       "   [[[221.0, 363.0], [265.0, 363.0], [265.0, 379.0], [221.0, 379.0]],\n",
       "    ['DAB', 0.9974067211151123]],\n",
       "   [[[494.0, 359.0], [531.0, 359.0], [531.0, 375.0], [494.0, 375.0]],\n",
       "    ['DAE', 0.9614904522895813]],\n",
       "   [[[715.0, 357.0], [755.0, 357.0], [755.0, 371.0], [715.0, 371.0]],\n",
       "    ['DAB', 0.9964422583580017]],\n",
       "   [[[1413.0, 385.0], [1502.0, 390.0], [1500.0, 420.0], [1412.0, 415.0]],\n",
       "    ['GAP(S)', 0.9913525581359863]],\n",
       "   [[[2.0, 429.0], [61.0, 437.0], [58.0, 461.0], [0.0, 454.0]],\n",
       "    ['IDis', 0.9952242374420166]],\n",
       "   [[[207.0, 526.0], [416.0, 526.0], [416.0, 552.0], [207.0, 552.0]],\n",
       "    ['3 x 3 Conv + ReLU', 0.9846125245094299]],\n",
       "   [[[546.0, 526.0], [763.0, 526.0], [763.0, 552.0], [546.0, 552.0]],\n",
       "    ['1 X 1 Conv + ReLU', 0.9684247374534607]],\n",
       "   [[[901.0, 519.0], [1014.0, 526.0], [1012.0, 557.0], [899.0, 550.0]],\n",
       "    ['L2 Pooling', 0.9560264348983765]],\n",
       "   [[[1172.0, 524.0], [1385.0, 524.0], [1385.0, 550.0], [1172.0, 550.0]],\n",
       "    ['Dual Attention Block', 0.9844415783882141]],\n",
       "   [[[1495.0, 526.0], [1578.0, 526.0], [1578.0, 554.0], [1495.0, 554.0]],\n",
       "    ['GAP(S)', 0.99835205078125]],\n",
       "   [[[1642.0, 524.0], [1758.0, 524.0], [1758.0, 544.0], [1642.0, 544.0]],\n",
       "    ['Spatial-wise', 0.9990318417549133]],\n",
       "   [[[1110.0, 536.0], [1158.0, 536.0], [1158.0, 552.0], [1110.0, 552.0]],\n",
       "    ['DAB', 0.9887515902519226]],\n",
       "   [[[1605.0, 540.0], [1820.0, 542.0], [1819.0, 565.0], [1604.0, 562.0]],\n",
       "    ['Global Average Pooling', 0.9987087249755859]]],\n",
       "  'ocr': [[[1693.0, 4.0], [1832.0, 4.0], [1832.0, 18.0], [1693.0, 18.0]],\n",
       "   ['ction Network', 0.9629231691360474]]},\n",
       " '2107.09008v2-Figure5-1.png': {'caption': 'Fig. 5. MIC diagram for Clubhouse as of January 2022. Affordance and relationship updates shown in red.',\n",
       "  'imageText': ['Platform-level',\n",
       "   'rules',\n",
       "   'and',\n",
       "   'etiquette',\n",
       "   'guidelines;',\n",
       "   'Club-speciÔ¨Åc',\n",
       "   'rules',\n",
       "   'room',\n",
       "   'hosts',\n",
       "   'App',\n",
       "   'has',\n",
       "   'feature',\n",
       "   'that',\n",
       "   'allows',\n",
       "   'users',\n",
       "   'to',\n",
       "   'send',\n",
       "   'money',\n",
       "   'to',\n",
       "   '‚Äúpopular‚Äù',\n",
       "   'Microphone',\n",
       "   'marker',\n",
       "   'Moderator',\n",
       "   'marker;',\n",
       "   'new',\n",
       "   'participant',\n",
       "   'maker;',\n",
       "   'block',\n",
       "   'list',\n",
       "   'marker;',\n",
       "   'communication',\n",
       "   '(Instagram,',\n",
       "   'Reddit)',\n",
       "   'for',\n",
       "   'text-based',\n",
       "   'Some',\n",
       "   'Clubs',\n",
       "   'direct',\n",
       "   'users',\n",
       "   'to',\n",
       "   'other',\n",
       "   'platforms',\n",
       "   'rooms',\n",
       "   'Live',\n",
       "   'rooms',\n",
       "   'are',\n",
       "   'organized',\n",
       "   'by',\n",
       "   'clubs',\n",
       "   'and',\n",
       "   'topics;',\n",
       "   'organization',\n",
       "   'is',\n",
       "   'largely',\n",
       "   'open',\n",
       "   'and',\n",
       "   'users',\n",
       "   'can',\n",
       "   'Ô¨Ånd',\n",
       "   'random',\n",
       "   'rooms',\n",
       "   'if',\n",
       "   'they',\n",
       "   'appear',\n",
       "   'on',\n",
       "   'their',\n",
       "   'homepage',\n",
       "   'Every',\n",
       "   'room',\n",
       "   'has',\n",
       "   'a',\n",
       "   'moderator;',\n",
       "   'App',\n",
       "   'keeps',\n",
       "   'recordings',\n",
       "   'of',\n",
       "   'rooms',\n",
       "   'for',\n",
       "   'a',\n",
       "   'short',\n",
       "   'period',\n",
       "   'of',\n",
       "   'time',\n",
       "   'to',\n",
       "   'review',\n",
       "   'in',\n",
       "   'case',\n",
       "   'of',\n",
       "   'reported',\n",
       "   'incident;',\n",
       "   'Room',\n",
       "   'moderators',\n",
       "   'can',\n",
       "   'record',\n",
       "   'select',\n",
       "   'users.',\n",
       "   'Audio',\n",
       "   'and',\n",
       "   'Text',\n",
       "   'App',\n",
       "   'is',\n",
       "   'Invite',\n",
       "   'Only;',\n",
       "   'Speaking',\n",
       "   'in',\n",
       "   'a',\n",
       "   'room',\n",
       "   'requires',\n",
       "   'permission',\n",
       "   'from',\n",
       "   'moderator;',\n",
       "   'Club',\n",
       "   'creation',\n",
       "   'is',\n",
       "   'limited',\n",
       "   'to',\n",
       "   'Synchronous',\n",
       "   'Ephemeral',\n",
       "   'and',\n",
       "   'Non-Ephemeral',\n",
       "   'Users',\n",
       "   'must',\n",
       "   'be',\n",
       "   'IdentiÔ¨Åable',\n",
       "   'Leaders',\n",
       "   'Clubs:',\n",
       "   'Admins,',\n",
       "   'Members,',\n",
       "   'Rooms:',\n",
       "   'Moderator,',\n",
       "   'Speaker,',\n",
       "   'and',\n",
       "   'Listener',\n",
       "   '(January',\n",
       "   '2022)',\n",
       "   'access',\n",
       "   'modalities',\n",
       "   'ephemeralitysynchronicity',\n",
       "   'inter-platform',\n",
       "   'mechanisms',\n",
       "   'organization',\n",
       "   'badges',\n",
       "   'monetization',\n",
       "   'rules',\n",
       "   'anonymity',\n",
       "   'users',\n",
       "   'Content-related',\n",
       "   'AÔ¨ÄordancesMember-related',\n",
       "   'AÔ¨Äordances',\n",
       "   'Infrastructure-related',\n",
       "   'AÔ¨Äordances'],\n",
       "  'image_file': '2107.09008v2-Figure5-1.png',\n",
       "  'sections': [{'heading': 'Analyzing Clubhouse Using MIC',\n",
       "    'text': 'First, we will describe the state of Clubhouse as of June of 2021 (Figure 4). We then describe the state of Clubhouse as of the time of writing this manuscript, and accordingly update the MIC diagram and discuss how these changes could effect potential moderation challenges and strategies (Figure 5). Finally, we will discuss how using MIC allows us to reason about moderation strategies and challenges that exist on Clubhouse in a more efficient and systematic way, and what insights MIC provides that may otherwise be overlooked.\\nClubhouse in June 2021. As of June 2021, Clubhouse was invite-only, so new users must be invited to the app using their phone number (access). Users must use their real name, as per the platform\\'s community guidelines (anonymity).\\nClubhouse users can only communicate with one another using audio in public or private voice rooms (modalities).\\nClubhouse is organized into topic-specific pages and groups called \"clubs\" (organization); only \"the most active members of the Clubhouse Community\" can create clubs (access). Each such page and club is made up of synchronous and ephemeral voice rooms (synchronicity, ephemerality). Every club has designated admins that have the ability to edit the club settings, name, and manage members (users). Public voice rooms can be accessed by any user on the app, regardless of their membership in its associated club or interest in the room\\'s subject (access). Private rooms can only be joined by the followers of the room host or the members of the room\\'s associated club (if it exists) (access).\\nAll participants of rooms are required to follow Clubhouse\\'s Community Guidelines [2] (rules). However, established clubs can publish a list of club-specific rules that can be applied to participants of rooms hosted by the club (rules).\\nUsers can have one of three roles in a room on Clubhouse (users). The moderator role (denoted by a green star symbol) is given to the user who creates the room. This user has the ability to end the room, invite users to the stage  to speak, mute speakers, and assign other users to be moderators as well. This means that every active room (i.e., every instance that audio-content is generated on the app) has a \"moderator\" present (mechanisms). All other users that enter the room start out as listeners, and do not have the ability to speak in this role-they cannot unmute their microphone. As a listener, users can press the \"raise hand\" button and ask to be a speaker. If a moderator accepts a listener\\'s request to speak, that listener gets moved up to the \"stage\" where they now have the role of speaker. As a speaker, they can unmute their own microphone and be heard by everyone else in the room (access).\\nAll speakers inside a room have a marker to show whether their microphone is muted or not. Speakers often click this marker on and off to indicate that they want a turn to speak. When users enter a room, they have a celebratory emoji by their icon and name to indicate that they are new to the room (badges). Clubhouse also a monetization feature that lets users send money to other Clubhouse users via their profile page (monetization). Clubhouse uses a block-list icon to indicate to a user that a specific user has been blocked by many people in their circle (mechanisms, badges).\\nMuch of the commentary about Clubhouse interactions happen on other platforms. One such platform that is heavily used by Clubhouse users for commentary is Twitter. Users often talk about what they are experiencing on Clubhouse on Twitter, and Clubhouse users will often link to their Twitter profiles in the Clubhouse app. There are even subreddits dedicated to talking about Clubhouse (i.e., r/Clubhouse). These other platforms are also used to announce and publicize rooms or clubs and invite new users to Clubhouse (inter-platform).',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Harmonizing the Cacophony with MIC: An Affordance-aware Framework for Platform Moderation',\n",
       "  'abstract': 'Social platforms are evolving at a rapid pace. With the addition of new features like real-time audio, the landscape of online communities and moderation work on these communities is being out-paced by platform development. In this paper, we present a novel framework that allows us to represent the dynamic moderation ecosystems of social platforms using a base-set of 12 platform-level affordances, along with inter-affordance relationships. These affordances fall into the three categories-Members, Infrastructure, and Content. We call this the MIC framework, and apply MIC to analyze several social platforms in two case studies. First we analyze individual platforms using MIC and demonstrate how MIC can be used to examine the effects of platform changes on the moderation ecosystem and identify potential new challenges in moderation. Next, we systematically compare three platforms using MIC and propose potential moderation mechanisms that platforms can adapt from one another. Moderation researchers and platform designers can use such comparisons to uncover where platforms can emulate established, successful and better-studied platforms, as well as learn from the pitfalls other platforms have encountered. CCS Concepts: ‚Ä¢ Human-centered computing ‚Üí Social networking sites.',\n",
       "  'paddleOCR': [[[[657.0, 19.0],\n",
       "     [1065.0, 19.0],\n",
       "     [1065.0, 39.0],\n",
       "     [657.0, 39.0]],\n",
       "    ['Infrastructure-related Affordances', 0.9923291206359863]],\n",
       "   [[[409.0, 78.0], [481.0, 78.0], [481.0, 97.0], [409.0, 97.0]],\n",
       "    ['badges', 0.9976909756660461]],\n",
       "   [[[714.0, 78.0], [843.0, 78.0], [843.0, 97.0], [714.0, 97.0]],\n",
       "    ['monetization', 0.9605917930603027]],\n",
       "   [[[1018.0, 78.0], [1072.0, 78.0], [1072.0, 97.0], [1018.0, 97.0]],\n",
       "    ['rules', 0.9976080656051636]],\n",
       "   [[[424.0, 108.0], [661.0, 109.0], [661.0, 133.0], [424.0, 131.0]],\n",
       "    ['Moderator marker; new', 0.9903045892715454]],\n",
       "   [[[744.0, 114.0], [951.0, 112.0], [951.0, 136.0], [745.0, 137.0]],\n",
       "    ['App has feature that', 0.9999299049377441]],\n",
       "   [[[1037.0, 125.0], [1271.0, 125.0], [1271.0, 144.0], [1037.0, 144.0]],\n",
       "    ['Platform-level rules and', 0.9980750679969788]],\n",
       "   [[[451.0, 137.0], [633.0, 136.0], [634.0, 159.0], [451.0, 161.0]],\n",
       "    ['participant maker;', 0.9995499849319458]],\n",
       "   [[[746.0, 143.0], [951.0, 143.0], [951.0, 162.0], [746.0, 162.0]],\n",
       "    ['allows users to send', 0.9996242523193359]],\n",
       "   [[[1052.0, 149.0], [1256.0, 151.0], [1256.0, 175.0], [1052.0, 173.0]],\n",
       "    ['etiquette guidelines;', 0.9996379613876343]],\n",
       "   [[[455.0, 162.0], [631.0, 165.0], [631.0, 187.0], [455.0, 184.0]],\n",
       "    ['block list marker;', 0.999014139175415]],\n",
       "   [[[747.0, 169.0], [947.0, 166.0], [947.0, 190.0], [748.0, 193.0]],\n",
       "    ['money to \"popular\"', 0.9832137227058411]],\n",
       "   [[[1059.0, 176.0], [1249.0, 177.0], [1249.0, 201.0], [1059.0, 199.0]],\n",
       "    ['Club-specific rules', 0.9998363852500916]],\n",
       "   [[[442.0, 190.0], [642.0, 191.0], [642.0, 215.0], [442.0, 213.0]],\n",
       "    ['Microphone marker', 0.9937044978141785]],\n",
       "   [[[792.0, 198.0], [905.0, 198.0], [905.0, 217.0], [792.0, 217.0]],\n",
       "    ['room hosts', 0.9867825508117676]],\n",
       "   [[[408.0, 287.0], [542.0, 287.0], [542.0, 306.0], [408.0, 306.0]],\n",
       "    ['inter-platform', 0.9990273118019104]],\n",
       "   [[[713.0, 287.0], [840.0, 287.0], [840.0, 306.0], [713.0, 306.0]],\n",
       "    ['mechanisms', 0.99836665391922]],\n",
       "   [[[1020.0, 287.0], [1142.0, 287.0], [1142.0, 306.0], [1020.0, 306.0]],\n",
       "    ['organization', 0.9975581169128418]],\n",
       "   [[[423.0, 314.0], [661.0, 316.0], [661.0, 335.0], [423.0, 334.0]],\n",
       "    ['Some Clubs direct users', 0.9964005947113037]],\n",
       "   [[[733.0, 305.0], [964.0, 305.0], [964.0, 324.0], [733.0, 324.0]],\n",
       "    ['Every room has a moderator;', 0.9987027645111084]],\n",
       "   [[[1037.0, 310.0], [1271.0, 310.0], [1271.0, 330.0], [1037.0, 330.0]],\n",
       "    ['Live rooms are organized by', 0.9998480677604675]],\n",
       "   [[[751.0, 327.0], [947.0, 325.0], [947.0, 345.0], [752.0, 346.0]],\n",
       "    ['App keeps recordings of', 0.9998363256454468]],\n",
       "   [[[1082.0, 331.0], [1227.0, 332.0], [1227.0, 352.0], [1081.0, 350.0]],\n",
       "    ['clubs and topics;', 0.9995236396789551]],\n",
       "   [[[455.0, 341.0], [631.0, 341.0], [631.0, 360.0], [455.0, 360.0]],\n",
       "    ['to other platforms', 0.9917600154876709]],\n",
       "   [[[742.0, 348.0], [955.0, 346.0], [955.0, 365.0], [742.0, 367.0]],\n",
       "    ['rooms for a short period of', 0.9742352366447449]],\n",
       "   [[[1043.0, 354.0], [1267.0, 354.0], [1267.0, 374.0], [1043.0, 374.0]],\n",
       "    ['organization is largely open', 0.9952441453933716]],\n",
       "   [[[434.0, 367.0], [652.0, 367.0], [652.0, 386.0], [434.0, 386.0]],\n",
       "    ['(Instagram, Reddit) for', 0.9926692843437195]],\n",
       "   [[[753.0, 370.0], [947.0, 370.0], [947.0, 388.0], [753.0, 388.0]],\n",
       "    ['time to review in case of', 0.9904204607009888]],\n",
       "   [[[1043.0, 377.0], [1267.0, 377.0], [1267.0, 395.0], [1043.0, 395.0]],\n",
       "    ['and users can find random', 0.9998888373374939]],\n",
       "   [[[489.0, 390.0], [598.0, 390.0], [598.0, 410.0], [489.0, 410.0]],\n",
       "    ['text-based', 0.999736487865448]],\n",
       "   [[[750.0, 392.0], [947.0, 390.0], [947.0, 410.0], [750.0, 411.0]],\n",
       "    ['reported incident; Room', 0.9997895956039429]],\n",
       "   [[[1036.0, 399.0], [1274.0, 399.0], [1274.0, 418.0], [1036.0, 418.0]],\n",
       "    ['rooms if they appear on their', 0.9941164255142212]],\n",
       "   [[[467.0, 417.0], [620.0, 417.0], [620.0, 436.0], [467.0, 436.0]],\n",
       "    ['communication', 0.9993292689323425]],\n",
       "   [[[753.0, 414.0], [946.0, 414.0], [946.0, 432.0], [753.0, 432.0]],\n",
       "    ['moderators can record', 0.993524432182312]],\n",
       "   [[[1107.0, 421.0], [1201.0, 424.0], [1200.0, 443.0], [1106.0, 440.0]],\n",
       "    ['homepage', 0.9982892274856567]],\n",
       "   [[[821.0, 439.0], [876.0, 439.0], [876.0, 454.0], [821.0, 454.0]],\n",
       "    ['rooms', 0.9986084699630737]],\n",
       "   [[[71.0, 483.0], [323.0, 489.0], [322.0, 532.0], [70.0, 526.0]],\n",
       "    ['clubhouse', 0.9974421262741089]],\n",
       "   [[[110.0, 534.0], [262.0, 534.0], [262.0, 558.0], [110.0, 558.0]],\n",
       "    ['(January 2022)', 0.9987465143203735]],\n",
       "   [[[724.0, 544.0], [857.0, 547.0], [856.0, 571.0], [724.0, 567.0]],\n",
       "    ['synchronicity', 0.9993420243263245]],\n",
       "   [[[1032.0, 544.0], [1161.0, 547.0], [1160.0, 571.0], [1031.0, 567.0]],\n",
       "    ['ephemerality', 0.9993543028831482]],\n",
       "   [[[391.0, 555.0], [491.0, 555.0], [491.0, 575.0], [391.0, 575.0]],\n",
       "    ['anonymity', 0.9979884624481201]],\n",
       "   [[[1055.0, 604.0], [1270.0, 604.0], [1270.0, 623.0], [1055.0, 623.0]],\n",
       "    ['Ephemeral and Non-', 0.9561676383018494]],\n",
       "   [[[444.0, 615.0], [595.0, 615.0], [595.0, 634.0], [444.0, 634.0]],\n",
       "    ['Users must be', 0.9744490385055542]],\n",
       "   [[[792.0, 616.0], [930.0, 616.0], [930.0, 640.0], [792.0, 640.0]],\n",
       "    ['Synchronous', 0.9997541904449463]],\n",
       "   [[[1104.0, 630.0], [1224.0, 630.0], [1224.0, 653.0], [1104.0, 653.0]],\n",
       "    ['Ephemeral', 0.998891294002533]],\n",
       "   [[[464.0, 642.0], [578.0, 642.0], [578.0, 662.0], [464.0, 662.0]],\n",
       "    ['Identifiable', 0.9994304180145264]],\n",
       "   [[[392.0, 763.0], [449.0, 763.0], [449.0, 782.0], [392.0, 782.0]],\n",
       "    ['users', 0.9986962080001831]],\n",
       "   [[[725.0, 760.0], [796.0, 760.0], [796.0, 775.0], [725.0, 775.0]],\n",
       "    ['access', 0.996807336807251]],\n",
       "   [[[1033.0, 757.0], [1136.0, 757.0], [1136.0, 777.0], [1033.0, 777.0]],\n",
       "    ['modalities', 0.9987545013427734]],\n",
       "   [[[417.0, 779.0], [621.0, 781.0], [621.0, 804.0], [417.0, 803.0]],\n",
       "    ['Rooms: Moderator,', 0.9998146295547485]],\n",
       "   [[[782.0, 785.0], [940.0, 785.0], [940.0, 803.0], [782.0, 803.0]],\n",
       "    ['App is Invite Only;', 0.9867948293685913]],\n",
       "   [[[410.0, 811.0], [634.0, 811.0], [634.0, 831.0], [410.0, 831.0]],\n",
       "    ['Speaker, and Listener', 0.9994345903396606]],\n",
       "   [[[746.0, 807.0], [977.0, 807.0], [977.0, 827.0], [746.0, 827.0]],\n",
       "    ['Speaking in a room requires', 0.9996214509010315]],\n",
       "   [[[746.0, 829.0], [976.0, 829.0], [976.0, 849.0], [746.0, 849.0]],\n",
       "    ['permission from moderator;', 0.9924361705780029]],\n",
       "   [[[1087.0, 825.0], [1248.0, 825.0], [1248.0, 845.0], [1087.0, 845.0]],\n",
       "    ['Audio and Text', 0.9995124936103821]],\n",
       "   [[[756.0, 851.0], [968.0, 851.0], [968.0, 869.0], [756.0, 869.0]],\n",
       "    ['Club creation is limited to', 0.9816130995750427]],\n",
       "   [[[387.0, 864.0], [653.0, 865.0], [653.0, 889.0], [387.0, 887.0]],\n",
       "    ['Clubs: Admins, Members,', 0.9977312684059143]],\n",
       "   [[[808.0, 876.0], [910.0, 876.0], [910.0, 892.0], [808.0, 892.0]],\n",
       "    ['select users.', 0.9332621097564697]],\n",
       "   [[[473.0, 892.0], [567.0, 892.0], [567.0, 915.0], [473.0, 915.0]],\n",
       "    ['Leaders', 0.9951621890068054]],\n",
       "   [[[426.0, 947.0], [616.0, 947.0], [616.0, 966.0], [426.0, 966.0]],\n",
       "    ['Member-related', 0.9987627863883972]],\n",
       "   [[[864.0, 962.0], [1202.0, 962.0], [1202.0, 982.0], [864.0, 982.0]],\n",
       "    ['Content-related Affordances', 0.9860588312149048]],\n",
       "   [[[448.0, 973.0], [595.0, 976.0], [594.0, 1001.0], [448.0, 998.0]],\n",
       "    ['Affordances', 0.999485194683075]]],\n",
       "  'ocr': [[[821.0, 439.0], [876.0, 439.0], [876.0, 454.0], [821.0, 454.0]],\n",
       "   ['rooms', 0.9986084699630737]]},\n",
       " '2101.05996v1-Figure1-1.png': {'caption': 'Fig 1. The whole process of experiments',\n",
       "  'imageText': [],\n",
       "  'image_file': '2101.05996v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Method',\n",
       "    'text': 'Fig. 1 shows the process of experiments: MNIST data is loaded as training and testing sets to building CNN models. Next go into the training and evaluation iteration for the CNN model built in the previous step. Modification of connected layer size are needed to explore the effects of the CNN fully connected layer size on the recognition accuracy. The evaluation results for different hidden size are recorded. Pruning method would be performed on the best trained CNN model. Finally, use evaluation approaches to test the performance of pruning. ',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Convolutional Neural Network with Pruning Method for Handwritten Digit Recognition',\n",
       "  'abstract': 'CNN model is a popular method for imagery analysis, so it could be utilized to recognize handwritten digits based on MNIST datasets. For higher recognition accuracy, various CNN models with different fully connected layer sizes are exploited to figure out the relationship between the CNN fully connected layer size and the recognition accuracy. Inspired by previous pruning work, we performed pruning methods of distinctiveness on CNN models and compared the pruning performance with NN models. For better pruning performances on CNN, the effect of angle threshold on the pruning performance was explored. The evaluation results show that: for the fully connected layer size, there is a threshold, so that when the layer size increases, the recognition accuracy grows if the layer size smaller than the threshold, and falls if the layer size larger than the threshold; the performance of pruning performed on CNN is worse than on NN; as pruning angle threshold increases, the fully connected layer size and the recognition accuracy decreases. This paper also shows that for CNN models trained by the MNIST dataset, they are capable of handwritten digit recognition and achieve the highest recognition accuracy with fully connected layer size 400. In addition, for same dataset MNIST, CNN models work better than big, deep, simple NN models in a published paper.',\n",
       "  'paddleOCR': [[[[297.0, 51.0], [406.0, 51.0], [406.0, 83.0], [297.0, 83.0]],\n",
       "    ['Create', 0.9999459385871887]],\n",
       "   [[[538.0, 49.0], [670.0, 55.0], [668.0, 92.0], [536.0, 86.0]],\n",
       "    ['Training', 0.9998977184295654]],\n",
       "   [[[259.0, 95.0], [440.0, 92.0], [440.0, 124.0], [260.0, 127.0]],\n",
       "    ['customized', 0.9997655749320984]],\n",
       "   [[[566.0, 95.0], [633.0, 95.0], [633.0, 129.0], [566.0, 129.0]],\n",
       "    ['and', 0.9995406270027161]],\n",
       "   [[[20.0, 111.0], [182.0, 111.0], [182.0, 142.0], [20.0, 142.0]],\n",
       "    ['Load data', 0.9958609342575073]],\n",
       "   [[[795.0, 110.0], [924.0, 117.0], [923.0, 153.0], [794.0, 147.0]],\n",
       "    ['Pruning', 0.9997856020927429]],\n",
       "   [[[1017.0, 115.0], [1191.0, 118.0], [1191.0, 149.0], [1017.0, 146.0]],\n",
       "    ['Evaluation', 0.9993734359741211]],\n",
       "   [[[311.0, 134.0], [392.0, 134.0], [392.0, 168.0], [311.0, 168.0]],\n",
       "    ['CNN', 0.9992682933807373]],\n",
       "   [[[520.0, 135.0], [690.0, 138.0], [689.0, 170.0], [519.0, 167.0]],\n",
       "    ['Evaluation', 0.9995657205581665]],\n",
       "   [[[300.0, 178.0], [401.0, 174.0], [402.0, 207.0], [301.0, 210.0]],\n",
       "    ['model', 0.9994635581970215]],\n",
       "   [[[383.0, 309.0], [582.0, 312.0], [582.0, 347.0], [383.0, 344.0]],\n",
       "    ['Change fully', 0.9999058842658997]],\n",
       "   [[[398.0, 355.0], [568.0, 352.0], [569.0, 383.0], [399.0, 386.0]],\n",
       "    ['connected', 0.9997676014900208]],\n",
       "   [[[393.0, 394.0], [575.0, 394.0], [575.0, 425.0], [393.0, 425.0]],\n",
       "    ['hidden size', 0.9999076724052429]]],\n",
       "  'ocr': [[[259.0, 95.0], [440.0, 92.0], [440.0, 124.0], [260.0, 127.0]],\n",
       "   ['customized', 0.9997655749320984]]},\n",
       " '2202.11039v1-Figure6-1.png': {'caption': 'Figure 6. A block diagram of the horn antenna characterization setup is shown on the left. The spectrum analyzer output after tuning the antenna at 1420MHz frequency is shown on the right. The figure on the top shows the laboratory setup depicting the arrangement of the horn antenna, directional coupler and spectrum analyzer.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2202.11039v1-Figure6-1.png',\n",
       "  'sections': [{'heading': 'Horn Antenna',\n",
       "    'text': 'Horn-antenna couples the electromagnetic radio emission to the electrical circuit. We used a custom made single polarisation pyramidal horn antenna having 30 degrees of beamwidth. The antennas feed length and the back-shot positions can be adjusted to operate it over a narrow range of frequencies.\\nThe horn is tuned for an optimal performance at 1420 MHz using simple laboratory tools 1 . The arrangement consisted of a directional coupler and a frequency generator as shown in figure 6. One port of the coupler was connected to the spectrometer to measure the reflected signal power from the Antenna. The other port was connected to the antenna. The third port of the coupler was fed with radio frequency tones around 1420 MHz. Frequency from a signal generator. The horn antenna and one of the bandpass filters (BPF2) are custom designed for this work. The outputs from the RF receiver chain feeds to a software-defined radio (SDR) module.\\nOptimal response of the antenna is achieved when a characteristic dip in the spectrum appears as seen in the spectrum analyzer display shown in figure 6 lower right side, which corresponds to the sensitive reception band of the horn antenna. This tuning required adjusting the horn back-shot position and varying the feed probe length.\\nSince, we used an existing horn of smaller aperture available from the laboratory and extended its flare portion to suit the 21 cm observations. It was made of aluminum, having a back-shot, feed mount, and a flare with a dimension of \"a b c\" and \"g f h\" as shown in the figure7. This flare portion was extended in the H-plane to the dimensions \"a-d-e\" and in the E-plane to the extent \"f-i-j\" to achieve the desired higher gain.  Dimension for the flare\\'s extension was calculated by expanding angles of H-plane and E-plane about 72.5 for the H-plane and 71.0 for the E-plane. Thus the dimensions extended to give a flare exit-width of 70 cm in H-plane and 60 cm in E-plane. Cardboard was used as a base material in the extended region with an aluminum foil cover on the top for conduction. A 10-micron thick aluminum foil was used to provide sufficient skin-depth needed (about 2.3 microns) for the 1420 MHz signal as shown in figure 8. We estimate the gain of the horn at 1420 MHz after the flare-extension as 13.3 dBi, which very closely matched the CST ¬Æ software simulation results presented in figure 9. Gain pattern,  ',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 4}],\n",
       "  'title': 'Galaxy Rotation Curve Measurements with Low Cost 21 cm Radio Telescope',\n",
       "  'abstract': 'Probing the Universe with atomic hydrogen 21 cm emission is a fascinating and challenging work in astronomy. Radio telescopes play a vital role in detecting and imaging these faint signals. Powerful radio telescopes are complex to construct and operate. We have built a simple, low-cost 21 cm radio telescope primarily for educational training purposes. The design uses a custom horn antenna, ready-to-use radio-frequency components, and a software-defined radio module. The telescope operates efficiently from a rooftop in a city environment. Using this telescope, we have conducted observations and successfully detected the 21 cm line emissions from the different directions of our galactic plane. Based on the Doppler-shift observed in these measurements, we have successfully derived the Galactic rotation velocity (rotation curve) in those directions. The paper presents the details of the telescope construction, 21 cm observation, and the Galactic rotation curve derivation.',\n",
       "  'paddleOCR': [[[[103.0, 54.0], [166.0, 54.0], [166.0, 73.0], [103.0, 73.0]],\n",
       "    ['HORN', 0.9987995624542236]],\n",
       "   [[[83.0, 79.0], [183.0, 79.0], [183.0, 98.0], [83.0, 98.0]],\n",
       "    ['ANTENNA', 0.996788501739502]],\n",
       "   [[[516.0, 301.0], [894.0, 301.0], [894.0, 318.0], [516.0, 318.0]],\n",
       "    ['MEASUREMENTSETUP IN THE LAB', 0.9374207258224487]],\n",
       "   [[[261.0, 358.0], [348.0, 358.0], [348.0, 383.0], [261.0, 383.0]],\n",
       "    ['Coupled', 0.9999245405197144]],\n",
       "   [[[585.0, 365.0], [817.0, 365.0], [817.0, 383.0], [585.0, 383.0]],\n",
       "    ['SPECTRUM ANALYSER', 0.9807378053665161]],\n",
       "   [[[274.0, 384.0], [335.0, 384.0], [335.0, 406.0], [274.0, 406.0]],\n",
       "    ['PORT', 0.9989141821861267]],\n",
       "   [[[60.0, 406.0], [207.0, 406.0], [207.0, 424.0], [60.0, 424.0]],\n",
       "    ['DIRECTIONAL', 0.9987621307373047]],\n",
       "   [[[80.0, 429.0], [190.0, 431.0], [189.0, 451.0], [80.0, 449.0]],\n",
       "    ['COUPLER', 0.9973068833351135]],\n",
       "   [[[901.0, 430.0], [940.0, 432.0], [939.0, 444.0], [900.0, 441.0]],\n",
       "    ['Spectrum', 0.9903097748756409]],\n",
       "   [[[906.0, 440.0], [934.0, 443.0], [932.0, 454.0], [904.0, 452.0]],\n",
       "    ['Traces', 0.9850226044654846]],\n",
       "   [[[701.0, 457.0], [712.0, 457.0], [712.0, 465.0], [701.0, 465.0]],\n",
       "    ['2.', 0.8134804964065552]],\n",
       "   [[[904.0, 460.0], [931.0, 460.0], [931.0, 472.0], [904.0, 472.0]],\n",
       "    ['Norma', 0.9388839602470398]],\n",
       "   [[[927.0, 481.0], [943.0, 481.0], [943.0, 494.0], [927.0, 494.0]],\n",
       "    ['Off', 0.8101561665534973]],\n",
       "   [[[897.0, 500.0], [928.0, 502.0], [927.0, 514.0], [896.0, 511.0]],\n",
       "    ['Average', 0.9683685898780823]],\n",
       "   [[[884.0, 527.0], [893.0, 527.0], [893.0, 536.0], [884.0, 536.0]],\n",
       "    ['On', 0.9950209259986877]],\n",
       "   [[[920.0, 523.0], [938.0, 523.0], [938.0, 542.0], [920.0, 542.0]],\n",
       "    ['off', 0.8388432860374451]],\n",
       "   [[[887.0, 548.0], [927.0, 548.0], [927.0, 563.0], [887.0, 563.0]],\n",
       "    ['Max Hold', 0.9901114702224731]],\n",
       "   [[[878.0, 566.0], [891.0, 566.0], [891.0, 583.0], [878.0, 583.0]],\n",
       "    ['On', 0.9506256580352783]],\n",
       "   [[[915.0, 569.0], [932.0, 569.0], [932.0, 582.0], [915.0, 582.0]],\n",
       "    ['off', 0.8772966861724854]],\n",
       "   [[[883.0, 590.0], [922.0, 590.0], [922.0, 604.0], [883.0, 604.0]],\n",
       "    ['Min Hold', 0.9939585328102112]],\n",
       "   [[[911.0, 610.0], [927.0, 610.0], [927.0, 624.0], [911.0, 624.0]],\n",
       "    ['ofr', 0.8095731735229492]],\n",
       "   [[[94.0, 632.0], [184.0, 632.0], [184.0, 654.0], [94.0, 654.0]],\n",
       "    ['SIGNAL', 0.9979230761528015]],\n",
       "   [[[886.0, 633.0], [907.0, 633.0], [907.0, 644.0], [886.0, 644.0]],\n",
       "    ['Reset', 0.9957584142684937]],\n",
       "   [[[620.0, 654.0], [645.0, 654.0], [645.0, 661.0], [620.0, 661.0]],\n",
       "    ['NP/9', 0.8271485567092896]],\n",
       "   [[[666.0, 654.0], [735.0, 654.0], [735.0, 664.0], [666.0, 664.0]],\n",
       "    ['1.42000000GH', 0.9776110649108887]],\n",
       "   [[[885.0, 649.0], [907.0, 652.0], [906.0, 663.0], [883.0, 660.0]],\n",
       "    ['Traces', 0.9206718802452087]],\n",
       "   [[[58.0, 662.0], [213.0, 662.0], [213.0, 683.0], [58.0, 683.0]],\n",
       "    ['GENERATOR', 0.9984618425369263]],\n",
       "   [[[465.0, 666.0], [498.0, 666.0], [498.0, 673.0], [465.0, 673.0]],\n",
       "    ['pectrum', 0.9704181551933289]],\n",
       "   [[[599.0, 665.0], [640.0, 665.0], [640.0, 690.0], [599.0, 690.0]],\n",
       "    ['Spectrum', 0.8320090770721436]],\n",
       "   [[[671.0, 666.0], [690.0, 666.0], [690.0, 673.0], [671.0, 673.0]],\n",
       "    ['Detec', 0.8719733357429504]],\n",
       "   [[[803.0, 659.0], [830.0, 659.0], [830.0, 666.0], [803.0, 666.0]],\n",
       "    ['.OOMH', 0.8805891871452332]],\n",
       "   [[[676.0, 684.0], [694.0, 684.0], [694.0, 691.0], [676.0, 691.0]],\n",
       "    ['Aut', 0.9648196697235107]]],\n",
       "  'ocr': [[[883.0, 590.0], [922.0, 590.0], [922.0, 604.0], [883.0, 604.0]],\n",
       "   ['Min Hold', 0.9939585328102112]]},\n",
       " '2102.02282v1-Figure3-1.png': {'caption': 'Figure 3. A global view of the neural network. The first group of layers are regular convolutional layers and act as onset detectors. They have a small receptive field, in order to focus on acoustic features and avoid learning rhythmic patterns, which will be learned by the successive tempoinvariant layers. The output tensor represents joint probabilities of downbeat presence D and tempo œÑ .',\n",
       "  'imageText': ['output',\n",
       "   'input',\n",
       "   'Time',\n",
       "   'p(¬¨D)',\n",
       "   'p(D,',\n",
       "   'œÑ)',\n",
       "   'Receptive',\n",
       "   'Ô¨Åeld',\n",
       "   'Tempo',\n",
       "   'Invariant',\n",
       "   'Convolution',\n",
       "   'Layers',\n",
       "   'Convolution',\n",
       "   'Layers'],\n",
       "  'image_file': '2102.02282v1-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Network',\n",
       "    'text': 'The tempo-invariant network (Fig. 3) is a fully convolutional deep neural network, where the layers are conceptually divided into two groups. The first group of layers are regular one-dimensional convolutional layers and act as onset detectors. The receptive field is constrained in order to preserve the tempo-invariance property of the model: if even short rhythmic fragments are learned at a specific tempo, the invariance assumption would be violated. We limit the maximum size of the receptive field to 0.25 seconds, i.e. the period of a beat at 240 BPM. They have a small receptive field, in order to focus on acoustic features and avoid learning rhythmic patterns, which will be learned by the successive tempoinvariant layers. The output tensor represents joint probabilities of downbeat presence D and tempo œÑ .\\nThe second group is a stack of tempo-invariant convolutional layers (as described in Sec. 2.1, 2.2). The receptive field is measured in musical-time, with each layer spanning B = 4 beats. The last layer outputs only one channel, producing a 2-dimensional (frame and tempo) output tensor.\\nThe activations of the last layer represent the scores (logits) of having a downbeat at a specific tempo. An additional constant zero bin 1 is concatenated to these activations for each frame to model the score of having no downbeat. After applying the softmax, the output o represents the joint probability of the downbeat presence D at a specific tempo œÑ o j = p(D, œÑ j ) j = 0, . . . , S ‚àí 1\\np(¬¨D) j = S(8)\\nThe categorical cross-entropy loss is then applied framewise, with a weighting scheme that balances the loss contribution on downbeat versus non-downbeat frames. 2 The target tensors are generated from the downbeat annotations by spreading the downbeat locations to the neighbouring time frames and tempi using a rectangular window (0.1 seconds wide) for time and a raised cosine window (2/T octaves wide) for tempo. The network is trained with stochastic gradient descent using RMSprop, early stopping and learning rate reduction when the validation loss reaches a plateau.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'DOWNBEAT TRACKING WITH TEMPO-INVARIANT CONVOLUTIONAL NEURAL NETWORKS',\n",
       "  'abstract': 'The human ability to track musical downbeats is robust to changes in tempo, and it extends to tempi never previously encountered. We propose a deterministic time-warping operation that enables this skill in a convolutional neural network (CNN) by allowing the network to learn rhythmic patterns independently of tempo. Unlike conventional deep learning approaches, which learn rhythmic patterns at the tempi present in the training dataset, the patterns learned in our model are tempo-invariant, leading to better tempo generalisation and more efficient usage of the network capacity. We test the generalisation property on a synthetic dataset created by rendering the Groove MIDI Dataset using FluidSynth, split into a training set containing the original performances and a test set containing temposcaled versions rendered with different SoundFonts (testtime augmentation). The proposed model generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both training and test sets), whereas a comparable conventional CNN achieves similar accuracy only for the training set (0.89) and drops to 0.54 on the test set. The generalisation advantage of the proposed model extends to real music, as shown by results on the GTZAN and Ballroom datasets.',\n",
       "  'paddleOCR': [[[[307.0, 27.0], [378.0, 27.0], [378.0, 61.0], [307.0, 61.0]],\n",
       "    ['input', 0.9905619621276855]],\n",
       "   [[[2.0, 105.0], [184.0, 103.0], [185.0, 130.0], [2.0, 133.0]],\n",
       "    ['Receptive field', 0.9983223676681519]],\n",
       "   [[[224.0, 165.0], [462.0, 167.0], [462.0, 194.0], [224.0, 192.0]],\n",
       "    ['Convolution Layers', 0.9993119835853577]],\n",
       "   [[[121.0, 298.0], [564.0, 298.0], [564.0, 322.0], [121.0, 322.0]],\n",
       "    ['Tempo Invariant Convolution Layers', 0.9997188448905945]],\n",
       "   [[[302.0, 458.0], [390.0, 458.0], [390.0, 487.0], [302.0, 487.0]],\n",
       "    ['output', 0.9555408358573914]],\n",
       "   [[[46.0, 539.0], [114.0, 543.0], [112.0, 564.0], [45.0, 564.0]],\n",
       "    ['Time', 0.9985058307647705]]],\n",
       "  'ocr': [[[46.0, 539.0], [114.0, 543.0], [112.0, 564.0], [45.0, 564.0]],\n",
       "   ['Time', 0.9985058307647705]]},\n",
       " '2204.03140v1-Figure4-1.png': {'caption': 'Fig. 4: Illustration of the network structure. The camera image and map projection image are sent to the encoders in parallel and then aggregated together to obtain the state value function. Note that state value function at each time step is a scalar thus the output size of the network is 1.',\n",
       "  'imageText': ['MLP',\n",
       "   'Layers',\n",
       "   'State',\n",
       "   'Value',\n",
       "   'Function',\n",
       "   'V(s)',\n",
       "   'Map',\n",
       "   'Projection',\n",
       "   'Embedding',\n",
       "   'Camera',\n",
       "   'Image',\n",
       "   'Embedding',\n",
       "   'Front-view',\n",
       "   'RGB',\n",
       "   'Image',\n",
       "   'Project',\n",
       "   'Voxels',\n",
       "   'Downward',\n",
       "   'Joint',\n",
       "   'State',\n",
       "   'Embedding',\n",
       "   'Œ¶(s)Robot‚Äôs',\n",
       "   'Location',\n",
       "   'Map',\n",
       "   'Projection',\n",
       "   'Encoder',\n",
       "   'Camera',\n",
       "   'Image',\n",
       "   'Encoder'],\n",
       "  'image_file': '2204.03140v1-Figure4-1.png',\n",
       "  'sections': [{'heading': 'BV (s',\n",
       "    'text': 't ) = R(t) + Œ≥E œÄ,p [V (s t+1 )](8)\\nThus, we use semi-gradient TD(0) in testing stage to further update the parameters Œ∏ of the value function approximator. Thus the parameters updating in testing is shown as the following equations,\\nŒ∏ = Œ∏ + Œ∑ R(t) + Œ≥V (œÜ(st+1), Œ∏) ‚àíV (œÜ(st), Œ∏) ‚àá Œ∏V (œÜ(st), Œ∏) = Œ∏ + Œ∑ BV (œÜ(st), Œ∏) ‚àíV (œÜ(st), Œ∏) ‚àá Œ∏V (œÜ(st), Œ∏)(9)\\nwhere s t+1 denotes the state after executing the action a t from the exploration policy œÄ(a t |s t ). The state transition is governed by a unknown state transition model p(s t+1 |s t , a t ).\\n2) Dealing with over-estimation: Value function estimation, either state value function or action value function, are susceptible to the problem of overestimation due to distribution shift and function approximation errors [12] [10], hence the estimated value function are expected to be larger than the real value function,\\nE[V (œÜ(s), Œ∏)] ‚â• E[V œÄ (s)], ‚àÄs ‚àà S (10)\\nWe also observe a fairly extent of over-estimation of value function. Thus inspired by the solution in [12], we propose to train several copies of value function networks and use the minimum prediction from these networks. The weights of these networks are denoted as\\nŒ∏ 1 , Œ∏ 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , Œ∏ N V ‚àí1 . N V\\ndenotes the number of value function. In our approach, we choose N V = 2. All of these copies of value network will be trained offline and learn online in the same way as shown in Equation 7and 9. During online learning and testing, the estimated value function is the minimum value obtained from each these value networks:\\nV (œÜ(s), Œ∏) = min Œ∏iV (œÜ(s), Œ∏ i )(11)\\n3) Value network structure: Next we introduce more details of the function approximator. We use two encoders to encode the features of the camera image and 2D projected image. The encoded features are concatenated and then passed to a Multi-layer Perceptron (MLP) layers to get the final state value function prediction. For the sake of less computational burden, we apply MobileNet-V3-Small [26] model for both camera image encoder and map state image encoder. The network structure as well as the generation of state representation is illustrated as in Fig. 4.  ',\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': '4) Overall algorithm:',\n",
       "    'text': 'After going through the major procedures of offline training and online learning, we present the overall algorithm illustration depicted in Fig. 5. Our value function learning algorithm consists of two major parts. The first one is the offline MC training described in Equation 7. The second step is online TD learning described as in Equation 9. The value function is trained and then online-learned to provide a feedback regarding how valuable the current state is for the robot.  First we collect datasets which are then processed to get the camera image and projected map image. Then we feed this data to the function approximator described in Fig. 4 and perform offline MC learning. After offline training, the network weights are ready to deploy for online value function estimation. However we perform one addition online TD learn step and get the final value function for the robot to determine how good the current state is.\\nThe full value function approximation algorithms are then presented as in Algorithm 1 and 2. Algorithm 1 describes offline training with MC and Algorithm 2 describes online TD learning and testing. Please note that the notations used in these two algorithms are slightly different, e.g. network weights Œ∏ for training and Œ∏ for testing, dataset D tr and D te , etc. Here we use double value network and in online learning phase, we use the minimum value as the final estimated value function. \\nD tr = {Œæ 1 , Œæ 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , Œæ M } Output: Learned value function: V (s, Œ∏ 1 ),V (s, Œ∏ 2 ), ‚àÄs ‚àà S 1 Initialize value networks weights Œ∏ 1 , Œ∏ 2 2 for each training epoch do 3 for Œæ i ‚àà D do 4 T = length of Œæ i 5 for t = 0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , T ‚àí 1 do 6\\nCompute return as:\\nG t = T i=0 Œ≥ i R(t + i + 1) 7\\nUpdate parameters as :  Receive reward R(t) following œÄ(a t |s t )\\n8 Œ∏ i = Œ∏ i + Œ∑ G t ‚àíV (œÜ(s t ), Œ∏ i ) ‚àá Œ∏iV (œÜ(s t ), Œ∏ i ) 9 ‚àÄi ‚àà {1, 2}\\nŒ∏ 1 = Œ∏ 1 , Œ∏ 2 = Œ∏ 2 2 T = length of Œæ 3 for t = 0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , T ‚àí 1 do 4 Receive state observation o s (t)\\n6\\nTransit to next state s t+1 following œÄ and p(s t+1 |s t , a t )\\n7\\nOnline update parameters as:\\n8 Œ∏ i = Œ∏ i + Œ∑ BV (œÜ(s t ), Œ∏ i ) ‚àíV (œÜ(s t ), Œ∏ i ) ‚àá Œ∏ iV (œÜ(s t ), Œ∏ i ) 9 ‚àÄi ‚àà {1, 2} 10V (œÜ(s t ), Œ∏ i ) = argmin Œ∏ iV (œÜ(s), Œ∏ i ) V. EXPERIMENTS\\nIn this section, we first introduce the data collection, including the drone platform we used to collect the data, as well as the environments where data is collected. Then we present the experiment results, in both qualitative and quantitative forms.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Learning and Transferring Value Function for Robot Exploration in Subterranean Environments',\n",
       "  'abstract': 'In traditional robot exploration methods, the robot usually does not have prior biases about the environment it is exploring. Thus the robot assigns equal importance to the goals which leads to insufficient exploration efficiency. Alternative, often a hand-tuned policy is used to tweak the value of goals. In this paper, we present a method to learn how \"good\" some states are, measured by the state value function, to provide a hint for the robot to make exploration decisions. We propose to learn state value functions from previous offline collected datasets and then transfer and improve the value function during testing in a new environment. Moreover, the environments usually have very few and even no extrinsic reward or feedback for the robot. Therefore in this work, we also tackle the problem of sparse extrinsic rewards from the environments. We design several intrinsic rewards to encourage the robot to obtain more information during exploration. These reward functions then become the building blocks of the state value functions. We test our method on challenging subterranean and urban environments. To the best of our knowledge, this work for the first time demonstrates value function prediction with previous collected datasets to help exploration in challenging subterranean environments.',\n",
       "  'paddleOCR': [[[[418.0, 50.0], [549.0, 53.0], [549.0, 74.0], [417.0, 72.0]],\n",
       "    ['Camera Image', 0.9896665215492249]],\n",
       "   [[[49.0, 79.0], [139.0, 81.0], [138.0, 100.0], [49.0, 98.0]],\n",
       "    ['Front-view', 0.9988963007926941]],\n",
       "   [[[446.0, 75.0], [522.0, 77.0], [521.0, 96.0], [446.0, 94.0]],\n",
       "    ['Encoder', 0.9986138939857483]],\n",
       "   [[[44.0, 101.0], [142.0, 105.0], [141.0, 124.0], [43.0, 120.0]],\n",
       "    ['RGB Image', 0.9339768886566162]],\n",
       "   [[[578.0, 120.0], [678.0, 124.0], [678.0, 142.0], [578.0, 138.0]],\n",
       "    ['Camera Image', 0.9840538501739502]],\n",
       "   [[[589.0, 138.0], [667.0, 142.0], [666.0, 159.0], [588.0, 156.0]],\n",
       "    ['Embedding', 0.9988933205604553]],\n",
       "   [[[75.0, 249.0], [119.0, 249.0], [119.0, 265.0], [75.0, 265.0]],\n",
       "    ['SE', 0.6056305766105652]],\n",
       "   [[[544.0, 241.0], [639.0, 244.0], [638.0, 263.0], [544.0, 260.0]],\n",
       "    ['Joint State', 0.9874927401542664]],\n",
       "   [[[143.0, 261.0], [277.0, 265.0], [276.0, 283.0], [142.0, 279.0]],\n",
       "    [\"Robot's Location\", 0.9551527500152588]],\n",
       "   [[[520.0, 265.0], [662.0, 268.0], [662.0, 289.0], [519.0, 286.0]],\n",
       "    ['Embedding (s)', 0.9666645526885986]],\n",
       "   [[[958.0, 272.0], [993.0, 272.0], [993.0, 292.0], [958.0, 292.0]],\n",
       "    ['V(s)', 0.9985756278038025]],\n",
       "   [[[925.0, 325.0], [1016.0, 327.0], [1015.0, 345.0], [925.0, 343.0]],\n",
       "    ['State Value', 0.9569130539894104]],\n",
       "   [[[935.0, 345.0], [1005.0, 347.0], [1004.0, 366.0], [934.0, 364.0]],\n",
       "    ['Function', 0.9982123374938965]],\n",
       "   [[[776.0, 360.0], [870.0, 360.0], [870.0, 378.0], [776.0, 378.0]],\n",
       "    ['MLP Layers', 0.998683750629425]],\n",
       "   [[[28.0, 392.0], [145.0, 392.0], [145.0, 410.0], [28.0, 410.0]],\n",
       "    ['Project Voxels', 0.9896608591079712]],\n",
       "   [[[41.0, 414.0], [131.0, 417.0], [131.0, 435.0], [40.0, 432.0]],\n",
       "    ['Downward', 0.9990856051445007]],\n",
       "   [[[424.0, 429.0], [558.0, 431.0], [558.0, 453.0], [424.0, 450.0]],\n",
       "    ['Map Projection', 0.9950161576271057]],\n",
       "   [[[455.0, 456.0], [528.0, 456.0], [528.0, 474.0], [455.0, 474.0]],\n",
       "    ['Encoder', 0.998048722743988]],\n",
       "   [[[588.0, 498.0], [689.0, 499.0], [689.0, 515.0], [588.0, 514.0]],\n",
       "    ['Map Projection', 0.9844139218330383]],\n",
       "   [[[599.0, 515.0], [678.0, 517.0], [678.0, 535.0], [598.0, 532.0]],\n",
       "    ['Embedding', 0.9986881613731384]]],\n",
       "  'ocr': [[[28.0, 392.0], [145.0, 392.0], [145.0, 410.0], [28.0, 410.0]],\n",
       "   ['Project Voxels', 0.9896608591079712]]},\n",
       " '818973-Figure1-1.png': {'caption': 'Figure 1. DCN overview. Our model applies the coarse layers on the whole image to get fc(x), chooses a set of salient patches Xs, applies the fine layers only on the salient patches Xs to obtain a set of few fine representation vectors ff (Xs), and finally combines them to make its prediction.',\n",
       "  'imageText': [],\n",
       "  'image_file': '818973-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Dynamic Capacity Networks',\n",
       "    'text': 'In this section, we describe the Dynamic Capacity Network (DCN) that dynamically distributes its network capacity across an input.\\nWe consider a deep neural network h, which we decompose into two parts: h(x) = g(f (x)) where f and g represent respectively the bottom layers and top layers of the network h while x is some input data. Bottom layers f operate directly on the input and output a representation, which is composed of a collection of vectors each of which represents a region in the input. For example, f can output a feature map, i.e. vectors of features each with a specific spatial location, or a probability map outputting probability distributions at each different spatial location. Top layers g consider as input the bottom layers\\' representations f (x) and output a distribution over labels. DCN introduces the use of two alternative sub-networks for the bottom layers f : the coarse layers f c or the fine Figure 1. DCN overview. Our model applies the coarse layers on the whole image to get fc(x), chooses a set of salient patches X s , applies the fine layers only on the salient patches X s to obtain a set of few fine representation vectors f f (X s ), and finally combines them to make its prediction. layers f f , which differ in their capacity. The fine layers correspond to a high-capacity sub-network which has a high-computational requirement, while the coarse layers constitute a low-capacity sub-network. Consider applying the top layers only on the fine representation, i.e. h f (x) = g(f f (x)). We refer to the composition h f = g ‚Ä¢ f f as the fine model. We assume that the fine model can achieve very good performance, but is computationally expensive. Alternatively, consider applying the top layers only on the coarse representation, i.e. h c (x) = g(f c (x)). We refer to this composition h c = g ‚Ä¢ f c as the coarse model. Conceptually, the coarse model can be much more computationally efficient, but is expected to have worse performance than the fine model.\\nThe key idea behind DCN is to have g use representations from either the coarse or fine layers in an adaptive, dynamic way. Specifically, we apply the coarse layers f c on the whole input x, and leverage the fine layers f f only at a few \"important\" input regions. This way, the DCN can leverage the capacity of f f , but at a lower computational cost, by applying the fine layers only on a small portion of the input. To achieve this, DCN requires an attentional mechanism, whose task is to identify good input locations on which to apply f f . In the remainder of this section, we focus on 2-dimensional inputs. However, our DCN model can be easily extended to be applied to any type of N-dimensional data.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'End-to-End Training',\n",
       "    'text': 'In this section, we describe an end-to-end procedure for training the DCN model that leverages our attention mechanism to learn f f and f c jointly. We emphasize, however, that DCN modules can be trained independently, by training a coarse and a fine model independently and combining them only at test-time using our attention based inference. In Section 4.2 we show an example of how this modular training can be used for transfer learning.\\nIn the context of image classification, suppose we have a training set D = {(x (i) , y (i) ); i = 1 . . . m}, where each x (i) ‚àà R h√ów is an image, and y (i) ‚àà {1, . . . , C} is its corresponding label. We denote the parameters of the coarse, fine and top layers by Œ∏ c , Œ∏ f , and Œ∏ t respectively. We learn all of these parameters (denoted as Œ∏) by minimizing the cross-entropy objective function (which is equivalent to maximizing the log-likelihood of the correct labels):\\nJ = ‚àí m i=1 log p y (i) | x (i) ; Œ∏ ,(5)\\nwhere p(\\n‚Ä¢ | x (i) ; Œ∏) = g(f r (x (i)\\n)) is the conditional multinomial distribution defined over the C labels given by the refined model (Figure 1). Gradients are computed by standard back-propagation through the refined model, i.e. propagating gradients at each position into either the coarse or fine features, depending on which was used.\\nAn important aspect of the DCN model is that the final prediction is based on combining representations from two different sets of layers, namely the coarse layers f c and the fine layers f f . Intuitively, we would like those representations to have close values such that they can be interchangeable. This is important for two reasons. First, we expect the top layers to have more success in correctly classifying the input if the transition from coarse to fine representations is smooth. The second is that, since the saliency map is based on the gradient at the coarse representation values and since the gradient is a local measure of variation, it is less likely to reflect the benefit of using the fine features if the latter is very different from the former.\\nTo encourage similarity between the coarse and fine representations while training, we use a hint-based training approach inspired by Romero et al. (2014). Specifically, we add an additional term to the training objective that minimizes the squared distance between coarse and fine representations:\\nxi,j ‚ààX s\\nf c (x i,j ) ‚àí f f (x i,j ) 2 2 . (6\\n)\\nThere are two important points to note here. First, we use this term to optimize only the coarse layers Œ∏ c . That is, we encourage the coarse layers to mimic the fine ones, and let the fine layers focus only on the signal coming from the top layers. Secondly, computing the above hint objective over representations at all positions would be as expensive as computing the full fine model; therefore, we encourage in this term similarity only over the selected salient patches.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Dynamic Capacity Networks',\n",
       "  'abstract': \"We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity subnetworks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN's output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.\",\n",
       "  'paddleOCR': [[[[809.0, 147.0],\n",
       "     [838.0, 147.0],\n",
       "     [838.0, 234.0],\n",
       "     [809.0, 234.0]],\n",
       "    ['Labels', 0.9998896718025208]],\n",
       "   [[[675.0, 175.0], [709.0, 175.0], [709.0, 199.0], [675.0, 199.0]],\n",
       "    ['b', 0.6567560434341431]],\n",
       "   [[[450.0, 323.0], [606.0, 325.0], [605.0, 351.0], [449.0, 349.0]],\n",
       "    ['Intermediate', 0.9999385476112366]],\n",
       "   [[[82.0, 345.0], [148.0, 345.0], [148.0, 373.0], [82.0, 373.0]],\n",
       "    ['Input', 0.9998895525932312]],\n",
       "   [[[773.0, 345.0], [870.0, 345.0], [870.0, 373.0], [773.0, 373.0]],\n",
       "    ['Outputs', 0.9998511075973511]],\n",
       "   [[[476.0, 364.0], [582.0, 364.0], [582.0, 388.0], [476.0, 388.0]],\n",
       "    ['Features', 0.9998794198036194]]],\n",
       "  'ocr': [[[82.0, 345.0], [148.0, 345.0], [148.0, 373.0], [82.0, 373.0]],\n",
       "   ['Input', 0.9998895525932312]]},\n",
       " '2204.10765v1-Figure1-1.png': {'caption': 'Fig. 1. Network Architecture. I1,..,I32 denote input RGB-images passed through ResNet(2+1)D encoder Enc. p1,..,p32 refers to predicted instance tags. O1,c,..,O32,c denote binary semantic segmentation masks generated by decoder Dec belonging to cth class category. f1,..,f32 and u1,..,u32 denote input to and output from the spatio-temporal attention module respectively. qt represents features passed to the self-attention module well as tag generator module. v1,..,v32 correspond to embeddings after the self-attention component. w1,..,w32 is the resultant output from the tag-based attention module. ‚äó, ‚äï, implies dot product, concatenate operation and element-wise multiplication.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2204.10765v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'III. METHOD',\n",
       "    'text': 'We propose an end-to-end solution to solve the VIS problem. As illustrated in Figure 1, the model assigns different tags to each object instance in the input RGB frames of a video clip and additionally generates semantic segmentation as a byproduct. Primarily, an ResNet(2+1)D [22] encoder is used to capture the features of the video clip. Thereafter, the resultant features are enriched by passing through spatiotemporal attention module. The generated frame embeddings are then compressed through a bottleneck, which separates the encoder from the tag generator and the decoder. Post the bottleneck component, the network branches into a tag generator that yields instance embeddings and a decoder that provides output semantic segmentation masks. The decoder relies on output from the tag-based attention module and the self-attention module. Though these modules provide input to the decoder, these components play a vital role in implicitly improving the video instance segmentation results through the propagation of loss. While the self-attention module allows us to model long-range dependencies within a frame, the tagbased attention module plays an integral role in improving the instance tags by capturing tag association across frames, which is discussed in detail in Section III-C.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Tag-Based Attention Guided Bottom-Up Approach for Video Instance Segmentation',\n",
       "  'abstract': \"Video Instance Segmentation is a fundamental computer vision task that deals with segmenting and tracking object instances across a video sequence. Most existing methods typically accomplish this task by employing a multi-stage topdown approach that usually involves separate networks to detect and segment objects in each frame, followed by associating these detections in consecutive frames using a learned tracking head. In this work, however, we introduce a simple end-to-end trainable bottom-up approach to achieve instance mask predictions at the pixel-level granularity, instead of the typical region-proposalsbased approach. Unlike contemporary frame-based models, our network pipeline processes an input video clip as a single 3D volume to incorporate temporal information. The central idea of our formulation is to solve the video instance segmentation task as a tag assignment problem, such that generating distinct tag values essentially separates individual object instances across the video sequence (here each tag could be any arbitrary value between 0 and 1). To this end, we propose a novel spatio-temporal tagging loss that allows for sufficient separation of different objects as well as necessary identification of different instances of the same object. Furthermore, we present a tag-based attention module that improves instance tags, while concurrently learning instance propagation within a video. Evaluations demonstrate that our method provides competitive results on YouTube-VIS and DAVIS'19 datasets, and has minimum run-time compared to other state-of-the-art performance methods.\",\n",
       "  'paddleOCR': [[[[1191.0, 24.0],\n",
       "     [1250.0, 33.0],\n",
       "     [1247.0, 57.0],\n",
       "     [1187.0, 48.0]],\n",
       "    ['Ltag', 0.9943252205848694]],\n",
       "   [[[345.0, 47.0], [440.0, 47.0], [440.0, 67.0], [345.0, 67.0]],\n",
       "    ['Location', 0.9795966148376465]],\n",
       "   [[[1116.0, 49.0], [1178.0, 53.0], [1176.0, 81.0], [1114.0, 76.0]],\n",
       "    ['Tag', 0.9995951056480408]],\n",
       "   [[[1264.0, 43.0], [1389.0, 48.0], [1388.0, 75.0], [1263.0, 71.0]],\n",
       "    [' Instance', 0.9816790819168091]],\n",
       "   [[[338.0, 67.0], [446.0, 73.0], [444.0, 104.0], [336.0, 98.0]],\n",
       "    ['Encoding', 0.999631404876709]],\n",
       "   [[[81.0, 87.0], [179.0, 87.0], [179.0, 118.0], [81.0, 118.0]],\n",
       "    ['I1,.., I32', 0.9146822690963745]],\n",
       "   [[[1074.0, 83.0], [1215.0, 88.0], [1215.0, 115.0], [1074.0, 111.0]],\n",
       "    ['Generator', 0.9998385906219482]],\n",
       "   [[[363.0, 106.0], [421.0, 106.0], [421.0, 133.0], [363.0, 133.0]],\n",
       "    ['(x,y)', 0.7902992963790894]],\n",
       "   [[[880.0, 146.0], [1077.0, 146.0], [1077.0, 171.0], [880.0, 171.0]],\n",
       "    ['32 x 14 x 14 x 256', 0.9974167943000793]],\n",
       "   [[[426.0, 187.0], [619.0, 187.0], [619.0, 211.0], [426.0, 211.0]],\n",
       "    ['32 x 14 x 14 x 258', 0.9937669038772583]],\n",
       "   [[[650.0, 186.0], [845.0, 186.0], [845.0, 211.0], [650.0, 211.0]],\n",
       "    ['32 x 14 x 14 x 258', 0.976577639579773]],\n",
       "   [[[404.0, 258.0], [490.0, 258.0], [490.0, 286.0], [404.0, 286.0]],\n",
       "    ['f1,.. f32', 0.888289213180542]],\n",
       "   [[[512.0, 260.0], [725.0, 260.0], [725.0, 286.0], [512.0, 286.0]],\n",
       "    ['Spatio-temporal', 0.9997293949127197]],\n",
       "   [[[747.0, 253.0], [865.0, 257.0], [864.0, 288.0], [746.0, 284.0]],\n",
       "    ['U1,.., U32', 0.9459821581840515]],\n",
       "   [[[557.0, 297.0], [676.0, 297.0], [676.0, 328.0], [557.0, 328.0]],\n",
       "    ['attention', 0.9974866509437561]],\n",
       "   [[[1499.0, 303.0], [1661.0, 314.0], [1659.0, 345.0], [1497.0, 334.0]],\n",
       "    ['Lcrossentropye', 0.9472860097885132]],\n",
       "   [[[1562.0, 387.0], [1704.0, 394.0], [1703.0, 432.0], [1560.0, 425.0]],\n",
       "    ['O1,c.., 032,0', 0.823693037033081]],\n",
       "   [[[1016.0, 451.0], [1151.0, 451.0], [1151.0, 477.0], [1016.0, 477.0]],\n",
       "    ['P1,.,P32', 0.9089503288269043]],\n",
       "   [[[1144.0, 450.0], [1337.0, 450.0], [1337.0, 475.0], [1144.0, 475.0]],\n",
       "    ['32 x 112 x 112 x 1', 0.9701230525970459]],\n",
       "   [[[412.0, 477.0], [850.0, 479.0], [850.0, 517.0], [412.0, 515.0]],\n",
       "    ['Loverall = Ltag + Lcrossentropy', 0.9596778750419617]],\n",
       "   [[[1072.0, 513.0], [1213.0, 513.0], [1213.0, 544.0], [1072.0, 544.0]],\n",
       "    ['Tag-based', 0.9998038411140442]],\n",
       "   [[[299.0, 531.0], [975.0, 531.0], [975.0, 573.0], [299.0, 573.0]],\n",
       "    ['Ltag = Lspectra + Lspecter + Ltempra + Ltemper', 0.9981964230537415]],\n",
       "   [[[23.0, 548.0], [95.0, 548.0], [95.0, 575.0], [23.0, 575.0]],\n",
       "    ['Input', 0.9991345405578613]],\n",
       "   [[[1081.0, 550.0], [1197.0, 550.0], [1197.0, 575.0], [1081.0, 575.0]],\n",
       "    ['attention', 0.9997400641441345]],\n",
       "   [[[15.0, 584.0], [113.0, 588.0], [112.0, 615.0], [14.0, 611.0]],\n",
       "    ['Images', 0.9992992281913757]],\n",
       "   [[[141.0, 601.0], [254.0, 601.0], [254.0, 626.0], [141.0, 626.0]],\n",
       "    ['Encoder', 0.9995598196983337]],\n",
       "   [[[1003.0, 615.0], [1122.0, 619.0], [1122.0, 645.0], [1002.0, 640.0]],\n",
       "    ['W1,.., W32', 0.8973811864852905]],\n",
       "   [[[1144.0, 617.0], [1335.0, 617.0], [1335.0, 642.0], [1144.0, 642.0]],\n",
       "    ['32 x 14 x 14 x 256', 0.9976084232330322]],\n",
       "   [[[106.0, 635.0], [297.0, 635.0], [297.0, 666.0], [106.0, 666.0]],\n",
       "    ['ResNet(2+1)D', 0.999661386013031]],\n",
       "   [[[1012.0, 773.0], [1119.0, 777.0], [1118.0, 803.0], [1011.0, 799.0]],\n",
       "    ['V1,.., V32', 0.8634243011474609]],\n",
       "   [[[1142.0, 772.0], [1333.0, 772.0], [1333.0, 797.0], [1142.0, 797.0]],\n",
       "    ['32 x 14 x 14 x 256', 0.9981482028961182]],\n",
       "   [[[1056.0, 851.0], [1233.0, 856.0], [1233.0, 887.0], [1056.0, 882.0]],\n",
       "    ['Self-attention.', 0.9504601955413818]],\n",
       "   [[[1534.0, 855.0], [1664.0, 855.0], [1664.0, 886.0], [1534.0, 886.0]],\n",
       "    ['Semantic', 0.9998987317085266]],\n",
       "   [[[812.0, 886.0], [1004.0, 886.0], [1004.0, 910.0], [812.0, 910.0]],\n",
       "    ['32 x 14 x 14 x 256', 0.994464635848999]],\n",
       "   [[[1510.0, 894.0], [1697.0, 894.0], [1697.0, 924.0], [1510.0, 924.0]],\n",
       "    ['Segmentation', 0.9997853636741638]],\n",
       "   [[[1369.0, 935.0], [1485.0, 935.0], [1485.0, 966.0], [1369.0, 966.0]],\n",
       "    ['Decoder', 0.9998155832290649]]],\n",
       "  'ocr': [[[1081.0, 550.0], [1197.0, 550.0], [1197.0, 575.0], [1081.0, 575.0]],\n",
       "   ['attention', 0.9997400641441345]]},\n",
       " '2102.09850v1-Figure1-1.png': {'caption': 'Figure 1. Graphical model of sparsity across state variables. Sparsity example: The dimension x3t+1 (shaded in blue) only depends on two dimensions x3t and x2t (in the blue box).',\n",
       "  'imageText': ['‚Ä¶',\n",
       "   'ùë•!&\"',\n",
       "   '%',\n",
       "   'ùë•!&\"$',\n",
       "   'ùë•!&\"#',\n",
       "   'ùë•!&\"\"',\n",
       "   '‚Ä¶',\n",
       "   'ùë•!',\n",
       "   '%',\n",
       "   'ùë•!$',\n",
       "   'ùë•!#',\n",
       "   'ùë•!\"'],\n",
       "  'image_file': '2102.09850v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'State Abstractions and Model Irrelevance',\n",
       "    'text': 'State abstractions allow us to map behaviorally equivalent states into a single abstract state, thus simplifying the learning problem which then makes use of the (potentially much smaller set of) abstract states instead of the original states (Bertsekas & Castanon, 1989). In theory, any function\\n! \" ! # ! $ ! % ‚Ä¶ !&\" \" !&\" # !&\" $ !&\" % ‚Ä¶ Figure 1\\n. Graphical model of sparsity across state variables. Sparsity example: The dimension x 3 t+1 (shaded in blue) only depends on two dimensions x 3 t and x 2 t (in the blue box).\\napproximation architecture can act as an abstraction, since it attempts to group similar states together. Therefore, it is worth exploring the properties of a representation learning scheme as a state abstraction. In the rest of the paper, we build our theory based on this connection.\\nWe are interested in a specific kind of state abstraction called model irrelevance state abstraction or bisimulation (Even-Dar & Mansour, 2003;Ravindran & Barto, 2004;Li, 2009). An abstraction œÜ : X ‚Üí S is model irrelevant if for any two states x, x ‚àà X , abstract state s ‚àà S, a ‚àà A where\\nœÜ(x) = œÜ(x ), R(x, a) = R(x , a), x ‚ààœÜ ‚àí1 (s) P (x |x, a) = x ‚ààœÜ ‚àí1 (s) P (x |x , a) .\\nSince an exact equivalence is not practical, prior work deals with approximate variants through the notion ofcloseness (Jiang, 2018). The main difference between a model irrelevance state abstraction and our proposed modelinvariance state abstraction is that the model irrelevance abstraction does not leverage sparsity in factored dynamics.\\nOur model-invariance state abstraction is variable specific, assuming the state space consists of a set of state variables.\\nWe formally define our model-invariance state abstraction in Section 3.',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 0},\n",
       "   {'heading': 'Theoretical Results',\n",
       "    'text': 'We now move on to providing a connection between causal invariance and model-invariant abstractions. First, we describe the causal setup below: Definition 3. (Causal Setup) For each future state variable indexed by i, x i t+1 , there exists a linear structural equation model consisting of state dimensions and actions, (x i t+1 , x 1 t , ..., x p t , a t ) with coefficients (Œ≤ jk ) j,k=1,...,p+2 , given by a directed acyclic graph. An experimental setting e ‚àà E arises due to one or more interventions on the variable set {x 1 t , ..., x p t , a t }, with the exception of X i t+1 . Assumption 2. (Invariant Prediction (Peters et al., 2015))\\nFor each e ‚àà E: the experimental setting e arises due to one or several interventions on variables from (x 1 t , ..., x p t , a t ) but not on x i t+1 ; here, we allow for do-interventions (Pearl, 2009) or soft-interventions (Eberhardt & Scheines, 2007).\\nFor our purposes, each intervention corresponds to a change in the action distribution, i.e., policy. Thus, in turn, each policy œÄ i defines an environment e. Proposition 1. (Causal Feature Set Existence) Under Assumption 2 the direct causes, i.e., parents of x i t+1 define a valid support over invariant predictors, namely S * = PA(x i t+1 ).\\nThe proof follows directly by applying Proposition 1 of Peters et al. ( 2015) (which itself follows from construction) to each dimension i.\\nNow that we consider each state variable individually, we wish to incorporate the causal invariance idea into the model prediction problem for each state variable. The key idea is to make sure that in predicting each state variable we use only its set of invariant predictors and not all state variables and actions (see Figure 1).\\nWith this intuition, it becomes clearer why our original model learning problem is inherently tied with learning better representations, in that having access to a representation which discards excess information for each state variable (more formally, a casually invariant representation), would be more suited to learning an accurate model over and thus, at least in principle, lead to improved generalization performance across different parts of the state space. We now show that such a casually invariant representation is in fact a model-invariant abstraction.\\nTheorem 1. For the abstraction\\nœÜ i (x) = [x] Si , where S i = PA(x i t+1 ), œÜ i is model-invariant.\\nProof in Appendix B. Next, we show that learning a transition model over a model-invariant abstraction œÜ and then planning over this model is optimal.\\nAssumption 3. (Concentratability Coefficient, Chen & Jiang (2019)) There exists C < ‚àû such that for any admissible distribution ŒΩ,\\n‚àÄ(x, a) ‚àà X √ó A, ŒΩ(x, a) ¬µ(x, a) < C .\\nHere, an admissible distribution refers to any distribution that can be realized in the given CDP by following a policy Figure 2. Consider the network topology CDP (Guestrin et al., 2001). We compare the mean and standard error over 10 random seeds of the estimated transition probability of our invariant learner (orange curve) and MLE (blue curve). œÄ1 is a policy that restarts whichever machine (based on index order) is not working and does nothing if all machines are working. œÄ2 is a random policy. œÄ3 restarts the middle machine most of the times, while acting randomly otherwise. We can see how our invariant learner converges faster and more stably to the common solution (dashed black curve).\\nfor some timesteps. ¬µ refers to the distribution the data is generated from.\\nTheorem 2. (Value bound) If œÜ is an R , i,P approximate model-invariant abstraction on CDP M , and M œÜ is the abstract CDP formed using œÜ, then we can bound the loss in the optimal state action value function in both the CDPs as:\\n[Q * M œÜ ] M ‚àí Q * M 2,ŒΩ ‚â§ ‚àö C 1 ‚àí Œ≥ [Q * M œÜ ] M ‚àí T [Q * M œÜ ] M 2,¬µ [Q * M œÜ ] M ‚àí T [Q * M œÜ ] M 2,¬µ ‚â§ R + Œ≥ p i=1 i,P R max (2(1 ‚àí Œ≥))\\nProof and all details surrounding the theoretical results are provided in Appendix B.',\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Model-Invariant State Abstractions for Model-Based Reinforcement Learning',\n",
       "  'abstract': 'Accuracy and generalization of dynamics models is key to the success of model-based reinforcement learning (MBRL). As the complexity of tasks increases, learning dynamics models becomes increasingly sample inefficient for MBRL methods. However, many tasks also exhibit sparsity in the dynamics, i.e., actions have only a local effect on the system dynamics. In this paper, we exploit this property with a causal invariance perspective in the single-task setting, introducing a new type of state abstraction called model-invariance. Unlike previous forms of state abstractions, a model-invariance state abstraction leverages causal sparsity over state variables. This allows for generalization to novel combinations of unseen values of state variables, something that non-factored forms of state abstractions cannot do. We prove that an optimal policy can be learned over this model-invariance state abstraction. Next, we propose a practical method to approximately learn a model-invariant representation for complex domains. We validate our approach by showing improved modeling performance over standard maximum likelihood approaches on challenging tasks, such as the MuJoCo-based Humanoid. Furthermore, within the MBRL setting we show strong performance gains w.r.t. sample efficiency across a host of other continuous control tasks.',\n",
       "  'paddleOCR': [[[[222.0, 23.0], [241.0, 23.0], [241.0, 47.0], [222.0, 47.0]],\n",
       "    ['1', 0.9999014139175415]],\n",
       "   [[[222.0, 130.0], [241.0, 130.0], [241.0, 151.0], [222.0, 151.0]],\n",
       "    ['2', 0.9989053010940552]],\n",
       "   [[[227.0, 323.0], [247.0, 323.0], [247.0, 330.0], [227.0, 330.0]],\n",
       "    ['...', 0.7426813244819641]],\n",
       "   [[[221.0, 390.0], [276.0, 394.0], [274.0, 419.0], [219.0, 415.0]],\n",
       "    ['t+1', 0.9991671442985535]]],\n",
       "  'ocr': [[[222.0, 130.0], [241.0, 130.0], [241.0, 151.0], [222.0, 151.0]],\n",
       "   ['2', 0.9989053010940552]]},\n",
       " '1382157-Figure1-1.png': {'caption': 'Figure 1: Our proposed method extends a CCAbased method of word embedding by means of multi-view spectral graph embedding frameworks of dimensionality reduction to deal with visual information associated with words in a corpus.',\n",
       "  'imageText': [],\n",
       "  'image_file': '1382157-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Introduction',\n",
       "    'text': 'Word embedding plays important roles in the field of Natural Language Processing (NLP). Many existing studies use word vectors for various downstream NLP tasks, such as text classification, Part-of-Speech tagging, and machine translation. One of the most famous approaches is skip-gram model (Mikolov et al., 2013), which is based on a neural network, and its extensions have also been widely studied as well.\\nThere are alternative approaches depending on a spectral graph embedding framework (Yan et al., 2007;Huang et al., 2012) for word embedding. For examples, Dhillon et al. (2015) proposed a method based on Canonical Correlation Analysis (CCA) (Hotelling, 1936), while a PCA based word embedding method was proposed in Lebret and Collobert (2014).\\nIn recent years, many researchers have been actively studying the use of multiple modalities in the fields of both NLP and computer vision. Those studies combine textual and visual information to propose methods for image-caption matching (Yan and Mikolajczyk, 2015), caption generation (Kiros et al., 2014), visual question answering (Antol et al., 2015), quantifying abstractness  of words, and so on.\\nAs for word embedding, multimodal versions of word2vec (Mikolov et al., 2013) have been proposed in Lazaridou et al. (2015) and Kottur et al. (2016). The first one jointly optimize the objective of both skip-gram model and a cross-modal objective across texts and images, and the latter uses abstract scenes as surrogate labels for capturing visually grounded semantic relatedness. More recently, Mao et al. (2016) proposed a multimodal word embedding methods based on a recurrent neural network to learn word vectors from their newly proposed large scale image caption dataset.\\nIn this paper, we introduce a new spectral graphbased method of multimodal word embedding. Specifically, we extend Eigenwords (Dhillon et al., 2015), a CCA-based method for word embedding, by applying a generalized framework of spectral graph embedding (Nori et al., 2012;Shimodaira, 2016). Figure 1 shows a schematic diagram of our method.\\nIn the rest of this paper, we call our method Multimodal Eigenwords (MM-Eigenwords). The most similar existing method is Multimodal Skip-gram model (MMskip-gram) (Lazaridou et al., 2015), which slightly differ in that our model can easily deal with many-to-many relationships between words in a corpus and their relevant images, while MMskip-gram only considers one-to-one relationships between concrete words and images.\\nUsing a corpus and datasets of image-word rela-Figure 1: Our proposed method extends a CCAbased method of word embedding by means of multi-view spectral graph embedding frameworks of dimensionality reduction to deal with visual information associated with words in a corpus.\\ntionships, which are available in common benchmark datasets or on online photo sharing services, MM-Eigenwords jointly learns word vectors on a common multimodal space and a linear mapping from a visual feature space to the multimodal space. Those word vectors also reflect similarities between words and images. We evaluated the multimodal word representations obtained by our model through word similarity task and concept-to-image search, having found that our model has ability to capture both semantic and word-to-image similarities. We also found that our model captures multimodal linguistic regularities (Kiros et al., 2014), whose examples are shown in Figure 2b.',\n",
       "    'n_publication_ref': 18,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Spectral Graph-Based Method of Multimodal Word Embedding *',\n",
       "  'abstract': 'In this paper, we propose a novel method for multimodal word embedding, which exploit a generalized framework of multiview spectral graph embedding to take into account visual appearances or scenes denoted by words in a corpus. We evaluated our method through word similarity tasks and a concept-to-image search task, having found that it provides word representations that reflect visual information, while somewhat trading-off the performance on the word similarity tasks. Moreover, we demonstrate that our method captures multimodal linguistic regularities, which enable recovering relational similarities between words and images by vector arithmetic.',\n",
       "  'paddleOCR': [[[[197.0, 13.0], [433.0, 13.0], [433.0, 47.0], [197.0, 47.0]],\n",
       "    ['Igenwords', 0.9241587519645691]],\n",
       "   [[[74.0, 73.0], [248.0, 76.0], [247.0, 115.0], [73.0, 112.0]],\n",
       "    ['Context', 0.999830424785614]],\n",
       "   [[[413.0, 73.0], [533.0, 73.0], [533.0, 113.0], [413.0, 113.0]],\n",
       "    ['Word', 0.9998823404312134]],\n",
       "   [[[717.0, 68.0], [854.0, 77.0], [852.0, 119.0], [714.0, 110.0]],\n",
       "    ['Image', 0.9998847842216492]],\n",
       "   [[[63.0, 140.0], [276.0, 136.0], [277.0, 171.0], [64.0, 175.0]],\n",
       "    ['pretty ... is', 0.9667937755584717]],\n",
       "   [[[69.0, 191.0], [271.0, 193.0], [271.0, 227.0], [68.0, 225.0]],\n",
       "    ['the ... and', 0.9801176190376282]],\n",
       "   [[[407.0, 205.0], [532.0, 203.0], [532.0, 227.0], [407.0, 229.0]],\n",
       "    ['anma', 0.9503970146179199]],\n",
       "   [[[75.0, 245.0], [260.0, 245.0], [260.0, 279.0], [75.0, 279.0]],\n",
       "    ['in ... with', 0.9785313010215759]],\n",
       "   [[[48.0, 294.0], [289.0, 294.0], [289.0, 331.0], [48.0, 331.0]],\n",
       "    ['large ... that', 0.9714312553405762]],\n",
       "   [[[425.0, 323.0], [511.0, 327.0], [510.0, 357.0], [423.0, 353.0]],\n",
       "    ['dnos', 0.9710341691970825]],\n",
       "   [[[68.0, 354.0], [271.0, 351.0], [272.0, 382.0], [69.0, 385.0]],\n",
       "    ['a ... made', 0.9599010348320007]],\n",
       "   [[[427.0, 390.0], [497.0, 390.0], [497.0, 416.0], [427.0, 416.0]],\n",
       "    ['bira', 0.7713225483894348]],\n",
       "   [[[39.0, 404.0], [297.0, 406.0], [297.0, 436.0], [39.0, 434.0]],\n",
       "    ['minute ... from', 0.9977308511734009]],\n",
       "   [[[66.0, 606.0], [178.0, 574.0], [196.0, 640.0], [85.0, 672.0]],\n",
       "    ['RK', 0.957385241985321]],\n",
       "   [[[170.0, 712.0], [260.0, 712.0], [260.0, 736.0], [170.0, 736.0]],\n",
       "    ['anos', 0.9905310869216919]],\n",
       "   [[[420.0, 724.0], [479.0, 718.0], [481.0, 740.0], [422.0, 746.0]],\n",
       "    ['Cal', 0.8337440490722656]]],\n",
       "  'ocr': [[[66.0, 606.0], [178.0, 574.0], [196.0, 640.0], [85.0, 672.0]],\n",
       "   ['RK', 0.957385241985321]]},\n",
       " '52459-Figure1-1.png': {'caption': 'Fig. 1. Illustration of the DRCN‚Äôs architecture. It consists of two pipelines: i) label prediction and ii) data reconstruction pipelines. The shared parameters between those two pipelines are indicated by the red color.',\n",
       "  'imageText': [],\n",
       "  'image_file': '52459-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Deep Reconstruction-Classification Networks',\n",
       "    'text': \"This section describes our proposed deep learning algorithm for unsupervised domain adaptation, which we refer to as Deep Reconstruction-Classification Networks (DRCN). We first briefly discuss the unsupervised domain adaptation problem. We then present the DRCN architecture, learning algorithm, and other useful aspects.\\nLet us define a domain as a probability distribution D XY (or just D) on X √ó Y, where X is the input space and Y is the output space. Denote the source domain by P and the target domain by Q, where P = Q. The aim in unsupervised domain adaptation is as follows: given a labeled i.i.d. sample from a source domain S s = {(x s i , y s i )} ns i=1 ‚àº P and an unlabeled sample from a target domain S t u = {(x t i )} nt i=1 ‚àº Q X , find a good labeling function f : X ‚Üí Y on S t u . We consider a feature learning approach: finding a function g : X ‚Üí F such that the discrepancy between distribution P and Q is minimized in F.\\nIdeally, a discriminative representation should model both the label and the structure of the data. Based on that intuition, we hypothesize that a domainadaptive representation should satisfy two criteria: i) classify well the source domain labeled data and ii) reconstruct well the target domain unlabeled data, which can be viewed as an approximate of the ideal discriminative representation. Our model is based on a convolutional architecture that has two pipelines with a shared encoding representation. The first pipeline is a standard convolutional network for source label prediction [35], while the second one is a convolutional autoencoder for target data reconstruction [44,45]. Convolutional architectures are a natural choice for object recognition to capture spatial correlation of images. The model is optimized through multitask learning [12], that is, jointly learns the (supervised) source label prediction and the (unsupervised) target data reconstruction tasks. 1 The aim is that the encoding shared representation should learn the commonality between those tasks that provides useful information for cross-domain object recognition. Figure 1 illustrates the architecture of DRCN. We now describe DRCN more formally. Let f c : X ‚Üí Y be the (supervised) label prediction pipeline and f r : X ‚Üí X be the (unsupervised) data reconstruction pipeline of DRCN. Define three additional functions: 1) an encoder / feature mapping g enc : X ‚Üí F, 2) a decoder g dec : F ‚Üí X , and 3) a feature labeling g lab : F ‚Üí Y. For m-class classification problems, the output of g lab usually forms an m-dimensional vector of real values in the range [0, 1] that add up to 1, i.e., softmax output. Given an input x ‚àà X , one can decompose f c and f r such that\\nf c (x) = (g lab ‚Ä¢ g enc )(x),(1)\\nf r (x) = (g dec ‚Ä¢ g enc )(x). (2\\n)\\nLet Œò c = {Œò enc , Œò lab } and Œò r = {Œò enc , Œò dec } denote the parameters of the supervised and unsupervised model. Œò enc are shared parameters for the feature mapping g enc . Note that Œò enc , Œò dec , Œò lab may encode parameters of multiple layers. The goal is to seek a single feature mapping g enc model that supports both f c and f r .\\nLearning algorithm: The learning objective is as follows. Suppose the inputs lie in X ‚äÜ R d and their labels lie in Y ‚äÜ R m . Let c : Y √ó Y ‚Üí R and r : X √ó X ‚Üí R be the classification and reconstruction loss respectively. Given labeled source sample S s = {(x s i , y s i )} ns i=1 ‚àº P, where y i ‚àà {0, 1} m is a one-hot vector, and unlabeled target sample S t u = {(x t j )} nt j=1 ‚àº Q, we define the empirical losses as:\\nL ns c ({Œò enc , Œò lab }) := ns i=1 c (f c (x s i ; {Œò enc , Œò lab }), y s i ) ,(3)\\nL nt r ({Œò enc , Œò dec }) := nt j=1 r f r (x t j ; {Œò enc , Œò dec }), x t j ) .(4)\\nTypically, c is of the form cross-entropy loss\\nm k=1 y k log[f c (x)] k (recall that f c (x)\\nis the softmax output) and r is of the form squared loss x ‚àí f r (x) 2 2 . Our aim is to solve the following objective:\\nmin ŒªL ns c ({Œò enc , Œò lab }) + (1 ‚àí Œª)L nt r ({Œò enc , Œò dec }),(5)\\nwhere 0 ‚â§ Œª ‚â§ 1 is a hyper-parameter controlling the trade-off between classification and reconstruction. The objective is a convex combination of supervised and unsupervised loss functions. We justify the approach in Section 5. Objective ( 5) can be achieved by alternately minimizing L ns c and L nt r using stochastic gradient descent (SGD). In the implementation, we used RM-Sprop [46], the variant of SGD with a gradient normalization -the current gradient is divided by a moving average over the previous root mean squared gradients. We utilize dropout regularization [47] during L ns c minimization, which is effective to reduce overfitting. Note that dropout regularization is applied in the fully-connected/dense layers only, see Figure 1.\\nThe stopping criterion for the algorithm is determined by monitoring the average reconstruction loss of the unsupervised model during training -the process is stopped when the average reconstruction loss stabilizes. Once the training is completed, the optimal parametersŒò enc andŒò lab are used to form a classification model f c (x t ; {Œò enc ,Œò lab }) that is expected to perform well on the target domain. The DRCN learning algorithm is summarized in Algorithm 1 and implemented using Theano [48].\\nData augmentation and denoising: We use two well-known strategies to improve DRCN's performance: data augmentation and denoising. Data augmentation generates additional training data during the supervised training with respect to some plausible transformations over the original data, which improves generalization, see e.g. [49]. Denoising involves reconstructing clean inputs given their noisy counterparts. It is used to improve the feature invariance of denoising autoencoders (DAE) [33]. Generalization and feature invariance are two properties needed to improve domain adaptation. Since DRCN has both classification and reconstruction aspects, we can naturally apply these two tricks simultaneously in the training stage.\\nLet QX |X denote the noise distribution given the original data from which the noisy data are sampled from. The classification pipeline of DRCN f c thus actually observes additional pairs {(x s i , y s i )} ns i=1 and the reconstruction pipeline f r observes {(x t i , x t i )} nt i=1 . The noise distribution QX |X are typically geometric transformations (translation, rotation, skewing, and scaling) in data augmentation, while either zero-masked noise or Gaussian noise is used in the denoising strategy. In this work, we combine all the fore-mentioned types of noise for denoising and use only the geometric transformations for data augmentation.\",\n",
       "    'n_publication_ref': 10,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation',\n",
       "  'abstract': \"In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target data. In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks. We evaluate the performance of DRCN on a series of cross-domain object recognition tasks, where DRCN provides a considerable improvement (up to ‚àº8% in accuracy) over the prior state-of-the-art algorithms. Interestingly, we also observe that the reconstruction pipeline of DRCN transforms images from the source domain into images whose appearance resembles the target dataset. This suggests that DRCN's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. Finally, we provide a formal analysis to justify the algorithm's objective in domain adaptation context.\",\n",
       "  'paddleOCR': [[[[661.0, 20.0], [751.0, 20.0], [751.0, 36.0], [661.0, 36.0]],\n",
       "    ['source)Class', 0.95066237449646]],\n",
       "   [[[606.0, 94.0], [662.0, 94.0], [662.0, 110.0], [606.0, 110.0]],\n",
       "    ['Dropout', 0.9988489151000977]],\n",
       "   [[[521.0, 171.0], [579.0, 171.0], [579.0, 187.0], [521.0, 187.0]],\n",
       "    ['Dropout', 0.9987972378730774]],\n",
       "   [[[76.0, 211.0], [133.0, 211.0], [133.0, 231.0], [76.0, 231.0]],\n",
       "    ['Conv', 0.9907024502754211]],\n",
       "   [[[217.0, 209.0], [273.0, 213.0], [272.0, 233.0], [216.0, 229.0]],\n",
       "    ['Conv', 0.9836759567260742]],\n",
       "   [[[371.0, 208.0], [417.0, 212.0], [415.0, 233.0], [370.0, 229.0]],\n",
       "    ['Conv', 0.9894202351570129]],\n",
       "   [[[511.0, 211.0], [566.0, 211.0], [566.0, 231.0], [511.0, 231.0]],\n",
       "    ['Dense', 0.9988186955451965]],\n",
       "   [[[205.0, 293.0], [240.0, 293.0], [240.0, 308.0], [205.0, 308.0]],\n",
       "    ['Max', 0.99787837266922]],\n",
       "   [[[347.0, 294.0], [381.0, 294.0], [381.0, 310.0], [347.0, 310.0]],\n",
       "    ['Max', 0.9982221722602844]],\n",
       "   [[[478.0, 294.0], [529.0, 297.0], [527.0, 314.0], [477.0, 311.0]],\n",
       "    ['Flatten', 0.9973081946372986]],\n",
       "   [[[203.0, 306.0], [260.0, 310.0], [259.0, 330.0], [202.0, 326.0]],\n",
       "    ['Pooling', 0.9978414177894592]],\n",
       "   [[[346.0, 308.0], [402.0, 313.0], [400.0, 333.0], [344.0, 328.0]],\n",
       "    ['Pooling', 0.993474543094635]],\n",
       "   [[[38.0, 355.0], [76.0, 355.0], [76.0, 371.0], [38.0, 371.0]],\n",
       "    ['Input', 0.9983474612236023]],\n",
       "   [[[734.0, 359.0], [790.0, 363.0], [789.0, 383.0], [733.0, 379.0]],\n",
       "    ['Dense', 0.996730625629425]],\n",
       "   [[[881.0, 359.0], [931.0, 363.0], [930.0, 383.0], [879.0, 379.0]],\n",
       "    ['Conv', 0.986788809299469]],\n",
       "   [[[1028.0, 359.0], [1078.0, 363.0], [1077.0, 383.0], [1026.0, 379.0]],\n",
       "    ['Conv', 0.9890883564949036]],\n",
       "   [[[1177.0, 360.0], [1222.0, 360.0], [1222.0, 380.0], [1177.0, 380.0]],\n",
       "    [' Conv', 0.9338232278823853]],\n",
       "   [[[833.0, 439.0], [906.0, 443.0], [905.0, 463.0], [832.0, 459.0]],\n",
       "    ['Unpooling', 0.9611104130744934]],\n",
       "   [[[977.0, 439.0], [1050.0, 443.0], [1049.0, 463.0], [976.0, 459.0]],\n",
       "    ['Unpoolinge', 0.9151712656021118]],\n",
       "   [[[1116.0, 436.0], [1191.0, 440.0], [1190.0, 460.0], [1116.0, 456.0]],\n",
       "    ['Unpooling', 0.9478047490119934]],\n",
       "   [[[705.0, 475.0], [768.0, 475.0], [768.0, 491.0], [705.0, 491.0]],\n",
       "    ['Unflattend', 0.9227075576782227]],\n",
       "   [[[1211.0, 487.0], [1264.0, 487.0], [1264.0, 507.0], [1211.0, 507.0]],\n",
       "    ['(target)', 0.9931098222732544]],\n",
       "   [[[1212.0, 508.0], [1312.0, 508.0], [1312.0, 522.0], [1212.0, 522.0]],\n",
       "    ['Reconstruction', 0.9988337755203247]]],\n",
       "  'ocr': [[[734.0, 359.0], [790.0, 363.0], [789.0, 383.0], [733.0, 379.0]],\n",
       "   ['Dense', 0.996730625629425]]},\n",
       " '2110.02192v2-Figure3-1.png': {'caption': 'Figure 3: Current Model',\n",
       "  'imageText': [],\n",
       "  'image_file': '2110.02192v2-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Proposed model',\n",
       "    'text': 'By augmenting the plot of the live acceleration data, a loop between human and reality is formed that eliminates gaze distraction as a barrier to vibration monitoring. Figure 3 illustrates gaze distraction as a barrier and Figure 4 shows the proposed model aided by AR. The user receives direct information on reality via the augmented plot of live data in the AR headset thereby improving cognition of structural response while maintaining an area near central vision. In the framework of this research a user reacts to data by attempting to synchronize the acceleration of a handheld sensor with a moving sensor.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Reducing Gaze Distraction for Real-time Vibration Monitoring Using Augmented Reality',\n",
       "  'abstract': \"Operators want to maintain awareness of the structure being tested while observing sensor data. Normally the human's gaze shifts to a separate device or screen during the experiment for data information, missing the structure's physical response. The human-computer interaction provides valuable data and information but separates the human from the reality. The sensor data does not collect experiment safety, quality, and other contextual information of critical value to the operator. To solve this problem, this research provides humans with real-time information about vibrations using an Augmented Reality (AR) application. An application is developed to augment sensor data on top of the area of interest, which allows the user to perceive real-time changes that the data may not warn of. This paper presents the results of an experiment that show how AR can provide a channel for direct sensor feedback while increasing awareness of reality. In the experiment a researcher attempts to closely follow a moving sensor with their own sensor while observing the moving sensor's data with and without AR. The results of the reported experiment indicate that augmenting the information collected from sensors in real-time narrows the operator's focus to the structure of interest for more efficient and informed experimentation.\",\n",
       "  'paddleOCR': [[[[57.0, 123.0],\n",
       "     [331.0, 125.0],\n",
       "     [331.0, 160.0],\n",
       "     [57.0, 158.0]],\n",
       "    ['Reality - measured', 0.9682904481887817]],\n",
       "   [[[585.0, 127.0], [839.0, 127.0], [839.0, 156.0], [585.0, 156.0]],\n",
       "    ['Time delay in data', 0.9995776414871216]],\n",
       "   [[[1137.0, 123.0], [1325.0, 127.0], [1325.0, 162.0], [1136.0, 158.0]],\n",
       "    ['Computer or', 0.9973264336585999]],\n",
       "   [[[1713.0, 143.0], [1787.0, 148.0], [1785.0, 181.0], [1711.0, 176.0]],\n",
       "    ['User', 0.9999288320541382]],\n",
       "   [[[124.0, 168.0], [264.0, 168.0], [264.0, 197.0], [124.0, 197.0]],\n",
       "    ['by sensor', 0.9997977018356323]],\n",
       "   [[[622.0, 168.0], [803.0, 168.0], [803.0, 197.0], [622.0, 197.0]],\n",
       "    ['transmission', 0.9996293187141418]],\n",
       "   [[[1114.0, 166.0], [1343.0, 166.0], [1343.0, 195.0], [1114.0, 195.0]],\n",
       "    ['seperate screen', 0.9997093081474304]],\n",
       "   [[[404.0, 461.0], [604.0, 461.0], [604.0, 490.0], [404.0, 490.0]],\n",
       "    ['User reacts to', 0.9749430418014526]],\n",
       "   [[[1299.0, 461.0], [1500.0, 461.0], [1500.0, 490.0], [1299.0, 490.0]],\n",
       "    ['User reacts to', 0.999572217464447]],\n",
       "   [[[841.0, 482.0], [1067.0, 482.0], [1067.0, 508.0], [841.0, 508.0]],\n",
       "    ['Gaze distraction', 0.9784733057022095]],\n",
       "   [[[471.0, 500.0], [539.0, 500.0], [539.0, 531.0], [471.0, 531.0]],\n",
       "    ['data', 0.9994407296180725]],\n",
       "   [[[1351.0, 493.0], [1446.0, 499.0], [1444.0, 536.0], [1349.0, 530.0]],\n",
       "    ['reality', 0.9637506604194641]]],\n",
       "  'ocr': [[[1713.0, 143.0], [1787.0, 148.0], [1785.0, 181.0], [1711.0, 176.0]],\n",
       "   ['User', 0.9999288320541382]]},\n",
       " '2103.04784v1-Figure1-1.png': {'caption': 'Fig. 1. System model for the RIS-assisted spatial equalization.',\n",
       "  'imageText': ['Artificial',\n",
       "   'multi-path',\n",
       "   'Multi-path',\n",
       "   'scattering',\n",
       "   'g',\n",
       "   'R',\n",
       "   'g',\n",
       "   'D',\n",
       "   'Direct',\n",
       "   'ray',\n",
       "   'Reflection',\n",
       "   'ray',\n",
       "   'Spatial',\n",
       "   'equalizer',\n",
       "   'RIS',\n",
       "   'controller',\n",
       "   'User',\n",
       "   'K',\n",
       "   'User',\n",
       "   '1',\n",
       "   'User',\n",
       "   '2',\n",
       "   'Base',\n",
       "   'Station'],\n",
       "  'image_file': '2103.04784v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'SYSTEM MODEL',\n",
       "    'text': 'As shown in Fig. 1, we consider a downlink multi-user RISassisted MISO communication network consisting of one base station (BS) with M antennas and K single-antenna users, denoted by K = {1, . . . , K}. To reduce the ISI, an RIS is deployed as a spatial equalizer. The RIS is composed of N electrically controllable elements with the size length being a, denoted by N = 1, . . . , N . Each element can adjust its phase shift by switching Positive-Intrinsic-Negative (PIN) diodes between \"ON\" and \"OFF\" states. Due to some physical limitations, the state transition for each PIN diode may cost some time. In this paper, within a considered period, we assume that the phase shift for each element is fixed. Define Œ∏ n as the phase shift for element n, and the reflection factor of element n can be written by Œì n = Œìe ‚àíjŒ∏n , where\\nŒì ‚àà [0, 1] is a constant.\\nFor each user, it can receive two rays of signals. The first ray is the direct link from the BS, which consists of the scattered signals from the environment. We define g D k (t) as the channel impulse response of the direct link from the BS to user k, which models independent fast fading and path loss. To be specific, g D k (t) can be written as\\ng D k (t) = (Œ≤ D k ) 1/2 h D k (t),(1)\\nwhere h D k (t) is the fast fading coefficient caused by the multipath effect and Œ≤ D k is the path loss related to distance d k between the BS and user k, i.e., Œ≤\\nD k = Gd ‚àíŒ± k .\\nHere, G is a normalized factor for the direct link and Œ± is the path loss exponent.\\nThe second ray is the reflection link through the RIS. Each RIS element will reflect the incident signals from the BS to users to eliminate the multi-path effect. We define g R n,k (t) as the channel impulse response of the reflection link through RIS element n to user k, which also includes independent fast fading and path loss. Specifically, g R n,k (t) can be written as\\ng D n,k (t) = (Œ≤ R n,k ) 1/2 Œì n h R n,k (t),(2)\\nwhere h R n,k (t) is the fast fading coefficient and Œ≤ R n,k is the path loss related to distance l n between the BS and the nth RIS element, and distance l n,k between the n-th RIS element and user k. According to the result in [9], we have Œ≤ R n,k = G ‚Ä≤ (l n l n,k ) ‚àíŒ± where G ‚Ä≤ is a normalized factor for the reflection link. It is worthwhile to point out that we can approximate the distance to different RIS elements as the distance to the center of the RIS when l n , l n,k ‚â´ a [6]. Therefore, we have Œ≤ R n,k ‚âàŒ≤ R k , ‚àÄn ‚àà N , whereŒ≤ R k is the path loss of the link going through the center of the RIS.\\nDefine one-bit signal for user k as s k (t), and the received signal at user k can be written as 1\\ny k (t) = g D k (t) + n‚ààN g D n,k (t) * s k (t),(3)\\nwhere * is the convolution operator.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'SPATIAL EQUALIZATION BEFORE RECEPTION: RECONFIGURABLE INTELLIGENT SURFACES FOR MULTI-PATH MITIGATION',\n",
       "  'abstract': 'Reconfigurable intelligent surfaces (RISs), which enable tunable anomalous reflection, have appeared as a promising method to enhance wireless systems. In this paper, we propose to use an RIS as a spatial equalizer to address the well-known multi-path fading phenomenon. By introducing some controllable paths artificially against the multi-path fading through the RIS, we can perform equalization during the transmission process instead of at the receiver, and thus all the users can share the same equalizer. Unlike the beamforming application of the RIS, which aims to maximize the received energy at receivers, the objective of the equalization application is to reduce the inter-symbol interference (ISI), which makes phase shifts at the RIS different. To this end, we formulate the phase shift optimization problem and propose an iterative algorithm to solve it. Simulation results show that the multi-path fading effect can be eliminated effectively compared to benchmark schemes.',\n",
       "  'paddleOCR': [[[[446.0, 1.0], [584.0, 34.0], [579.0, 55.0], [441.0, 22.0]],\n",
       "    ['Reconfigurable', 0.9996570944786072]],\n",
       "   [[[429.0, 21.0], [590.0, 62.0], [584.0, 83.0], [424.0, 42.0]],\n",
       "    ['intelligent surface', 0.990219235420227]],\n",
       "   [[[234.0, 58.0], [390.0, 58.0], [390.0, 80.0], [234.0, 80.0]],\n",
       "    ['Spatial equalizer', 0.9995303153991699]],\n",
       "   [[[478.0, 59.0], [527.0, 70.0], [522.0, 92.0], [473.0, 81.0]],\n",
       "    ['(RIS)', 0.8937735557556152]],\n",
       "   [[[0.0, 103.0], [128.0, 108.0], [128.0, 129.0], [0.0, 124.0]],\n",
       "    ['Reflection ray', 0.9992212057113647]],\n",
       "   [[[667.0, 167.0], [796.0, 170.0], [795.0, 191.0], [666.0, 189.0]],\n",
       "    ['RIS controller', 0.9986433386802673]],\n",
       "   [[[20.0, 186.0], [112.0, 189.0], [112.0, 211.0], [20.0, 207.0]],\n",
       "    ['Direct ray', 0.9995288848876953]],\n",
       "   [[[686.0, 217.0], [854.0, 217.0], [854.0, 234.0], [686.0, 234.0]],\n",
       "    ['Artificial multi-path', 0.9883162975311279]],\n",
       "   [[[758.0, 307.0], [825.0, 307.0], [825.0, 328.0], [758.0, 328.0]],\n",
       "    ['User K', 0.9969314932823181]],\n",
       "   [[[552.0, 392.0], [614.0, 392.0], [614.0, 411.0], [552.0, 411.0]],\n",
       "    ['User 1', 0.9994277954101562]],\n",
       "   [[[646.0, 391.0], [712.0, 391.0], [712.0, 413.0], [646.0, 413.0]],\n",
       "    ['User 2', 0.9998447895050049]],\n",
       "   [[[113.0, 433.0], [233.0, 435.0], [232.0, 456.0], [113.0, 454.0]],\n",
       "    ['Base Station', 0.9601294994354248]],\n",
       "   [[[475.0, 434.0], [663.0, 438.0], [663.0, 458.0], [475.0, 455.0]],\n",
       "    ['Multi-path scattering', 0.9797131419181824]],\n",
       "   [[[271.0, 512.0], [363.0, 536.0], [358.0, 555.0], [266.0, 534.0]],\n",
       "    ['Obstacles', 0.9990483522415161]]],\n",
       "  'ocr': [[[646.0, 391.0], [712.0, 391.0], [712.0, 413.0], [646.0, 413.0]],\n",
       "   ['User 2', 0.9998447895050049]]},\n",
       " '2207.09399v1-Figure5-1.png': {'caption': 'Fig. 5. The illustration of the lane decoder. The forward branch predicts the forward part of the lane via forward transfer map Tf and forward distance map Df . The backward part can be decoded from the backward branch similarly.',\n",
       "  'imageText': ['Backward',\n",
       "   'branch',\n",
       "   'Forward',\n",
       "   'branch',\n",
       "   'A',\n",
       "   'start',\n",
       "   'point',\n",
       "   'ùëùùëñ',\n",
       "   'Merge',\n",
       "   'S',\n",
       "   'BDM',\n",
       "   'FDM',\n",
       "   'ùê∑ùëè',\n",
       "   'ùëùùëñIterine',\n",
       "   '√ó',\n",
       "   'ùê∑ùëì',\n",
       "   'ùëùùëñIterine',\n",
       "   '√ó',\n",
       "   'ùëùùëñ',\n",
       "   'ùëì',\n",
       "   'ùëáùëì',\n",
       "   'ùëùùëñ',\n",
       "   'ùëì+',\n",
       "   'ùëùùëñ',\n",
       "   'ùëè',\n",
       "   'ùëáùëè',\n",
       "   'ùëùùëñ',\n",
       "   'ùëè+'],\n",
       "  'image_file': '2207.09399v1-Figure5-1.png',\n",
       "  'sections': [{'heading': 'Related work',\n",
       "    'text': 'Existing methods for lane detection can be categorized into: segmentation-based methods, proposal-based methods, row-wise methods and polynomial regression methods.\\nSegmentation-based methods. Segmentation-based methods [7,12,13,20,21], typically make predictions based on pixel-wise classification. Each pixel will be classified as either on lane or background to generate a binary segmentation mask. Then a post-processing step is used to decode it into a set of lanes. But it is still challenging to assign different points to their corresponding lane instances. A common solution is to predict the instance segmentation mask. However, the number of lanes has to be predefined and fixed when using this strategy, which is not robust for real driving scenarios.\\nProposal-based methods. Proposal-based methods [4,34,27], take a top-todown pipeline that directly regresses the relative coordinates of lane shapes. Nevertheless, they always struggle in lanes with complex topologies such as curve lanes and Y-shaped lanes. The fixed anchor shape has a major flaw when regressing the variable lane shapes in some hard scenes. Row-wise methods. Based on the grid division of the input image, row-wise detection approaches [6,22,23,35,15] have achieved great progress in terms of accuracy and efficiency. Generally, row-wise detection methods directly predict the lane position for each row and construct the set of lanes through postprocessing. However, detecting nearly horizontal lanes which fall at small vertical intervals is still a major problem. Polynomial regression methods. Polynomial regression methods [16,28] directly outputs polynomials representing each lane. The deep network is firstly used in [28] to predict the lane curve equation, along with the domains for these polynomials and confidence scores for each lane. [16] uses a transformer [31] to learn richer structures and context, and reframes the lane detection output as parameters of a lane shape model. However, despite of the fast speed polynomial  [33] is used as backbone. The output head consists of three branches. The segment head predicts segmentation map (S ). The distance head and the transfer head predict distance map (D) and transfer map (T ) respectively. Both kinds of maps contain forward and backward parts. Then, Point-NMS is used for sparse segmentation results. All predictions are fed into the lane decoder (Fig. 5), to get final results. regression methods achieve, there is still some distance from the state of the art results.',\n",
       "    'n_publication_ref': 19,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'RCLane: Relay Chain Prediction for Lane Detection',\n",
       "  'abstract': 'Lane detection is an important component of many realworld autonomous systems. Despite a wide variety of lane detection approaches have been proposed, reporting steady benchmark improvements over time, lane detection remains a largely unsolved problem. This is because most of the existing lane detection methods either treat the lane detection as a dense prediction or a detection task, few of them consider the unique topologies (Y-shape, Fork-shape, nearly horizontal lane) of the lane markers, which leads to sub-optimal solution. In this paper, we present a new method for lane detection based on relay chain prediction. Specifically, our model predicts a segmentation map to classify the foreground and background region. For each pixel point in the foreground region, we go through the forward branch and backward branch to recover the whole lane. Each branch decodes a transfer map and a distance map to produce the direction moving to the next point, and how many steps to progressively predict a relay station (next point). As such, our model is able to capture the keypoints along the lanes. Despite its simplicity, our strategy allows us to establish new state-of-the-art on four major benchmarks including TuSimple, CULane, CurveLanes and LLAMAS.',\n",
       "  'paddleOCR': [[[[30.0, 23.0], [212.0, 28.0], [211.0, 58.0], [29.0, 53.0]],\n",
       "    ['-> Merge', 0.9661814570426941]],\n",
       "   [[[888.0, 18.0], [1096.0, 18.0], [1096.0, 44.0], [888.0, 44.0]],\n",
       "    ['Forward branch', 0.9994555115699768]],\n",
       "   [[[505.0, 29.0], [625.0, 29.0], [625.0, 56.0], [505.0, 56.0]],\n",
       "    ['Iterine ', 0.999732494354248]],\n",
       "   [[[694.0, 29.0], [788.0, 29.0], [788.0, 62.0], [694.0, 62.0]],\n",
       "    ['Df(pi)', 0.9721570014953613]],\n",
       "   [[[380.0, 229.0], [443.0, 229.0], [443.0, 251.0], [380.0, 251.0]],\n",
       "    ['FDM', 0.9980521202087402]],\n",
       "   [[[0.0, 285.0], [123.0, 285.0], [123.0, 321.0], [0.0, 321.0]],\n",
       "    ['A start', 0.9999513626098633]],\n",
       "   [[[506.0, 329.0], [627.0, 329.0], [627.0, 355.0], [506.0, 355.0]],\n",
       "    ['Iterine ', 0.9996576905250549]],\n",
       "   [[[691.0, 329.0], [785.0, 329.0], [785.0, 361.0], [691.0, 361.0]],\n",
       "    ['Dp(pi)', 0.9924243092536926]],\n",
       "   [[[869.0, 323.0], [1096.0, 323.0], [1096.0, 349.0], [869.0, 349.0]],\n",
       "    ['Backward branch', 0.9870027899742126]],\n",
       "   [[[0.0, 338.0], [139.0, 335.0], [140.0, 373.0], [0.0, 377.0]],\n",
       "    ['point Pi', 0.9968085289001465]],\n",
       "   [[[556.0, 451.0], [647.0, 451.0], [647.0, 489.0], [556.0, 489.0]],\n",
       "    ['Tp(pi)', 0.9243906140327454]],\n",
       "   [[[378.0, 511.0], [448.0, 515.0], [447.0, 544.0], [377.0, 540.0]],\n",
       "    ['BDM', 0.9979405999183655]]],\n",
       "  'ocr': [[[0.0, 285.0], [123.0, 285.0], [123.0, 321.0], [0.0, 321.0]],\n",
       "   ['A start', 0.9999513626098633]]},\n",
       " '2107.11990v2-Figure5-1.png': {'caption': 'Fig. 5: The network architecture of our high-order heterogeneous augmentation pathways network. Four heterogeneous neural pathways (HeAP4) are responding to four different input images (lightly augmented images, GridShuffled images with g=(2, 4, 7)). Note that only the main neural pathway in red color is activated during inference.',\n",
       "  'imageText': ['u',\n",
       "   'g.',\n",
       "   't',\n",
       "   'A',\n",
       "   'Li',\n",
       "   'gh',\n",
       "   'ff',\n",
       "   'le',\n",
       "   'Sh',\n",
       "   'u',\n",
       "   'ri',\n",
       "   'd',\n",
       "   'g.',\n",
       "   'G',\n",
       "   'A',\n",
       "   'u',\n",
       "   'at',\n",
       "   'a',\n",
       "   'vy',\n",
       "   'D',\n",
       "   'H',\n",
       "   'ea',\n",
       "   'ùëî',\n",
       "   '=',\n",
       "   '7',\n",
       "   'ùëî',\n",
       "   '=',\n",
       "   '4',\n",
       "   'ùëî',\n",
       "   '=',\n",
       "   '2',\n",
       "   'Feature',\n",
       "   'Maps',\n",
       "   'HeAP-Conv',\n",
       "   'Neural',\n",
       "   'Pathway',\n",
       "   '...',\n",
       "   '...',\n",
       "   '...',\n",
       "   '...'],\n",
       "  'image_file': '2107.11990v2-Figure5-1.png',\n",
       "  'sections': [{'heading': 'Extensions for Augmentation Pathways',\n",
       "    'text': 'As shown in Table 1, some augmentation policies have several choices of hyperparameters. Deep models are usually sensitive to these hyperparameters, since different augmentation hyperparameters for the same image may lead to a wide variety of appearances. Previous methods tend to find one proper hyperparameter according to expert knowledge or automatically searching results.\\nWe found that common visual patterns exist among augmentation policy under different hyperparameters, and the shared feature space among them usually present dependencies. For example, the shared feature learned from Blur(k = 5) can benefit the recognition of image with Blur(k < 5). For GridShuffle, some visual detail patterns learned from small grids can be reused to represent images with large grids. Thus we extend the augmentation pathways for handling augmentation policy under various hyperparameter settings. We rank the hyperparameters of augmentation according to their distribution similarities to the original training image, and then feed the images augmented with different hyperparameters into different pathways in a high-order (nested) manner. In this way, our high-order AP can gather and structure information from augmentations with various hyperparameters. Extension-1: High-order Homogeneous Augmentation Pathways We extend the basic augmentation pathway into high-order to mine shared visual patterns in different levels. Take GridShuffle as an example, we choose two different hyper-parameters to  generate augmented image œï = GridShuffle(g = 2) and œï = GridShuffle(g = 7). The images augmented by GridShuffle are expected to learn visual patterns inner grids, since the positions of all grids in image have been shuffled [11]. Considering grids in œï are smaller than œÜ and grids in œï, the local detail features learned from œï can be reused in œï and œÜ. We propose a convolution with 3rd-order homogeneous augmentation pathways (AP 3 -Conv), which consists of three homogeneous convolutions c 1 t , c 2 t , and c 3 t for handling different inputs. Similar to the basic AP-Conv, c 1 t is the main augmentation pathway targeting at light augmentations œÜ-specific feature, while augmentation pathway c 2 t and c 3 t are designed for learning the shared visual patterns of {œÜ, œï} and {œÜ, œï, œï }, respectively. The operation of AP 3 -Conv can be formulated as: (5) In general, the standard convolution c j t (x) can be defined as an operation filtering information from the j-th to the last neural pathways, c j t (x) = W 1 t c j t‚àí1 (x) + + c j+1 t‚àí1 (x)... + + c k t‚àí1 (x) + b k t , (6) where 1 ‚â§ j ‚â§ k, k is the count of neural pathways in total. For AP 3 -Conv, we set k = 3. c 1 t takes the outputs of\\nc 1 t‚àí1 , c 2 t‚àí1 , c 3 t‚àí1\\nas inputs, while c 2 t takes the outputs of c 2 t‚àí1 , c 3 t‚àí1 as inputs. In this way, the dependency across œÜ, œï and œï can be built. Fig. 4 indicates a network with 3rd-order homogeneous augmentation pathways (AP 3 ) handling two different hyperparameters for Grid- Fig. 5: The network architecture of our high-order heterogeneous augmentation pathways network. Four heterogeneous neural pathways (HeAP 4 ) are responding to four different input images (lightly augmented images, GridShuffled images with g=(2, 4, 7)). Note that only the main neural pathway in red color is activated during inference. Shuffle, whose objective function is defined as:\\nL cls = N i=1 L (f œÜ ( T (œÜ i )), l i ) + L (f œï ( T (œï i )), l i ) + L f œï ( T (œï i )), l i + ŒªS i ,(7)\\nS i = T t=1 c 1 t (œÜ i ), c 2 t (œÜ i ), c 3 t (œÜ i ) + c 2 t (œï i ), c 3 t (œï i ) .\\nThe original image œÜ = I i is predicted by f œÜ ( T (I i )) during inference.\\nBy analogy, we can design higher-order augmentation pathways network of k different homogeneous dataflow pathways, for handling k‚àí1 different settings of a given heavy data augmentation policy. In general, our high-order AP k -Conv can handle various settings of the given augmentation and collect useful visual patterns in different levels. At last, all features are integrated in a dependency manner and results in well-structured feature space for original image classification. Extension-2: High-order Heterogeneous Augmentation Pathways We have adapted homogeneous neural pathways and loss functions for various hyperparameters of given heavy data augmentation in a high-order augmentation pathway network. The basic structure and settings (e.g., kernel sizes, strides in each subconvolutional layer) of these neural pathways are the same in AP k . However, images augmented using different hyperparameters may have different characteristics, which is a reasonable motivation for customizing the basic settings of neural pathways for inputs with different properties. Again we take GridShuffle as an example, higher-resolution representations are more suitable for learning from detailed features in smaller grids. It means that the neural pathway consists of convolutions with larger feature map outputs that would be more friendly to GridShuffle with a larger g.\\nHere we introduce another high-order extension of basic augmentation pathways for integrating representations learned from heterogeneous augmentation pathways for different characteristics. Fig. 5 shows the pipeline of a 4th-order heterogeneous augmentation pathways (HeAP 4 ) based network with heavy augmentation in three different settings GridShuffle(g = 2, 4, 7). Similar to the architecture of HRNet [29], [30], different neural pathways are configured with convolutions with different kernel sizes and channel sizes and result in feature maps in different resolutions. The augmentation pathway in green color is shared among all pathways since detailed visual patterns inner grids of GridShuffle(g = 7) is useful for the classification of all other inputs. Four-resolution feature maps are fed into the main pathway in a nested way during inference of the original image. We apply convolution-based downsample for zooming out the feature maps to its dependent ones. Our heterogeneous neural pathway based convolutions are used for integrating features learned from different augmentations. Each neural pathway is followed by one specific classification head. The objective function of HeAP 4 network is the same as the 4th-order homogeneous augmentation pathways network.',\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 3},\n",
       "   {'heading': 'Discussions',\n",
       "    'text': 'To evaluate the statistical significance and stability of the proposed method, we report the mean and standard deviation of the accuracy from five trials for all below ablation experiments on ImageNet 100 . Impact of the Cross Pathways Connections We design ablation studies by removing cross-pathways connections (w/o feature sharing among pathways) in AP-Conv but remaining the loss functions in Eq. (4) and Eq. (7) (as shown in Fig. 5). For standard ConvNet, heavily augmented views can directly influence the training of all parameters. However for AP-Net w/o sharing weights, heavily augmented views can only affect a half set of parameters\\' training (if we set m t = 1 2 n t as default). The results in Table 6 show that (1) our proposed loss function leads to +0.87% improvement over baselines, and (2) AP-style architecture further boost 1.18% gain, due to the visual commonality learned among pathways.\\nMoreover, Table 5 shows that increasing the influence of heavily augmented views leads to performance drop (ConvNet is equal to AP-Net w/o sharing weight when m t = n t ). Such phenomenon is owing to the irrelevant feature bias introduced by the heavy augmentations. The divided pathways design can suppress such irrelevance. Impact of Distortion Magnitudes of Augmentations The experimental results in Fig. 7 shows that our AP method can stably boosts the performance of ConvNet under various hyperparameters for RandAugment. Impact of Cross Pathways Regularization S To demonstrate the effects of S, we perform the regularization item separation experiments on AP-ResNet-50 with RandAugment. The results are shown in Table 7. We also compared the AP-ResNet-50 performance by applying different settings of Œª = n √ó œâ for evaluating AP-Net\\'s sensitivity to the choice of Œª. It shows that cross pathways regularization benefits the feature space structure ResNet-50 AP-ResNet-50 Fig. 7: Top-1 accuracy (%) on ImageNet 100 by using RandAugment with different (n,m). across different neural pathways, resulting in better performance. But too high loss weight for S would lead to a performance drop, behaving similarly to the standard weight decay in the common neural network training. Generalize the \\'light vs. heavy\" Augmentation Policy Settings to \"basic vs. heavier\" Inspired by the related work [6], defining d as the deviation of augmented view from the original view, given two augmented view œÜ and œï, we denote œï is heavier than œÜ only if d(œï) > d(œÜ). There are two situations to adjudge d(œï) > d(œÜ):\\n1) œï and œÜ are augmented by the same policies, but œï is augmented with more aggressive hyperparameter. 2) œï is augmented by policies which is a proper superset of augmentations used for generating œÜ. In AP, the basic view œÜ and the heavier view œï are fed to the main and augmentation pathway, respectively. It means some heavy augmentation policies may generate basic view œÜ, e.g. ConvNeXt applies the combination of Random Crop, Mixup, Cutmix, RandAugment, and Random Erasing as basic augmentations for generating œÜ. We can introduce another RandAugment on œÜ to generate heavier view œï for ConvNeXt. The experimental results in Table 2 show that AP-ConvNeXt-Tiny with twice RandAugment outperforms ConvNeXt-Tiny. Accordingly, heavier view œï can be generated by applying additional light augmentation, e.g. we can apply another crop operation based on œÜ to generate the heavier view œï (simulating the aggressive crop operation), and it still results in performance improvement, as shown in Table 8. Model Inference The augmented pathways are designed to stabilize main-pathway training when heavy data augmentations are present. During inference, no heavy augmentation are adopted, only f œÜ in the main neural pathway for the original image are used for computing probability. Model Complexity Although AP usually takes more memory cost during model training than the standard ConvNet, many connections can be cut out while replacing traditional convolutions with AP-Convs. Thus the AP version of a given standard CNN network has fewer parameters (#Params.) to learn and lower computational cost (GMACs, Multiply-Accumulate Operations) during inference, as specified in Tables 2, 4 and Eq. (3)..',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Augmentation Pathways Network for Visual Recognition',\n",
       "  'abstract': 'Data augmentation is practically helpful for visual recognition, especially at the time of data scarcity. However, such success is only limited to quite a few light augmentations (e.g., random crop, flip). Heavy augmentations are either unstable or show adverse effects during training, owing to the big gap between the original and augmented images. This paper introduces a novel network design, noted as Augmentation Pathways (AP), to systematically stabilize training on a much wider range of augmentation policies. Notably, AP tames various heavy data augmentations and stably boosts performance without a careful selection among augmentation policies. Unlike traditional single pathway, augmented images are processed in different neural paths. The main pathway handles the light augmentations, while other pathways focus on the heavier augmentations. By interacting with multiple paths in a dependent manner, the backbone network robustly learns from shared visual patterns among augmentations, and suppresses the side effect of heavy augmentations at the same time. Furthermore, we extend AP to high-order versions for high-order scenarios, demonstrating its robustness and flexibility in practical usage. Experimental results on ImageNet demonstrate the compatibility and effectiveness on a much wider range of augmentations, while consuming fewer parameters and lower computational costs at inference time.',\n",
       "  'paddleOCR': [[[[1326.0, 307.0],\n",
       "     [1645.0, 319.0],\n",
       "     [1642.0, 401.0],\n",
       "     [1323.0, 389.0]],\n",
       "    ['H', 0.7243263721466064]],\n",
       "   [[[247.0, 329.0], [401.0, 329.0], [401.0, 358.0], [247.0, 358.0]],\n",
       "    [' Feature Maps', 0.9624086022377014]],\n",
       "   [[[17.0, 343.0], [46.0, 344.0], [41.0, 455.0], [12.0, 453.0]],\n",
       "    ['ight Aug.', 0.9994558691978455]],\n",
       "   [[[247.0, 375.0], [422.0, 379.0], [421.0, 409.0], [246.0, 404.0]],\n",
       "    ['Neural Pathway', 0.9991225600242615]],\n",
       "   [[[249.0, 425.0], [376.0, 425.0], [376.0, 454.0], [249.0, 454.0]],\n",
       "    ['HeAP-Conv', 0.9990251064300537]]],\n",
       "  'ocr': [[[247.0, 329.0], [401.0, 329.0], [401.0, 358.0], [247.0, 358.0]],\n",
       "   [' Feature Maps', 0.9624086022377014]]},\n",
       " '1339538-Figure3-1.png': {'caption': 'Fig. 3. LSTM architecture for phase recognition.',\n",
       "  'imageText': ['4096/4103',\n",
       "   '1024',\n",
       "   '8',\n",
       "   'time',\n",
       "   '3',\n",
       "   'fc7/fc8',\n",
       "   'fc_phase',\n",
       "   '8',\n",
       "   'LSTM',\n",
       "   'LSTM',\n",
       "   'LSTM',\n",
       "   'LSTM‚Ä¶'],\n",
       "  'image_file': '1339538-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Methodology',\n",
       "    'text': 'In previous work [6], we proposed two convolutional neural network (CNN) architectures to perform surgical phase recognition: PhaseNet and EndoNet, shown in Fig. 2. PhaseNet is designed to solely perform the phase recognition task, while EndoNet is designed to jointly perform the phase recognition and tool presence detection tasks. In [6], it has been shown that the multi-task network performs better than the single-task counterpart. However, the multi-task network requires both phase and tool presence annotations which are not available in the m2cai16-workflow dataset. In Section 3.1, we will explain how we conduct our experiments to cope with this limitation.\\nNote that the network is finetuned to perform the phase recognition task using solely image features, thus there is no temporal constraint incorporated in the prediction process. In order to enforce the temporal constraints, we propose to use two different approaches: (1) HMM-based and (2) LSTM-based. The HMM-based approach is similar to the one presented in [6]. First, we extract image features (the output of the second last layer of each network, i.e., fc7 in PhaseNet and fc8 in EndoNet) from the video frames. Then, they are passed to a multi-class linear SVM to compute the values representing the confidences of an image belonging to the phases. Ultimately, these confidences are then taken as input to a hierarchical HMM (HHMM). Since the recognition is performed online, we use the forward algorithm to compute the final predictions.\\nThe second approach uses long-short term memory (LSTM) network to enforce the temporal constraint. We pass the image features to an LSTM network with 1024 states. These states are then passed to a fully connected layer with 8 nodes (equal to the number of phases in the m2cai16-workflow dataset). The output values of this fully connected layer represent the confidences of the image belonging to the phases and are used for final predictions. The LSTM network is shown in Figure 3.',\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Single-and Multi-Task Architectures for Surgical Workflow Challenge at M2CAI 2016',\n",
       "  'abstract': '',\n",
       "  'paddleOCR': [[[[0.0, 100.0], [89.0, 100.0], [89.0, 123.0], [0.0, 123.0]],\n",
       "    ['fc_phase', 0.9998009204864502]],\n",
       "   [[[1123.0, 106.0], [1141.0, 106.0], [1141.0, 127.0], [1123.0, 127.0]],\n",
       "    ['8', 0.9994576573371887]],\n",
       "   [[[133.0, 236.0], [227.0, 236.0], [227.0, 271.0], [133.0, 271.0]],\n",
       "    ['LSTM', 0.9967424869537354]],\n",
       "   [[[336.0, 237.0], [432.0, 237.0], [432.0, 271.0], [336.0, 271.0]],\n",
       "    ['LSTM', 0.9942947030067444]],\n",
       "   [[[542.0, 237.0], [639.0, 237.0], [639.0, 271.0], [542.0, 271.0]],\n",
       "    ['LSTM', 0.9819933176040649]],\n",
       "   [[[926.0, 241.0], [1020.0, 241.0], [1020.0, 275.0], [926.0, 275.0]],\n",
       "    ['LSTM', 0.9973054528236389]],\n",
       "   [[[1102.0, 238.0], [1158.0, 238.0], [1158.0, 263.0], [1102.0, 263.0]],\n",
       "    ['1024', 0.9998095035552979]],\n",
       "   [[[4.0, 382.0], [78.0, 382.0], [78.0, 406.0], [4.0, 406.0]],\n",
       "    ['fc7/fc8', 0.9992529153823853]],\n",
       "   [[[1071.0, 386.0], [1183.0, 386.0], [1183.0, 409.0], [1071.0, 409.0]],\n",
       "    ['4096/4103', 0.9997685551643372]],\n",
       "   [[[953.0, 487.0], [1036.0, 492.0], [1034.0, 519.0], [951.0, 519.0]],\n",
       "    ['time', 0.9996976256370544]]],\n",
       "  'ocr': [[[1123.0, 106.0], [1141.0, 106.0], [1141.0, 127.0], [1123.0, 127.0]],\n",
       "   ['8', 0.9994576573371887]]},\n",
       " '2206.01256v2-Figure2-1.png': {'caption': 'Figure 2: (a) The illustration of the coordinate system transformation from frame t‚àí 1 to frame t. (b) Architecture of feature-guided position encoder. Different from PETR [21], 3D PE in PETRv2 is generated in a data-dependent way.',\n",
       "  'imageText': ['(a)', '(b)'],\n",
       "  'image_file': '2206.01256v2-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Temporal Modeling',\n",
       "    'text': \"PETR [21] leverages image features and projected 3D points to generate 3D features and works well in multi-view 3D detection. In this section, we extend it with the temporal modeling, which is realized by a 3D coordinates alignment (CA) and feature-guided position encoder (FPE), for better localization and speed estimation.\\n3D Coordinates Alignment The temporal alignment is to transform the 3D coordinates of frame t ‚àí 1 to the coordinate system of frame t (see Fig. 2(a)). For clarity, we first denote some coordinate systems: camera coordinate as c(t), lidar coordinate as l(t), and ego coordinate as e(t) at frame t. What's more, global coordinates as g. We define T dst src as the transformation matrix from the source coordinate system to the target coordinate system.\\nWe use l(t) as the default 3D space for multi-view camera 3D position-aware feature generation. The 3D points P l(t) i (t) projected from i-th camera can be formulated as:\\nP l(t) i (t) = T l(t) ci(t) K ‚àí1 i P m (t)(1)\\nwhere P m (t) is the points set in the meshgrid of camera frustum space at frame t. K i ‚àà R 4√ó4 is the camera intrinsic matrix of the i-th camera. Given the auxiliary frame t ‚àí 1, we align the coordinates of 3D points from frame t ‚àí 1 to frame t:\\nP l(t) i (t ‚àí 1) = T l(t) l(t‚àí1) P l(t‚àí1) i (t ‚àí 1)(2)\\nWith global coordinate space acting as a bridge between frame t ‚àí 1 and frame t, T l(t) l(t‚àí1) can be easily calculated:\\nT l(t) l(t‚àí1) = T l(t) e(t) T e(t) g T e(t‚àí1) g ‚àí1 T l(t‚àí1) e(t‚àí1) ‚àí1 (3) The aligned point sets [P l(t) i (t ‚àí 1), P l(t) i\\n(t)] will be used to generate the 3D position embedding, as described below.\\nFeature-guided Position Encoder PETR [21] transforms the 3D coordinates into 3D position embedding (3D PE). The generation of 3D position embedding can be formulated as:\\nP E 3d i (t) = œà(P l(t) i (t))(4)\\nwhere œà(.) is a simple multi-layer perception (MLP). The 3D PE in PETR is independent with the input image. We argue that the 3D PE should be driven by the 2D features since the image feature can provide some informative guidance (e.g., depth). In this paper, we propose a feature-guided position encoder, which implicitly introduces vision prior. The generation of feature-guided 3D position embedding can be formulated as:\\nP E 3d i (t) = Œæ(F i (t)) * œà(P l(t) i (t)) (5\\n)\\nwhere Œæ is also a small MLP network. F i (t) is the 2D image features of the i-th camera. As illustrated in Fig. 2(b), the 2D image features projected by a 1 √ó 1 convolution are fed into a small MLP network Figure 3: The architecture of BEV segmentation branch. The seg queries, uniformly initialized by the anchor points in BEV space, update the representation by transformer decoder. The updated queries are further input to the segmentation head to predict the BEV map. R is the reshape operation.\\nŒæ and Sigmoid function to obtain the attention weights. The 3D coordinates are transformed by another MLP network œà and multiplied with the attention weights to generate the 3D PE. The 3D PE is added with 2D features to obtain the key value for transformer decoder. The projected 2D features are used as the value component for transformer decoder.\",\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images',\n",
       "  'abstract': 'In this paper, we propose PETRv2, a unified framework for 3D perception from multi-view images. Based on PETR [21], PETRv2 explores the effectiveness of temporal modeling, which utilizes the temporal information of previous frames to boost 3D object detection. More specifically, we extend the 3D position embedding (3D PE) in PETR for temporal modeling. The 3D PE achieves the temporal alignment on object position of different frames. A feature-guided position encoder is further introduced to improve the data adaptability of 3D PE. To support for high-quality BEV segmentation, PETRv2 provides a simply yet effective solution by adding a set of segmentation queries. Each segmentation query is responsible for segmenting one specific patch of BEV map. PETRv2 achieves state-of-the-art performance on 3D object detection and BEV segmentation. Detailed robustness analysis is also conducted on PETR framework. We hope PETRv2 can serve as a strong baseline for 3D perception.',\n",
       "  'paddleOCR': [[[[829.0, 36.0], [875.0, 36.0], [875.0, 66.0], [829.0, 66.0]],\n",
       "    ['2D', 0.9996962547302246]],\n",
       "   [[[238.0, 65.0], [319.0, 65.0], [319.0, 94.0], [238.0, 94.0]],\n",
       "    ['object', 0.9994997382164001]],\n",
       "   [[[797.0, 72.0], [906.0, 78.0], [905.0, 105.0], [796.0, 100.0]],\n",
       "    ['Features', 0.9998056888580322]],\n",
       "   [[[1402.0, 73.0], [1469.0, 73.0], [1469.0, 97.0], [1402.0, 97.0]],\n",
       "    ['value', 0.9996017217636108]],\n",
       "   [[[1217.0, 99.0], [1259.0, 99.0], [1259.0, 116.0], [1217.0, 116.0]],\n",
       "    ['X', 0.5315845608711243]],\n",
       "   [[[1216.0, 163.0], [1270.0, 167.0], [1267.0, 198.0], [1213.0, 193.0]],\n",
       "    ['relu', 0.9982614517211914]],\n",
       "   [[[1205.0, 241.0], [1264.0, 241.0], [1264.0, 265.0], [1205.0, 265.0]],\n",
       "    ['1 x ]', 0.877568244934082]],\n",
       "   [[[1191.0, 310.0], [1294.0, 310.0], [1294.0, 344.0], [1191.0, 344.0]],\n",
       "    ['sigmoid', 0.9998989701271057]],\n",
       "   [[[825.0, 364.0], [870.0, 364.0], [870.0, 394.0], [825.0, 394.0]],\n",
       "    ['3D', 0.9998011589050293]],\n",
       "   [[[1029.0, 365.0], [1053.0, 365.0], [1053.0, 418.0], [1029.0, 418.0]],\n",
       "    ['relu', 0.9988822340965271]],\n",
       "   [[[1112.0, 393.0], [1123.0, 393.0], [1123.0, 401.0], [1112.0, 401.0]],\n",
       "    ['X', 0.5807428956031799]],\n",
       "   [[[645.0, 407.0], [696.0, 407.0], [696.0, 430.0], [645.0, 430.0]],\n",
       "    ['t - 1', 0.8442445993423462]],\n",
       "   [[[773.0, 404.0], [923.0, 404.0], [923.0, 431.0], [773.0, 431.0]],\n",
       "    ['Coordinates', 0.9999454021453857]],\n",
       "   [[[1269.0, 406.0], [1352.0, 406.0], [1352.0, 433.0], [1269.0, 433.0]],\n",
       "    ['3D PE', 0.9613739848136902]],\n",
       "   [[[354.0, 486.0], [393.0, 486.0], [393.0, 514.0], [354.0, 514.0]],\n",
       "    ['(a)', 0.9982144832611084]],\n",
       "   [[[1101.0, 480.0], [1150.0, 480.0], [1150.0, 512.0], [1101.0, 512.0]],\n",
       "    ['(b)', 0.9991452693939209]]],\n",
       "  'ocr': [[[1269.0, 406.0], [1352.0, 406.0], [1352.0, 433.0], [1269.0, 433.0]],\n",
       "   ['3D PE', 0.9613739848136902]]},\n",
       " '1360152-Figure1-1.png': {'caption': 'Figure 1. Interactive 3D modeling with a GAN. The user iteratively makes edits to a voxel grid with a simple painting interface and then hits a SNAP command to refine the current shape. The SNAP command projects the current shape into a latent vector shape manifold learned with a GAN, and then generates a new shape with the generator network. SNAP aims to increase the realism of the user‚Äôs input, while maintaining similarity.',\n",
       "  'imageText': [],\n",
       "  'image_file': '1360152-Figure1-1.png',\n",
       "  'sections': [{'heading': 'I. INTRODUCTION',\n",
       "    'text': 'There has been growing demand in recent years for interactive tools that allow novice users to create new 3D models of their own designs. Minecraft for example, has sold over 120 million copies, up from 20 million just two years ago.\\nYet 3D modeling is difficult for novice users. Current modeling systems provide either a simple user interface suitable for novices (e.g., [15], [23]) or the ability to make arbitrary 3D models with the details and complexity of realworld objects (e.g., [3], [2]). Achieving both is an open and fundamental research challenge.\\nIn this paper, we investigate how to use Generative Adversarial Networks (GANs) [12] to help novices create realistic 3D models of their own designs using a simple interactive modeling tool. 3D GANs have recently been proposed for generating distributions of 3D voxel grids representing a class of objects [30]. Given a latent vector (e.g., a 200-dimensional vector with random values), a 3D-GAN can produce a sample from a latent distribution of voxel grids learned from examples (see the right side of Figure 1). Previous work has used 3D GANs for object classification, shape interpolation, and generating random shapes [30]. However, they have never before been used for interactive 3D modeling; nor has any other generative deep network. An important limitation with GANs in general has been that while certain subspaces on the manifold generate realistic outputs, there are inherently in-between spaces that contain unrealistic outputs (discussed in Section III).\\nWe propose a model framework around a 3D-GAN which helps hide its weaknesses and allow novice users to easily Figure 1. Interactive 3D modeling with a GAN. The user iteratively makes edits to a voxel grid with a simple painting interface and then hits a SNAP command to refine the current shape. The SNAP command projects the current shape into a latent vector shape manifold learned with a GAN, and then generates a new shape with the generator network. SNAP aims to increase the realism of the user\\'s input, while maintaining similarity. perform interactive modeling, constraining the output to feasible and realistic shapes. The user iteratively paints voxels with a simple interface similar to Minecraft [23] and then hits the \"SNAP\" button, which replaces the current voxel grid with a similar one generated by a 3D GAN.\\nOur approach is fueled by insights about the disjoint subspaces on the GAN manifold that contain realistic outputs. While there have been various approaches toward a projecting an input into the latent space of a GAN [19], [35], ours is the first to ensure that the generated output is similar in shape to the input but constrained to the \"good\" spaces of the manifold. This ensures that users are able to generate realistic looking inputs using our GAN framework. The main challenge in implementing such a system is designing this projection operator P (x) from a user-provided 3D voxel grid x to a feature vector z in the latent space of a 3D-GAN (Figure 1). With such an operator, each SNAP operator can map x to x = G(P (x)), ideally producing an output x that is not only similar to the input but also representative of real-world objects in a given training set. We integrate this operator into an interactive modeling tool and demonstrate the effectiveness of the resulting SNAP command in several typical novice editing sessions. Figure 2 depicts an example workflow of this proposed approach. At the beginning, the user sketches the rough shape of an office chair (leftmost panel). When he/she hits the SNAP button, the system fills in the details of a similar chair generated with a 3D GAN (second panel). Then the user removes voxels corresponding to the top half of the back, which snaps to a new chair with a lower-back, and then the user truncates the legs of the school chair, which then Figure 2. A typical editing sequence. The user alternates between painting voxels (dotted arrows) and executing SNAP commands (solid arrows). For each SNAP, the system projects the current shape into a shape manifold learned with a GAN (depicted in blue) and synthesizes a new shape with a generator network.\\nsnaps to a lounge chair with a low base (note that the back becomes reclined to accommodate the short legs). In each case, the user provides approximate inputs with a simple interface, and the system generates a new shape sampled from a continuous distribution.\\nThe contributions of the paper are four-fold. First, it is the first to utilize a GAN in an interactive 3D model editing tool. Second, it proposes a novel way to project an arbitrary input into the latent space of a GAN, balancing both similarity to the input shape and realism of the output shape. Third, it provides a dataset of 3D polygonal models comprised of 101 object classes each with at least 120 examples in each class, which is the largest, consistently-oriented 3D dataset to date. Finally, it provides a simple interactive modeling tool for novice users.',\n",
       "    'n_publication_ref': 10,\n",
       "    'n_figure_ref': 5}],\n",
       "  'title': 'Interactive 3D Modeling with a Generative Adversarial Network',\n",
       "  'abstract': 'We propose the idea of using a generative adversarial network (GAN) to assist users in designing realworld shapes with a simple interface. Users edit a voxel grid with a Minecraft-like interface. Yet they can execute a SNAP command at any time, which transforms their rough model into a desired shape that is both similar and realistic. They can edit and snap until they are satisfied with the result. The advantage of this approach is to assist novice users to create 3D models characteristic of the training data by only specifying rough edits. Our key contribution is to create a suitable projection operator around a 3D-GAN that maps an arbitrary 3D voxel input to a latent vector in the shape manifold of the generator that is both similar in shape to the input but also realistic. Experiments show our method is promising for computer-assisted interactive modeling.',\n",
       "  'paddleOCR': [[[[455.0, 17.0], [520.0, 17.0], [520.0, 40.0], [455.0, 40.0]],\n",
       "    ['SNAP', 0.9993113875389099]],\n",
       "   [[[513.0, 93.0], [627.0, 93.0], [627.0, 108.0], [513.0, 108.0]],\n",
       "    ['Latent Vector z', 0.9453399777412415]],\n",
       "   [[[270.0, 137.0], [349.0, 139.0], [348.0, 158.0], [269.0, 155.0]],\n",
       "    ['Projection', 0.9988981485366821]],\n",
       "   [[[589.0, 142.0], [737.0, 143.0], [737.0, 161.0], [589.0, 159.0]],\n",
       "    ['Generator Network', 0.978197455406189]],\n",
       "   [[[267.0, 159.0], [352.0, 159.0], [352.0, 178.0], [267.0, 178.0]],\n",
       "    ['Operator P', 0.9985318183898926]],\n",
       "   [[[655.0, 165.0], [670.0, 165.0], [670.0, 181.0], [655.0, 181.0]],\n",
       "    ['G', 0.9964704513549805]],\n",
       "   [[[327.0, 246.0], [645.0, 245.0], [645.0, 265.0], [327.0, 266.0]],\n",
       "    ['Latent Space of Shape Manifold', 0.999718427658081]],\n",
       "   [[[819.0, 242.0], [966.0, 245.0], [965.0, 269.0], [818.0, 266.0]],\n",
       "    ['Output G(P(x))', 0.9998860359191895]],\n",
       "   [[[61.0, 259.0], [131.0, 259.0], [131.0, 281.0], [61.0, 281.0]],\n",
       "    ['Input x', 0.9891137480735779]],\n",
       "   [[[389.0, 319.0], [594.0, 319.0], [594.0, 336.0], [389.0, 336.0]],\n",
       "    ['Additional User Edits', 0.9555981159210205]]],\n",
       "  'ocr': [[[513.0, 93.0], [627.0, 93.0], [627.0, 108.0], [513.0, 108.0]],\n",
       "   ['Latent Vector z', 0.9453399777412415]]},\n",
       " '2102.08307v1-Figure1-1.png': {'caption': 'Fig. 1. Flowchart of the ATA-RIA algorithm. On receiving a composite task, an agent can carry out ùê∏ùëãùê∏ùê∂ or ùëÉùëÖùëÇùëâ ùêºùê∑ùê∏_ùêºùëÅùêπùëÇ actions immediately, or will choose amongst ùê¥ùêøùêøùëÇùê∂ , ùêºùëÅùêπùëÇ and ùêøùêºùëÅùêæ using the RT-ARP algorithm. Taking an ùêºùëÅùêπùëÇ or ùêøùêºùëÅùêæ action will lead to knowledge removal through the SAS-KR algorithm or neighbourhood pruning through the N-Prune algorithm respectively.',\n",
       "  'imageText': ['|ùëÅ(ùëî)|',\n",
       "   '>',\n",
       "   'ùõøùëõ(ùëî)?',\n",
       "   'No',\n",
       "   'Yes',\n",
       "   'Decide',\n",
       "   'info',\n",
       "   'to',\n",
       "   'remove',\n",
       "   '(SAS-KR)',\n",
       "   'PROVIDE_INFO',\n",
       "   'Decide',\n",
       "   'link',\n",
       "   'to',\n",
       "   'remove',\n",
       "   '(N-prune)',\n",
       "   'Decide',\n",
       "   'next',\n",
       "   'action',\n",
       "   '(RT-ARP)',\n",
       "   'Alloc',\n",
       "   'action',\n",
       "   'Info',\n",
       "   'action',\n",
       "   'action',\n",
       "   'REMOVE_LINK',\n",
       "   'Link',\n",
       "   'REMOVE_INFO',\n",
       "   'LINK',\n",
       "   'INFO',\n",
       "   'ALLOC',\n",
       "   '‚àÉ<{ùëñùëõùëìùëú},',\n",
       "   'ùë°,ùëî',\n",
       "   '>',\n",
       "   '‚ààùêøùëî',\n",
       "   'No',\n",
       "   'Yes',\n",
       "   'EXEC',\n",
       "   'Yes‚àÉ<{ùëéùë°},',\n",
       "   'ùë°,ùëî>‚ààùêøùëî',\n",
       "   ':',\n",
       "   'ùëéùë°',\n",
       "   '‚àà',\n",
       "   'ùëê(ùëî)',\n",
       "   'No',\n",
       "   'Retrieve',\n",
       "   'tasks',\n",
       "   'allocated',\n",
       "   'to',\n",
       "   'g',\n",
       "   'ùêøùëî'],\n",
       "  'image_file': '2102.08307v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'ALGORITHMS FOR OPTIMAL TASK ALLOCATION',\n",
       "    'text': 'We now give a high-level introduction to our algorithms for solving the task-allocation problem. The concepts and notation will be covered in more depth in Section 5.\\n‚Ä¢ The agent task allocation with risk-impact awareness (ATA-RIA) algorithm learns to take actions to optimise the task-allocation problem described. Its main purpose is to integrate the following three algorithms, as well as updating Q-values and sample data. It also makes action selections based on measured progress towards composite task completion. (See Figure 1). ‚Ä¢ The reward trends for action-risks probabilities (RT-ARP) algorithm increases the probability of an agent taking neighbourhood-altering actions and increasing exploration when the possible optimal allocation achievable in its current neighbourhood is relatively poor compared to previous neighbourhoods.\\n‚Ä¢ The state-action space knowledge-retention (SAS-KR) algorithm implements a knowledge retention scheme under dynamic neighbourhood changes. This removes parts of an agents knowledge less relevant to the optimisation problem so the agent can stay within resource bounds.\\n‚Ä¢ The neighbourhood update (N-Prune) algorithm maintains an agents neighbourhood within resource constraints by removing information on child agents based on their recent relative contribution to task completion quality.\\nIn these algorithms we utilise some standard functions which we summarise in Table 1.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Dynamic neighbourhood optimisation for task allocation using multi-agent learning',\n",
       "  'abstract': \"In large-scale systems there are fundamental challenges when centralised techniques are used for task allocation. The number of interactions is limited by resource constraints such as on computation, storage, and network communication. We can increase scalability by implementing the system as a distributed task-allocation system, sharing tasks across many agents. However, this also increases the resource cost of communications and synchronisation, and is difficult to scale. In this paper we present four algorithms to solve these problems. The combination of these algorithms enable each agent to improve their task allocation strategy through reinforcement learning, while changing how much they explore the system in response to how optimal they believe their current strategy is, given their past experience. We focus on distributed agent systems where the agents' behaviours are constrained by resource usage limits, limiting agents to local rather than system-wide knowledge. We evaluate these algorithms in a simulated environment where agents are given a task composed of multiple subtasks that must be allocated to other agents with differing capabilities, to then carry out those tasks. We also simulate real-life system effects such as networking instability. Our solution is shown to solve the task allocation problem to 6.7% of the theoretical optimal within the system configurations considered. It provides 5√ó better performance recovery over no-knowledge retention approaches when system connectivity is impacted, and is tested against systems up to 100 agents with less than a 9% impact on the algorithms' performance.\",\n",
       "  'paddleOCR': [[[[41.0, 16.0], [201.0, 20.0], [200.0, 46.0], [41.0, 42.0]],\n",
       "    ['Retrieve tasks.', 0.9582566618919373]],\n",
       "   [[[45.0, 42.0], [196.0, 48.0], [194.0, 75.0], [44.0, 69.0]],\n",
       "    ['allocated to g', 0.9999271035194397]],\n",
       "   [[[72.0, 111.0], [101.0, 123.0], [91.0, 145.0], [62.0, 132.0]],\n",
       "    ['Lg', 0.9754018783569336]],\n",
       "   [[[76.0, 215.0], [155.0, 222.0], [153.0, 248.0], [74.0, 241.0]],\n",
       "    ['3<{at},', 0.9725221991539001]],\n",
       "   [[[255.0, 233.0], [295.0, 239.0], [292.0, 259.0], [252.0, 253.0]],\n",
       "    ['Yes', 0.9981730580329895]],\n",
       "   [[[68.0, 243.0], [174.0, 249.0], [171.0, 282.0], [66.0, 276.0]],\n",
       "    ['t,g>ELg:', 0.9966622591018677]],\n",
       "   [[[359.0, 254.0], [430.0, 254.0], [430.0, 280.0], [359.0, 280.0]],\n",
       "    ['EXEC', 0.9997238516807556]],\n",
       "   [[[68.0, 279.0], [170.0, 284.0], [168.0, 309.0], [67.0, 305.0]],\n",
       "    ['(6)53 7t', 0.625021755695343]],\n",
       "   [[[65.0, 371.0], [99.0, 371.0], [99.0, 393.0], [65.0, 393.0]],\n",
       "    ['No', 0.9997832775115967]],\n",
       "   [[[69.0, 466.0], [169.0, 466.0], [169.0, 492.0], [69.0, 492.0]],\n",
       "    ['3<{inf0},', 0.9816371202468872]],\n",
       "   [[[298.0, 468.0], [338.0, 468.0], [338.0, 490.0], [298.0, 490.0]],\n",
       "    ['Yes', 0.9998977780342102]],\n",
       "   [[[68.0, 490.0], [173.0, 494.0], [172.0, 527.0], [66.0, 523.0]],\n",
       "    ['t,g>ELg', 0.9992070198059082]],\n",
       "   [[[389.0, 488.0], [604.0, 488.0], [604.0, 512.0], [389.0, 512.0]],\n",
       "    ['PROVIDE_INFO', 0.9798722267150879]],\n",
       "   [[[414.0, 577.0], [477.0, 582.0], [475.0, 608.0], [412.0, 603.0]],\n",
       "    ['Alloc', 0.9991142153739929]],\n",
       "   [[[409.0, 613.0], [480.0, 613.0], [480.0, 633.0], [409.0, 633.0]],\n",
       "    ['action', 0.998558521270752]],\n",
       "   [[[64.0, 636.0], [100.0, 641.0], [96.0, 663.0], [60.0, 657.0]],\n",
       "    ['No', 0.9972268342971802]],\n",
       "   [[[677.0, 635.0], [760.0, 635.0], [760.0, 660.0], [677.0, 660.0]],\n",
       "    ['ALLOC', 0.9988394975662231]],\n",
       "   [[[919.0, 755.0], [1041.0, 755.0], [1041.0, 776.0], [919.0, 776.0]],\n",
       "    ['Decide info', 0.9937648773193359]],\n",
       "   [[[21.0, 770.0], [225.0, 774.0], [225.0, 799.0], [20.0, 795.0]],\n",
       "    ['Decide next action', 0.9922789335250854]],\n",
       "   [[[422.0, 766.0], [470.0, 771.0], [466.0, 796.0], [419.0, 791.0]],\n",
       "    [' Info', 0.9061578512191772]],\n",
       "   [[[686.0, 779.0], [749.0, 779.0], [749.0, 805.0], [686.0, 805.0]],\n",
       "    ['INFO', 0.9947795867919922]],\n",
       "   [[[924.0, 781.0], [1036.0, 781.0], [1036.0, 807.0], [924.0, 807.0]],\n",
       "    ['to remove', 0.9998696446418762]],\n",
       "   [[[1223.0, 779.0], [1400.0, 779.0], [1400.0, 805.0], [1223.0, 805.0]],\n",
       "    ['REMOVE_INFO', 0.9949942231178284]],\n",
       "   [[[65.0, 801.0], [175.0, 801.0], [175.0, 827.0], [65.0, 827.0]],\n",
       "    ['(RT-ARP)', 0.999255359172821]],\n",
       "   [[[415.0, 799.0], [478.0, 799.0], [478.0, 819.0], [415.0, 819.0]],\n",
       "    ['action', 0.9987451434135437]],\n",
       "   [[[928.0, 812.0], [1034.0, 812.0], [1034.0, 832.0], [928.0, 832.0]],\n",
       "    ['(SAS-KR)', 0.9320405721664429]],\n",
       "   [[[1147.0, 838.0], [1183.0, 838.0], [1183.0, 860.0], [1147.0, 860.0]],\n",
       "    ['No', 0.9998154640197754]],\n",
       "   [[[1354.0, 889.0], [1478.0, 889.0], [1478.0, 915.0], [1354.0, 915.0]],\n",
       "    ['Decide link', 0.9997372031211853]],\n",
       "   [[[688.0, 918.0], [749.0, 918.0], [749.0, 944.0], [688.0, 944.0]],\n",
       "    ['LINK', 0.994852602481842]],\n",
       "   [[[1060.0, 918.0], [1203.0, 918.0], [1203.0, 940.0], [1060.0, 940.0]],\n",
       "    ['N(g)l> 8n(g)', 0.9273836016654968]],\n",
       "   [[[1356.0, 916.0], [1473.0, 920.0], [1472.0, 946.0], [1355.0, 942.0]],\n",
       "    ['to remove', 0.9996557235717773]],\n",
       "   [[[1588.0, 918.0], [1768.0, 918.0], [1768.0, 942.0], [1588.0, 942.0]],\n",
       "    ['REMOVE LINK', 0.9982797503471375]],\n",
       "   [[[420.0, 944.0], [474.0, 944.0], [474.0, 971.0], [420.0, 971.0]],\n",
       "    ['Link', 0.9997434616088867]],\n",
       "   [[[1255.0, 946.0], [1294.0, 946.0], [1294.0, 968.0], [1255.0, 968.0]],\n",
       "    ['Yes', 0.9998460412025452]],\n",
       "   [[[1361.0, 947.0], [1471.0, 947.0], [1471.0, 973.0], [1361.0, 973.0]],\n",
       "    ['(N-prune)', 0.9716476798057556]],\n",
       "   [[[415.0, 977.0], [480.0, 977.0], [480.0, 994.0], [415.0, 994.0]],\n",
       "    ['action', 0.9979686737060547]]],\n",
       "  'ocr': [[[420.0, 944.0], [474.0, 944.0], [474.0, 971.0], [420.0, 971.0]],\n",
       "   ['Link', 0.9997434616088867]]},\n",
       " '2010.03465v1-Figure1-1.png': {'caption': 'Figure 1: System Model',\n",
       "  'imageText': ['al',\n",
       "   's',\n",
       "   'te',\n",
       "   'rv',\n",
       "   'e',\n",
       "   'in',\n",
       "   'Ti',\n",
       "   'm',\n",
       "   'Client',\n",
       "   'Server',\n",
       "   'TagsEncrypt',\n",
       "   'and',\n",
       "   'generate',\n",
       "   'token',\n",
       "   'Evaluate',\n",
       "   'token',\n",
       "   'in',\n",
       "   'search',\n",
       "   'index',\n",
       "   'Œ≥1',\n",
       "   'Œ≥3',\n",
       "   'Œ≥2',\n",
       "   'Œ≥1',\n",
       "   'Observed',\n",
       "   'access',\n",
       "   'patterns',\n",
       "   'w12',\n",
       "   'w51',\n",
       "   'w23',\n",
       "   'w12',\n",
       "   'Queries',\n",
       "   '1',\n",
       "   '2',\n",
       "   'œÅ'],\n",
       "  'image_file': '2010.03465v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'System Model and Notation',\n",
       "    'text': \"We present a general model that captures the leakage of many proposed privacy-preserving SSE schemes while abstracting from the cryptographic and implementation details of these protocols. The notation that we use is summarized in Table 1. We use upper-case boldface characters to denote matrices and lower-case boldface characters to denote vectors. The (i, j)th entry of matrix A is (A) i, j , and tr(A) is the trace of A. We represent the natural logarithm as log; other logarithm bases are written explicitly.\\nLet ‚àÜ = [w 1 , w 2 , . . . , w n ] be the keyword universe, where w i is the ith keyword, and let n . = |‚àÜ| be the total number Auxiliary (Background) Informatio√± v i Auxiliary volume information for keyword w i . v Volume vector of keywords,·πΩ\\n.\\n= [·πΩ 1 , . . . ,·πΩ n ]. M Auxiliary keyword co-occurrence matrix (n √ó n). f i,k Query frequency of w i in the kth time interval. f i Query frequency vector of w i ,f i . = [f i,1 , . . . ,f i,œÅ\\n]. F Query frequency matrix of all keywords (size n √ó œÅ).\\nAttack Goal p( j) Index of the keyword that the attack assigns to Œ≥ j .\\nP Permutation matrix, P p( j), j = 1, else 0 (n √ó m).\\nTable 1: Summary of notation of keywords. Let N D be the number of documents in the encrypted database that the client sends to the server. For each query, the adversary observes the tuple (t, a) where t is the timestamp of the query and a is the access pattern, i.e., a vector with the positions of the documents that match the query. The leakage of all the SSE schemes that we consider in this work can be characterized by a sequence of tuples (t, a). We use |a| to denote the response volume, i.e., the number of documents returned to the client in response to a query. We consider SSE schemes that leak the search pattern, i.e., they leak which queries within a sequence are for the same keyword. The search pattern leakage can be explicit or implicit. Explicit search pattern occurs when querying for a certain keyword always generates the same query token [4,6,24]. Implicit leakage refers to SSE schemes where the queries for the same keyword w i always generate the same access pattern a, and the adversary can compare access patterns to check whether or not different tokens aim for the same keyword [7]. We discuss how to hide search patterns in Section 7.\\nUsing the search pattern leakage, the adversary can assign a tag to each different access pattern it observes. The number of tags m will be at most equal to the number of keywords n (i.e., Then, the goal of the query recovery attack is to assign each tag its correct keyword. We denote this assignment, which is an injective mapping, by p(\\n‚Ä¢) : [m] ‚Üí [n]\\n. We also represent it in matricial form as a (n √ó m) permutation (column-selection) matrix that we denote by P and define as\\n(P) i, j = 1 , if i = p( j) , 0 , otherwise.(1)\\nFigure 1 illustrates this model and notation. In the figure, the client queries for keywords w 12 , w 23 , w 51 , . . . , w 12 . The server evaluates the query tokens in the search index and obtains which documents in the encrypted database match each query (i.e., the observed access patterns). Then, the server assigns a tag Œ≥ j to each distinct access pattern. Note that the access patterns that result from evaluating different query tokens generated from the same keyword (e.g., w 12 ) are identical. The goal of the attack is to map each Œ≥ j to a keyword w i . In order to perform this mapping, the server uses information from the structure of the access patterns and from the frequency with which the server observes each access pattern, as well as some auxiliary information that we specify below.\\nBelow, we define different data structures that the adversary can compute from the observations. Several query recovery attacks [15,22,26], as well as our proposal, can be defined by using these variables. The following structures are computed from the access patterns:\\n‚Ä¢ Query volume (v, v j ). The query volume refers to the number of documents in the database that are returned as a response to a certain query. We use v j ‚àà [0, 1] to denote the normalized volume of the jth tag, i.e., v j . = |a j |/N D , and v\\n.\\n= [v 1 , . . . , v m ].\\n‚Ä¢ Co-occurence matrix (M). This variable refers to the number of documents that simultaneously match two dif-ferent queries, normalized by the total number of documents in the database. We use M to denote the symmetric matrix whose (i, j)th element is (M) i, j .\\n= |a i ‚à© a j |/N D ‚àà [0, 1].\\nThe following structures are computed from the search patterns, i.e., from how many times the client sends a query tagged as Œ≥ j . In order to compute these structures, the adversary first splits the observation time into œÅ intervals (e.g., weeks).\\n‚Ä¢ Query number (Œ∑ Œ∑ Œ∑, Œ∑ k ). We use Œ∑ k to denote the number of queries the client sent in the kth interval, and define the vector Œ∑ Œ∑ Œ∑ . = [Œ∑ 1 , . . . , Œ∑ œÅ ].\\n‚Ä¢ Query frequency (F, f j , f j,k ). The query frequency refers to how often the client performs a certain query.\\nFor each tag Œ≥ j ( j ‚àà [m]) and each time interval, indexed by k ‚àà [œÅ], we use f j,k to denote the frequency of tag j in the kth interval, i.e., the total number of times the client queries for tag j in the interval, divided by the total number of queries in that interval. We use f j to denote the vector that stores f j,k for all k ‚àà [œÅ] and F is the (m √ó œÅ) matrix that stores all the frequencies.\\nIn addition to the observations, the adversary has certain auxiliary background information (e.g., a training set) that helps them carrying out the query recovery attack. The adversary uses this information to compute data structures like the ones defined above, but for each keyword instead of each tag. We denote the auxiliary query volume information by·πΩ i for each keyword i ‚àà [n], the n √ó n co-occurrence matrix of keywords byM, and the n √ó œÅ matrix storing the query trends of each keyword byF. We note that background information is a strong assumption and attacks that rely on high-quality auxiliary information to be effective might be unrealistic [2]. In our evaluation in Section 6, we show that our attack is strong under weak assumptions on the auxiliary information. Namely, in our experiments the adversary computes·πΩ and M using a training set that is disjoint with the actual client's database, andF using public information about query trends with a time offset.\\nBelow, we explain state-of-the-art query recovery attacks using access pattern [26] and search pattern [22] leakage using our notation.\",\n",
       "    'n_publication_ref': 10,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Hiding the Access Pattern is Not Enough: Exploiting Search Pattern Leakage in Searchable Encryption',\n",
       "  'abstract': \"Recent Searchable Symmetric Encryption (SSE) schemes enable secure searching over an encrypted database stored in a server while limiting the information leaked to the server. These schemes focus on hiding the access pattern, which refers to the set of documents that match the client's queries. This provides protection against current attacks that largely depend on this leakage to succeed. However, most SSE constructions also leak whether or not two queries aim for the same keyword, also called the search pattern. In this work, we show that search pattern leakage can severely undermine current SSE defenses. We propose an attack that leverages both access and search pattern leakage, as well as some background and query distribution information, to recover the keywords of the queries performed by the client. Our attack follows a maximum likelihood estimation approach, and is easy to adapt against SSE defenses that obfuscate the access pattern. We empirically show that our attack is efficient, it outperforms other proposed attacks, and it completely thwarts two out of the three defenses we evaluate it against, even when these defenses are set to high privacy regimes. These findings highlight that hiding the search pattern, a feature that most constructions are lacking, is key towards providing practical privacy guarantees in SSE.\",\n",
       "  'paddleOCR': [[[[265.0, 2.0], [415.0, 2.0], [415.0, 27.0], [265.0, 27.0]],\n",
       "    ['Encrypt and', 0.9998924136161804]],\n",
       "   [[[471.0, 3.0], [652.0, 3.0], [652.0, 25.0], [471.0, 25.0]],\n",
       "    ['Evaluate token', 0.9670692086219788]],\n",
       "   [[[729.0, 0.0], [847.0, 2.0], [847.0, 25.0], [728.0, 22.0]],\n",
       "    ['Observed', 0.9980926513671875]],\n",
       "   [[[117.0, 13.0], [214.0, 13.0], [214.0, 42.0], [117.0, 42.0]],\n",
       "    ['Queries', 0.9991490244865417]],\n",
       "   [[[918.0, 8.0], [982.0, 15.0], [979.0, 46.0], [915.0, 39.0]],\n",
       "    ['Tags', 0.9998671412467957]],\n",
       "   [[[248.0, 42.0], [430.0, 37.0], [431.0, 61.0], [249.0, 67.0]],\n",
       "    ['generate token', 0.9878836274147034]],\n",
       "   [[[468.0, 42.0], [654.0, 42.0], [654.0, 64.0], [468.0, 64.0]],\n",
       "    ['in search index', 0.9997466206550598]],\n",
       "   [[[694.0, 42.0], [880.0, 40.0], [880.0, 65.0], [694.0, 67.0]],\n",
       "    ['access patterns', 0.9997307062149048]],\n",
       "   [[[144.0, 108.0], [190.0, 113.0], [188.0, 132.0], [142.0, 128.0]],\n",
       "    ['W12', 0.9917895793914795]],\n",
       "   [[[934.0, 107.0], [960.0, 107.0], [960.0, 133.0], [934.0, 133.0]],\n",
       "    ['Y1', 0.9979752898216248]],\n",
       "   [[[143.0, 189.0], [190.0, 193.0], [188.0, 214.0], [141.0, 209.0]],\n",
       "    ['W23', 0.9901561141014099]],\n",
       "   [[[933.0, 187.0], [963.0, 187.0], [963.0, 216.0], [933.0, 216.0]],\n",
       "    ['Y2', 0.9989691972732544]],\n",
       "   [[[3.0, 248.0], [25.0, 248.0], [25.0, 415.0], [3.0, 415.0]],\n",
       "    ['ime intervals', 0.9977420568466187]],\n",
       "   [[[143.0, 266.0], [190.0, 273.0], [187.0, 296.0], [139.0, 289.0]],\n",
       "    ['W51', 0.9102388024330139]],\n",
       "   [[[933.0, 267.0], [963.0, 267.0], [963.0, 295.0], [933.0, 295.0]],\n",
       "    ['Y3', 0.9987308979034424]],\n",
       "   [[[67.0, 290.0], [88.0, 290.0], [88.0, 316.0], [67.0, 316.0]],\n",
       "    ['2', 0.9996408224105835]],\n",
       "   [[[143.0, 440.0], [190.0, 445.0], [188.0, 466.0], [141.0, 462.0]],\n",
       "    ['W12', 0.9875643253326416]],\n",
       "   [[[938.0, 443.0], [960.0, 443.0], [960.0, 464.0], [938.0, 464.0]],\n",
       "    ['Y1', 0.9964184761047363]],\n",
       "   [[[68.0, 471.0], [87.0, 471.0], [87.0, 493.0], [68.0, 493.0]],\n",
       "    ['d', 0.9704261422157288]],\n",
       "   [[[234.0, 511.0], [313.0, 511.0], [313.0, 538.0], [234.0, 538.0]],\n",
       "    ['Client', 0.9994681477546692]],\n",
       "   [[[715.0, 512.0], [799.0, 512.0], [799.0, 537.0], [715.0, 537.0]],\n",
       "    ['Server', 0.9992365837097168]]],\n",
       "  'ocr': [[[234.0, 511.0], [313.0, 511.0], [313.0, 538.0], [234.0, 538.0]],\n",
       "   ['Client', 0.9994681477546692]]},\n",
       " '2102.02111v2-Figure6-1.png': {'caption': 'Figure 6. Attention Mechanism in the Transformer. Illustration of the attention mechanism in the first Transformer encoder for the 8th token (‚Äòit‚Äô) in the example sentence ‚ÄòThe company is issuing a statement as it is bankrupt.‚Äô. The arrows pointing from the value vectors (v1, . . . ,v11) to context vector c8 are the weights (Œ±8,1, . . . , Œ±8,t‚àó , . . . , Œ±8,11). A single weight Œ±8,t‚àó indicates the contribution of token t‚àó to the representation of token 8, c8. The larger Œ±8,t‚àó is assumed to be in this example, the thicker the arrow and the darker the corresponding value vector. The dotted lines symbolize the computation of the weights (Œ±8,1, . . . , Œ±8,t‚àó , . . . , Œ±8,11).',\n",
       "  'imageText': ['9‚Ñé%',\n",
       "   '<?@.+,>',\n",
       "   \"&'\",\n",
       "   \"&''=&,-\",\n",
       "   '+',\n",
       "   \"'*+*%@%,*\",\n",
       "   \"+'\",\n",
       "   '&*',\n",
       "   \"&'\",\n",
       "   ';+,5)=.*',\n",
       "   '[(12]',\n",
       "   '\\'(,\"',\n",
       "   \"'(,$\",\n",
       "   \"'(,#\",\n",
       "   \"'(,&\",\n",
       "   \"'(,'\",\n",
       "   \"'(,*\",\n",
       "   \"'(,(\",\n",
       "   \"'(,+\",\n",
       "   \"'(,!,\",\n",
       "   \"'(,!!\",\n",
       "   '(\"',\n",
       "   ')\"',\n",
       "   '*\"',\n",
       "   '($',\n",
       "   ')$',\n",
       "   '*$',\n",
       "   '(#',\n",
       "   ')#',\n",
       "   '*#',\n",
       "   '(&',\n",
       "   ')&',\n",
       "   '*&',\n",
       "   \"('\",\n",
       "   \")'\",\n",
       "   \"*'\",\n",
       "   '(*',\n",
       "   ')*',\n",
       "   '**',\n",
       "   '((',\n",
       "   ')(',\n",
       "   '*(',\n",
       "   '(+',\n",
       "   ')+',\n",
       "   '*+',\n",
       "   '(!,)!,*!,',\n",
       "   '(!!)!!*!!',\n",
       "   '<[,&]',\n",
       "   \"<[,']\",\n",
       "   '<[,(]',\n",
       "   '<[,)]',\n",
       "   '<[,!*]',\n",
       "   ')!',\n",
       "   '*!',\n",
       "   \"'(,!\",\n",
       "   '%-',\n",
       "   '<[,!]',\n",
       "   '<[,\"]',\n",
       "   '<[,#]',\n",
       "   '<[,$]',\n",
       "   '<[,%]',\n",
       "   '<[,!!]',\n",
       "   '(!'],\n",
       "  'image_file': '2102.02111v2-Figure6-1.png',\n",
       "  'sections': [{'heading': 'Figure 5. Transformer Encoder Architecture. This visualization details the processes in the first',\n",
       "    'text': 'Transformer encoder. The encoder comprises a multi-head self-attention layer and a feedforward neural network, each followed by residual learning and layer normalization. The first encoder takes as an input position-aware embeddings, (z [a 1 ] , . . . , z [a 6 ] ). (A position-aware embedding is the sum of a pure embedding vector and a positional encoding vector (Vaswani et al. 2017, p. 6003). The positional encoding vector contains information on the position of the tth token within the input sequence, thereby making the model aware of token positions (Vaswani et al. 2017, p. 6002-6003).) The position-aware embeddings then are transformed into eight sets of key, query and value vectors. One set is (k1, q1, v1, . . . , k6, q6, v6). These are processed in the multi-head self-attention layer to produce eight sets of context vectors (one set being (c1, . . . , c6)). The sets then are concatenated and transformed linearly to become the updated representations (u1, . . . , u6). After residual learning and layer normalization, (u * 1 , . . . , u * 6 ) enter the feedforward neural network, whose output-after residual learning and layer normalization-are the updated representations produced by the first Transformer encoder: (h * 1 , . . . , h * 6 ). The representations (h * 1 , . . . , h * 6 ) constitute the input to the next encoder, where they are first transformed to sets of key, query and value vectors. bankrupt.\\' were a sentence to be processed, then the embedding for the token \\'it\\' that enters the Transformer would not contain any information regarding which other token in the sentence \\'it\\' is referring to. Is it the company or the statement? In the self-attention mechanism, the representation for \\'it\\' is updated by attending to-and incorporating information from-other tokens in this sentence (Alammar 2018b). It, therefore, is to be expected that after passing through the self-attention layers, the representation of \\'it\\' absorbed some of the representation for \\'company\\' and so encodes information on the dependency between \\'it\\' and \\'company\\' (Alammar 2018b).\\nThe first operation within a self-attention layer is that each input embedding z [at] is transformed into three separate vectors, called key k t , query q t , and value v t (see Figure 5). The key, query, and value vectors are three different projections of the input embedding z [at] (Alammar 2018b). They are generated by matrix multiplication of z [at] with three different weight matrices, W k , W q , and W v (Vaswani et al. 2017, p. 6002):\\n14 k t = z [at] W k q t = z [at] W q v t = z [at] W v (10)\\nThen, for each token a t , an updated representation (named context vector c t ) is computed as a weighted sum over the value vectors of all tokens that are in the same sequence as token a t (Vaswani et al., 2017, p. 6000-6002):\\nc t = T * t * =1 Œ± t,t * v t * (11)\\nThe attention weight Œ± t,t * is a function of the similarity between token a t , represented by q t , and token a t * , that is represented as k t * :\\nŒ± t,t * = exp(score(q t , k t * )) T * t * =1 exp(score(q t , k t * )) (12\\n)\\nwhere score is (q t k t * )/ |k t * | (Vaswani et al. 2017, p. 6001). Œ± t,t * indicates the contribution of token a t * for the representation of token a t . Thus, attention vector c t is calculated as in a basic attention mechanism (see Equations 8 and 9)-except that the attention now is with respect to the value vectors of the tokens that are part of the same sequence as a t (see also Figure 6).\\nThe self-attention mechanism outlined so far is conducted eight times in parallel (Vaswani et al. 2017, p. 6001-6002). Hence, for each token a t , eight different sets of query, key and value vectors are generated and there will be not one but eight attention vectors {c t,1 , . . . , c t,8 } (Vaswani et al. 2017, p. 6001-6002). In doing so, each attention vector can attend to different tokens in each of the eight different representation spaces (Vaswani et al. 2017, p. 6002). For example, in one representation space the attention vector for token a t may learn syntactic structures and in another representation space the attention vector may attend to semantic connections (Vaswani et al. 2017, p. 6004;Clark et al. 2019). Because the self-attention mechanism is implemented eight times in parallel and generates eight attention vectors (or heads), the procedure is called multi-head selfattention (Vaswani et al. 2017, p. 6001). The eight attention vectors subsequently are concatenated into a single vector, c t = [c t,1 ; . . . ; c t,8 ], and multiplied with a corresponding weight matrix W 0 to produce vector u t (Vaswani et al. 2017, p. 6002\\n): u t = c t W 0 .\\nAfterward, u t is added to z [at] , thereby allowing for residual learning (He et al. 2015). 15  Then, layer normalization as suggested in Ba et al. (2016) is conducted (Vaswani et al. 2017, p. 6000). 16\\n( ! < [, ! ] < [, \" ] < [, # ] < [, $ ] < [, % ] < [, !! ] % - \\' (,! ) ! * ! < [, & ] < [, \\' ] < [, ( ] < [, ) ] < [, !* ] ( \" ) \" * \" ( $ ) $ * $ ( # ) # * # ( & ) & * & ( \\' ) \\' * \\' ( * ) * * * ( ( ) ( * ( ( + ) + * + ( !, ) !, * !, ( !! ) !! * !! \\' (,\"\\' (,\\nu * t = LayerN orm(u t + z [at] )(13)\\nu * t then enters a feedforward neural network with a Rectified Linear Unit (ReLU) activation function (Vaswani et al. 2017, p. 6002\\n) h t = max(0, u * t W 1 + b 1 )W 2 + b 2 (14)\\nfollowed by a residual connection with layer normalization (Vaswani et al. 2017, p. 6000):\\nh * t = LayerN orm(h t + u * t )(15)\\nh * t finally is the representation of token a t produced by the encoder. It constitutes an updated representation of input embedding z [at] . Due to the self-attention mechanism, is learned (He et al. 2015). Here ut can be conceived of as the residual on the original representation z [a t ] . Residual learning has been shown to facilitate the optimization of very deep neural networks (He et al. 2015). 16 In layer normalization, for each training instance, the values of the hidden units within a layer are standardized by using the mean and standard deviation of the layer\\'s hidden units (Ba et al. 2016). Layer normalization reduces training time and enhances generalization performance due to its regularizing effects (Ba et al. 2016).\\nh * t is a function of the other tokens in the same sequence and thus captures contextdependent information. Hence, h * t is a contextualized representation of token a t . The same token in another sequence would obtain another token representation vector.\\nThe entire sequence of representations, (h * 1 , . . . , h * t , . . . , h * T ), that is produced as the encoder output, serves as the input for the next encoder that generates eight sets of query, key, and value vectors from each representation h * t to implement multi-head self attention and to finally produce an updated set of representations, (h * 1 , . . . , h * t , . . . , h * T ) * , that are passed to the next encoder and so on. The last encoder from the stack of encoders passes the key and value vectors from its produced sequence of updated representations to each encoder-decoder multi-head attention layer in each decoder (see Figure 4) (Vaswani et al. 2017, p. 6002). Except for the encoder-decoder attention layer in which the decoder pays attention to the encoder input, the architecture of each decoder is largely the same as those of the encoders (Vaswani et al. 2017, p. 6000). Note, however, that the stack of decoders operates in an autoregressive manner (Vaswani et al. 2017, p. 5999). This is, when making the prediction for the next output token o s , the decoders have access to and process the sequence of previous output tokens, (a T , o s , . . . , o s‚àí1 ), as additional inputs (see Figure 4) (Vaswani et al. 2017, p. 5999). In order to ensure that the decoders are autoregressive, self-attention in each decoder is masked, meaning that the attention vector for output token o s can only attend to output tokens preceding token o s (Vaswani et al. 2017, p. 6000). To predict an output token, the hidden state of the last decoder is handed to a linear and softmax layer to produce a probability distribution over the vocabulary (Vaswani et al. 2017, p. 6002).',\n",
       "    'n_publication_ref': 32,\n",
       "    'n_figure_ref': 4}],\n",
       "  'title': 'Introduction to Neural Transfer Learning with Transformers for Social Science Text Analysis',\n",
       "  'abstract': 'Transformer-based models for transfer learning have the potential to achieve high prediction accuracies on text-based supervised learning tasks with relatively few training data instances. These models are thus likely to benefit social scientists that seek to have as accurate as possible text-based measures but only have limited resources for annotating training data. To enable social scientists to leverage these potential benefits for their research, this paper explains how these methods work, why they might be advantageous, and what their limitations are. Additionally, three Transformer-based models for transfer learning, BERT (Devlin et al. 2019), RoBERTa (Liu et al. 2019, and the Longformer (Beltagy et al. 2020), are compared to conventional machine learning algorithms on three applications. Across all evaluated tasks, textual styles, and training data set sizes, the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to textbased social science research.',\n",
       "  'paddleOCR': [[[[767.0, 59.0], [822.0, 77.0], [810.0, 114.0], [756.0, 96.0]],\n",
       "    ['C8', 0.985001802444458]],\n",
       "   [[[594.0, 409.0], [659.0, 423.0], [654.0, 451.0], [588.0, 437.0]],\n",
       "    ['08,4', 0.7998533248901367]],\n",
       "   [[[704.0, 407.0], [763.0, 420.0], [758.0, 447.0], [699.0, 435.0]],\n",
       "    ['08,5', 0.9443332552909851]],\n",
       "   [[[808.0, 405.0], [868.0, 422.0], [861.0, 448.0], [800.0, 431.0]],\n",
       "    ['08,6', 0.7787595391273499]],\n",
       "   [[[922.0, 407.0], [985.0, 424.0], [978.0, 451.0], [915.0, 434.0]],\n",
       "    ['08,7', 0.7584574222564697]],\n",
       "   [[[484.0, 422.0], [537.0, 434.0], [532.0, 456.0], [479.0, 445.0]],\n",
       "    [\"E'8p\", 0.7856412529945374]],\n",
       "   [[[1103.0, 419.0], [1165.0, 428.0], [1161.0, 454.0], [1099.0, 445.0]],\n",
       "    [\"8'8p\", 0.9832735657691956]],\n",
       "   [[[1215.0, 416.0], [1276.0, 428.0], [1271.0, 457.0], [1209.0, 444.0]],\n",
       "    [\"6'8p\", 0.9642103314399719]],\n",
       "   [[[1327.0, 421.0], [1402.0, 430.0], [1398.0, 458.0], [1324.0, 448.0]],\n",
       "    ['08,10', 0.8765166401863098]],\n",
       "   [[[1461.0, 419.0], [1536.0, 430.0], [1532.0, 458.0], [1457.0, 447.0]],\n",
       "    ['08,11', 0.8658435940742493]],\n",
       "   [[[341.0, 446.0], [378.0, 454.0], [373.0, 475.0], [337.0, 467.0]],\n",
       "    ['80', 0.7258405685424805]],\n",
       "   [[[23.0, 625.0], [54.0, 625.0], [54.0, 646.0], [23.0, 646.0]],\n",
       "    ['K1', 0.9671112298965454]],\n",
       "   [[[221.0, 622.0], [263.0, 628.0], [260.0, 656.0], [218.0, 650.0]],\n",
       "    ['q2', 0.9571835994720459]],\n",
       "   [[[269.0, 625.0], [305.0, 625.0], [305.0, 646.0], [269.0, 646.0]],\n",
       "    ['V', 0.7023087739944458]],\n",
       "   [[[324.0, 619.0], [370.0, 624.0], [367.0, 650.0], [321.0, 645.0]],\n",
       "    ['k3', 0.8860876560211182]],\n",
       "   [[[376.0, 623.0], [411.0, 628.0], [408.0, 652.0], [373.0, 647.0]],\n",
       "    ['q3', 0.8659123182296753]],\n",
       "   [[[421.0, 623.0], [460.0, 628.0], [457.0, 650.0], [418.0, 645.0]],\n",
       "    ['V3', 0.9594936370849609]],\n",
       "   [[[478.0, 621.0], [534.0, 628.0], [531.0, 654.0], [474.0, 646.0]],\n",
       "    ['k4', 0.9442427158355713]],\n",
       "   [[[571.0, 627.0], [607.0, 627.0], [607.0, 647.0], [571.0, 647.0]],\n",
       "    ['Vs', 0.6905474662780762]],\n",
       "   [[[631.0, 627.0], [663.0, 627.0], [663.0, 647.0], [631.0, 647.0]],\n",
       "    ['K 5', 0.9105220437049866]],\n",
       "   [[[725.0, 623.0], [760.0, 628.0], [756.0, 650.0], [722.0, 645.0]],\n",
       "    ['V5', 0.5692225694656372]],\n",
       "   [[[783.0, 625.0], [825.0, 632.0], [822.0, 654.0], [780.0, 647.0]],\n",
       "    ['K6', 0.9358776807785034]],\n",
       "   [[[828.0, 626.0], [867.0, 632.0], [863.0, 657.0], [825.0, 652.0]],\n",
       "    ['9b', 0.9894979000091553]],\n",
       "   [[[949.0, 625.0], [994.0, 625.0], [994.0, 647.0], [949.0, 647.0]],\n",
       "    ['K7', 0.8367689251899719]],\n",
       "   [[[993.0, 622.0], [1033.0, 632.0], [1028.0, 655.0], [987.0, 644.0]],\n",
       "    ['q7', 0.8244015574455261]],\n",
       "   [[[1041.0, 625.0], [1074.0, 625.0], [1074.0, 647.0], [1041.0, 647.0]],\n",
       "    ['V', 0.7112823724746704]],\n",
       "   [[[1107.0, 618.0], [1156.0, 626.0], [1152.0, 654.0], [1103.0, 646.0]],\n",
       "    ['k8', 0.9428681135177612]],\n",
       "   [[[1151.0, 625.0], [1195.0, 625.0], [1195.0, 653.0], [1151.0, 653.0]],\n",
       "    ['q8', 0.7938370704650879]],\n",
       "   [[[1200.0, 627.0], [1240.0, 627.0], [1240.0, 649.0], [1200.0, 649.0]],\n",
       "    ['V8', 0.7589699029922485]],\n",
       "   [[[1251.0, 618.0], [1715.0, 623.0], [1715.0, 660.0], [1250.0, 655.0]],\n",
       "    ['k9 q9 V9 k10910V10 k11q11V11', 0.9228693842887878]],\n",
       "   [[[119.0, 629.0], [139.0, 629.0], [139.0, 644.0], [119.0, 644.0]],\n",
       "    ['V', 0.5863902568817139]],\n",
       "   [[[177.0, 628.0], [205.0, 633.0], [202.0, 648.0], [175.0, 643.0]],\n",
       "    ['K2', 0.6832918524742126]],\n",
       "   [[[524.0, 629.0], [564.0, 629.0], [564.0, 651.0], [524.0, 651.0]],\n",
       "    ['94', 0.9567773342132568]],\n",
       "   [[[678.0, 629.0], [712.0, 629.0], [712.0, 649.0], [678.0, 649.0]],\n",
       "    ['95', 0.8641794323921204]],\n",
       "   [[[30.0, 777.0], [132.0, 789.0], [126.0, 840.0], [24.0, 827.0]],\n",
       "    ['Z[a1]', 0.9896095395088196]],\n",
       "   [[[183.0, 777.0], [284.0, 789.0], [278.0, 840.0], [177.0, 827.0]],\n",
       "    ['Z[a2]', 0.9928681254386902]],\n",
       "   [[[336.0, 779.0], [437.0, 789.0], [433.0, 839.0], [331.0, 829.0]],\n",
       "    ['Z[a3]', 0.9953319430351257]],\n",
       "   [[[493.0, 775.0], [596.0, 785.0], [591.0, 839.0], [488.0, 829.0]],\n",
       "    ['Z[a4]', 0.9952389001846313]],\n",
       "   [[[639.0, 779.0], [740.0, 788.0], [736.0, 838.0], [635.0, 829.0]],\n",
       "    ['Z[a5]', 0.9687767028808594]],\n",
       "   [[[793.0, 779.0], [888.0, 787.0], [885.0, 837.0], [789.0, 830.0]],\n",
       "    ['Z[a6]', 0.9936928749084473]],\n",
       "   [[[960.0, 777.0], [1062.0, 789.0], [1057.0, 840.0], [955.0, 827.0]],\n",
       "    ['Z[a7]', 0.9973742365837097]],\n",
       "   [[[1115.0, 775.0], [1220.0, 786.0], [1215.0, 841.0], [1110.0, 831.0]],\n",
       "    ['Z[ag]', 0.979012131690979]],\n",
       "   [[[1269.0, 774.0], [1371.0, 784.0], [1366.0, 837.0], [1264.0, 827.0]],\n",
       "    ['Z[a9]', 0.98997962474823]],\n",
       "   [[[1423.0, 779.0], [1539.0, 789.0], [1535.0, 839.0], [1419.0, 829.0]],\n",
       "    ['Z[a10]', 0.9903395175933838]],\n",
       "   [[[1577.0, 779.0], [1695.0, 793.0], [1690.0, 839.0], [1572.0, 825.0]],\n",
       "    ['Z[a11]', 0.9836032390594482]],\n",
       "   [[[46.0, 900.0], [318.0, 907.0], [318.0, 944.0], [45.0, 938.0]],\n",
       "    ['The company', 0.9890540838241577]],\n",
       "   [[[367.0, 906.0], [405.0, 906.0], [405.0, 941.0], [367.0, 941.0]],\n",
       "    ['is', 0.9968218803405762]],\n",
       "   [[[474.0, 902.0], [622.0, 907.0], [621.0, 946.0], [473.0, 941.0]],\n",
       "    ['issuing', 0.9998651742935181]],\n",
       "   [[[739.0, 908.0], [943.0, 908.0], [943.0, 940.0], [739.0, 940.0]],\n",
       "    ['statement', 0.9996784925460815]],\n",
       "   [[[1144.0, 904.0], [1180.0, 904.0], [1180.0, 940.0], [1144.0, 940.0]],\n",
       "    ['it', 0.9989098310470581]],\n",
       "   [[[1292.0, 901.0], [1334.0, 901.0], [1334.0, 941.0], [1292.0, 941.0]],\n",
       "    ['is', 0.9981900453567505]],\n",
       "   [[[1387.0, 899.0], [1692.0, 903.0], [1691.0, 942.0], [1386.0, 938.0]],\n",
       "    ['bankrupt [EOS]', 0.9808393120765686]],\n",
       "   [[[687.0, 912.0], [714.0, 912.0], [714.0, 938.0], [687.0, 938.0]],\n",
       "    ['a', 0.996801495552063]],\n",
       "   [[[989.0, 910.0], [1036.0, 910.0], [1036.0, 940.0], [989.0, 940.0]],\n",
       "    ['as', 0.9984306693077087]]],\n",
       "  'ocr': [[[367.0, 906.0], [405.0, 906.0], [405.0, 941.0], [367.0, 941.0]],\n",
       "   ['is', 0.9968218803405762]]},\n",
       " '2101.08385v1-Figure4-1.png': {'caption': 'Fig. 4: CNN model. (create better caption)',\n",
       "  'imageText': [],\n",
       "  'image_file': '2101.08385v1-Figure4-1.png',\n",
       "  'sections': [{'heading': 'A. Convolution Neural Network (CNN) Architecture',\n",
       "    'text': 'CNN is a class of deep learning models which can infer patterns based on data formatted as a grid structure, such as a set of prices over time for stock or a grid representation of pixels in an image (add reference for these architectures). These Artificial Neural Netowrk (ANNs) use a linear mathematical operation called convolution in at least one of their layers [3]. The convolution operation is commonly identified by the following two equations:\\ns(t) = x(a)w(t ‚àí a)da(4)\\ns(t) = (x * w)(t)(5)\\nEquation 4 explicitly denotes the equation for convolution, whereas Equation 5 displays how an asterisk can be used to for the linear operation. In both equations, x is referred to as the input. Typically, this is formatted as a multidimensional array, or a tensor, that matches the size and dimensions of the data. The second argument is w, representing a kernel, which stores parameters for the model also formatted as a tensor. This argument is adapted throughout the training process of the model. The output of both functions, s, is called the feature map of the convolution layer. This is what is fed into the next layer of the network [3]. Hidden layers are generated from applying a kernel, or filter, of weights over the receptive field of the inputs. More specifically, the hidden layer is computed based off of the filter weights and the input layer as it strides across the feature space [28]. This operation can either compress or expand input space depending on the applied kernel [29]. This paradigm is followed by rounds of activations, normalizations, and pooling [29]. The model typically ends with a fully connected layer to compute its outputs [28]. The proposed model is represented in Figure 4 [cite my paper]. The model is marked by three rounds of a 1-D convolution layer, a batch normalization layer, a dense layer, and a 1-D maximum pooling layer. After these 12 layers, the model finishes off with a 50% dropout layer, a flattened layer, and finally a fully connected layer corresponding to the 986 alignment scores for each sample [13] [12].\\nThe model described above is ran on all four data sets for 100 epochs with a batch size of 80 and compiled with the Adam optimizer (learning rate=0.001, beta 1=0.9, beta 2=0.999, epsilon=1e-07). Of the 10,000 samples in each dataset, 80% is reserved for training the network and the remaining 20% is used for validation after each epoch. For its loss function, the model relies on Mean Squared Error (MSE), which is calculated between predicted values (y pred ) and target values (y act ) with the following formula in Equation 6:\\nM SE(y pred , y act ) = 1 n n i=1 (y pred,i ‚àí y act,i )(6)',\n",
       "    'n_publication_ref': 7,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Motif Identification using CNN-based Pairwise Subsequence Alignment Score Prediction',\n",
       "  'abstract': \"A common problem in bioinformatics is related to identifying gene regulatory regions marked by relatively high frequencies of motifs, or deoxyribonucleic acid sequences that often code for transcription and enhancer proteins. Predicting alignment scores between subsequence k-mers and a given motif enables the identification of candidate regulatory regions in a gene, which correspond to the transcription of these proteins. We propose a one-dimensional (1-D) Convolution Neural Network trained on k-mer formatted sequences interspaced with the given motif pattern to predict pairwise alignment scores between the consensus motif and subsequence k-mers. Our model consists of fifteen layers with three rounds of a one-dimensional convolution layer, a batch normalization layer, a dense layer, and a 1-D maximum pooling layer. We train the model using mean squared error loss on four different data sets each with a different motif pattern randomly inserted in DNA sequences: the first three data sets have zero, one, and two mutations applied on each inserted motif, and the fourth data set represents the inserted motif as a position-specific probability matrix. We use a novel proposed metric in order to evaluate the model's performance, SŒ±, which is based on the Jaccard Index. We use 10-fold cross validation to evaluate out model. Using SŒ±, we measure the accuracy of the model by identifying the 15 highest-scoring 15-mer indices of the predicted scores that agree with that of the actual scores within a selected Œ± region. For the best performing data set, our results indicate on average 99.3% of the top 15 motifs were identified correctly within a one base pair stride (Œ± = 1) in the out of sample data. To the best of our knowledge, this is a novel approach that illustrates how data formatted in an intelligent way can be extrapolated using machine learning.\",\n",
       "  'paddleOCR': [[[[33.0, 16.0], [412.0, 16.0], [412.0, 32.0], [33.0, 32.0]],\n",
       "    ['* Followed by MaxPooling1D (padding=same, pool size=2)',\n",
       "     0.98183673620224]],\n",
       "   [[[30.0, 32.0], [486.0, 34.0], [486.0, 53.0], [30.0, 50.0]],\n",
       "    ['** Followed by BatchNormalization (momentum=0.99, epsilon=0.001)',\n",
       "     0.991248369216919]],\n",
       "   [[[252.0, 65.0], [326.0, 65.0], [326.0, 83.0], [252.0, 83.0]],\n",
       "    ['N985,6', 0.9261016845703125]],\n",
       "   [[[881.0, 81.0], [947.0, 81.0], [947.0, 100.0], [881.0, 100.0]],\n",
       "    ['(N, 7872)', 0.8642017841339111]],\n",
       "   [[[345.0, 114.0], [426.0, 114.0], [426.0, 133.0], [345.0, 133.0]],\n",
       "    ['N, 985, 16', 0.8313072323799133]],\n",
       "   [[[541.0, 113.0], [620.0, 113.0], [620.0, 131.0], [541.0, 131.0]],\n",
       "    ['N,492, 32', 0.915813148021698]],\n",
       "   [[[735.0, 113.0], [815.0, 113.0], [815.0, 131.0], [735.0, 131.0]],\n",
       "    ['N,492, 64', 0.8935868144035339]],\n",
       "   [[[959.0, 110.0], [1017.0, 113.0], [1016.0, 133.0], [958.0, 130.0]],\n",
       "    ['N985', 0.9947776794433594]],\n",
       "   [[[147.0, 125.0], [229.0, 125.0], [229.0, 144.0], [147.0, 144.0]],\n",
       "    ['N, 985, 15', 0.8665076494216919]],\n",
       "   [[[14.0, 156.0], [163.0, 156.0], [163.0, 178.0], [14.0, 178.0]],\n",
       "    ['[1,0,0,0,...0]', 0.8423271179199219]],\n",
       "   [[[1020.0, 150.0], [1030.0, 150.0], [1030.0, 163.0], [1020.0, 163.0]],\n",
       "    ['0', 0.8962221741676331]],\n",
       "   [[[447.0, 171.0], [520.0, 171.0], [520.0, 190.0], [447.0, 190.0]],\n",
       "    ['N,492,6', 0.894553542137146]],\n",
       "   [[[13.0, 189.0], [163.0, 192.0], [163.0, 215.0], [13.0, 212.0]],\n",
       "    ['[0,0,1,0,...,0]', 0.8438349962234497]],\n",
       "   [[[1020.0, 187.0], [1030.0, 187.0], [1030.0, 201.0], [1020.0, 201.0]],\n",
       "    ['1', 0.9984242916107178]],\n",
       "   [[[12.0, 227.0], [186.0, 229.0], [186.0, 252.0], [12.0, 250.0]],\n",
       "    ['[1,0,0,0,...1]', 0.81658935546875]],\n",
       "   [[[641.0, 223.0], [715.0, 226.0], [715.0, 245.0], [640.0, 242.0]],\n",
       "    ['N246,6', 0.9178256392478943]],\n",
       "   [[[1018.0, 222.0], [1031.0, 222.0], [1031.0, 239.0], [1018.0, 239.0]],\n",
       "    ['2', 0.998350977897644]],\n",
       "   [[[14.0, 266.0], [190.0, 266.0], [190.0, 288.0], [14.0, 288.0]],\n",
       "    ['[0,0,1,0,...0]', 0.8002362847328186]],\n",
       "   [[[844.0, 262.0], [859.0, 262.0], [857.0, 317.0], [841.0, 316.0]],\n",
       "    ['Flatten', 0.9960265755653381]],\n",
       "   [[[1018.0, 260.0], [1031.0, 260.0], [1031.0, 276.0], [1018.0, 276.0]],\n",
       "    ['3', 0.9987519979476929]],\n",
       "   [[[1019.0, 297.0], [1032.0, 297.0], [1032.0, 313.0], [1019.0, 313.0]],\n",
       "    ['4', 0.9991017580032349]],\n",
       "   [[[1020.0, 334.0], [1031.0, 334.0], [1031.0, 351.0], [1020.0, 351.0]],\n",
       "    ['S', 0.6709457635879517]],\n",
       "   [[[15.0, 359.0], [181.0, 359.0], [181.0, 378.0], [15.0, 378.0]],\n",
       "    ['[01,0,0,...0]', 0.7975960373878479]],\n",
       "   [[[637.0, 369.0], [718.0, 367.0], [719.0, 386.0], [637.0, 388.0]],\n",
       "    ['Conv1D**', 0.9909957647323608]],\n",
       "   [[[161.0, 402.0], [204.0, 402.0], [204.0, 422.0], [161.0, 422.0]],\n",
       "    ['Input', 0.9988583326339722]],\n",
       "   [[[440.0, 423.0], [522.0, 421.0], [523.0, 440.0], [440.0, 443.0]],\n",
       "    ['Conv1D**', 0.9643574357032776]],\n",
       "   [[[1013.0, 424.0], [1044.0, 424.0], [1044.0, 440.0], [1013.0, 440.0]],\n",
       "    ['984', 0.9992592930793762]],\n",
       "   [[[364.0, 470.0], [410.0, 470.0], [410.0, 486.0], [364.0, 486.0]],\n",
       "    ['Dense', 0.999108612537384]],\n",
       "   [[[558.0, 471.0], [605.0, 471.0], [605.0, 488.0], [558.0, 488.0]],\n",
       "    ['Dense', 0.9985524415969849]],\n",
       "   [[[751.0, 471.0], [799.0, 471.0], [799.0, 488.0], [751.0, 488.0]],\n",
       "    ['Dense', 0.9990266561508179]],\n",
       "   [[[962.0, 480.0], [1013.0, 480.0], [1013.0, 496.0], [962.0, 496.0]],\n",
       "    ['Output', 0.99686598777771]],\n",
       "   [[[358.0, 490.0], [412.0, 489.0], [413.0, 505.0], [359.0, 506.0]],\n",
       "    ['ReLU*', 0.9953063726425171]],\n",
       "   [[[552.0, 491.0], [607.0, 488.0], [608.0, 508.0], [553.0, 511.0]],\n",
       "    ['ReLU*', 0.9974830746650696]],\n",
       "   [[[745.0, 491.0], [801.0, 488.0], [802.0, 508.0], [746.0, 511.0]],\n",
       "    ['ReLU*', 0.9967387318611145]],\n",
       "   [[[836.0, 491.0], [870.0, 491.0], [870.0, 507.0], [836.0, 507.0]],\n",
       "    ['50%', 0.9995021224021912]],\n",
       "   [[[824.0, 510.0], [882.0, 510.0], [882.0, 528.0], [824.0, 528.0]],\n",
       "    ['Dropout', 0.9994295835494995]],\n",
       "   [[[248.0, 532.0], [328.0, 528.0], [329.0, 546.0], [249.0, 550.0]],\n",
       "    ['Conv1D**', 0.9807968139648438]]],\n",
       "  'ocr': [[[751.0, 471.0], [799.0, 471.0], [799.0, 488.0], [751.0, 488.0]],\n",
       "   ['Dense', 0.9990266561508179]]},\n",
       " '2010.15908v1-Figure1-1.png': {'caption': 'Figure 1: Schematic of Graph Neural Network combing Node and edge features. The convolutions on a 3 node graph with subsequent feature aggregation is shown. The convolution kernel operates on neighboring nodes with a neural network shared for all node-pairs. The SUM reduction of node features is shown.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.15908v1-Figure1-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Graph Neural Network for Metal Organic Framework Potential Energy Approximation',\n",
       "  'abstract': 'Metal-organic frameworks (MOFs) are nanoporous compounds composed of metal ions and organic linkers. MOFs play an important role in industrial applications such as gas separation, gas purification, and electrolytic catalysis. Important MOF properties such a potential energy are currently computed via techniques such as density functional theory (DFT). Although DFT provides accurate results, it is computationally costly. We propose a machine learning approach for estimating the potential energy of candidate MOFs, decomposing it into separate pair-wise atomic interactions using a graph neural network. Such a technique will allow high-throughput screening of candidates MOFs. We also generate a database of 50,000 spatial configurations and high quality potential energy values using DFT. Preprint. Under review.',\n",
       "  'paddleOCR': [[[[357.0, 204.0],\n",
       "     [499.0, 204.0],\n",
       "     [499.0, 222.0],\n",
       "     [357.0, 222.0]],\n",
       "    ['Arbitrary Neural Network', 0.9943146705627441]],\n",
       "   [[[870.0, 215.0], [1169.0, 219.0], [1169.0, 250.0], [869.0, 246.0]],\n",
       "    ['Y=SUM{}', 0.9606781005859375]],\n",
       "   [[[618.0, 347.0], [705.0, 347.0], [705.0, 366.0], [618.0, 366.0]],\n",
       "    ['New Graph', 0.9449602961540222]],\n",
       "   [[[201.0, 368.0], [353.0, 368.0], [353.0, 385.0], [201.0, 385.0]],\n",
       "    ['Repeat for All Nodes', 0.9581926465034485]],\n",
       "   [[[617.0, 368.0], [732.0, 368.0], [732.0, 385.0], [617.0, 385.0]],\n",
       "    ['Representation', 0.9986234903335571]],\n",
       "   [[[689.0, 458.0], [708.0, 458.0], [708.0, 621.0], [689.0, 621.0]],\n",
       "    ['Arbitrary Neural Network', 0.9706996083259583]],\n",
       "   [[[300.0, 477.0], [536.0, 480.0], [536.0, 503.0], [300.0, 500.0]],\n",
       "    ['Per Bond Function is', 0.9996188879013062]],\n",
       "   [[[334.0, 509.0], [502.0, 509.0], [502.0, 532.0], [334.0, 532.0]],\n",
       "    ['encoded in the', 0.9998762011528015]],\n",
       "   [[[315.0, 536.0], [521.0, 539.0], [521.0, 561.0], [315.0, 559.0]],\n",
       "    [\"MOF-GCN's inner\", 0.9974028468132019]],\n",
       "   [[[372.0, 568.0], [464.0, 568.0], [464.0, 592.0], [372.0, 592.0]],\n",
       "    ['network', 0.9986782670021057]]],\n",
       "  'ocr': [[[357.0, 204.0], [499.0, 204.0], [499.0, 222.0], [357.0, 222.0]],\n",
       "   ['Arbitrary Neural Network', 0.9943146705627441]]},\n",
       " '2204.04370v1-Figure7-1.png': {'caption': 'Figure 7: QuiKo Architecture for Input Audio Signal',\n",
       "  'imageText': [],\n",
       "  'image_file': '2204.04370v1-Figure7-1.png',\n",
       "  'sections': [{'heading': 'Drum Sample Database Preparation',\n",
       "    'text': \"First we need to prepare a database of audio samples to be used in the construction of the new generated beat. We will gather a collection of audio samples (i.e. single drum hits and long melodic and harmonic patterns and progressions). We then apply the filter bank as specified previously to each of the samples in the database. There should be a low, mid and high versions of each sample. For each of the samples' filtered versions the Harmonic Percussive Source Separation (HPSS) algorithm from the librosa library in python [9] is then applies to extract harmonic and percussive features of the signals. The algorithm returns two signals via median filtering [9]. (1) percussive part where the transients and onsets of the signal are more pronounced (2) the harmonic part where the tonal and spectral content is more defined. These resulting signals are shown in figure 7. For the percussive part shown figure 7(a), the values of the peaks(spikes) in the signal are identified and are summed together. This sum is then divided by the value of the maximum peak, which will become our Œ∏ angle for the unitary gates used in the quantum circuit. The parameters and matrix for the U gate (U3 gate in qiskit) is expressed in equation (5). For the harmonic part of the signal, shown in figure 7(b), the Fast Fourier Transform (FFT) is performed. From there the highest 3 peaks are identified within the spectrum, and the weighted average of these values is calculated. This will be our œÜ parameter for the U-gates. Finally, the spectral centroid is also calculated from the the harmonic part which will define our Œª parameter.  5) above expressed the U-gate operation in matrix form. Also it defines the parameters that are encoded onto each U-gate in the quantum circuit. Methods for this encoding will be discussed in further detail in the following sections. Also keep in mind that any set of features can be extracted and used as the parameter for these gates.\\nU (Œ∏, œÜ, Œª) = cos Œ∏ 2 ‚àíe i Œªsin Œ∏ 2 e i œÜ sin Œ∏ 2 e i (œÜ + Œª) cos Œ∏ 2 (5) œÜ = N ‚àí1 n=0 f (n)x(n) N ‚àí1 n=0 x(n)(6)\\nŒª = max{f (n) onset } (7) Œ∏ = argmax x=s { N ‚àí1 n=0 x n e ‚àíi2œÄkn N k = 0, ..., N ‚àí 1} (8)\",\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 3},\n",
       "   {'heading': 'Phase Kickback Sequencing',\n",
       "    'text': \"Static Encoding, however, is very expensive to implement as multi-controlled qubit gates (containing more than one control) do not correspond to the cost of the number of controls and targets. For example, a controlled X (cx) gate would have the cost of 2 for target and the control qubits [12]. Any more than one control qubit would need to decompose into a larger circuit as shown for the multi-controlled U-gate with 2 controls in figure 11.As a result, if we want to design our algorithms to deal with subdivisions any small than eight notes, the circuit cost would drastically increase. Alternative, more cheaper methods are needed if we want to scale our system for more detail and resolution.\\nHere we propose a method in order to reduce the cost associated with static encoding. This method is called Phase Kickback Sequence Encoding (PKBSE). In previous sections we discussed the effects of phase kickback produced by controlled quantum gates and how to estimate the amount of phase that gets kicked back into the control qubit register (in this case the spinal cord register).In order to reduce the cost of the static encoding circuit we need to replace the multi-controlled U-gates with single controlled U-gates, and sequence them in a way that apply parameters of a specific subdivision. Figure 12 outlines the logic behind this method. This has a number of steps:\\n1. Split the measure in half with half the subdivisions on one side (starting with 0 in binary) and the other on the right side (subdivisions starting with '1' in binary).\\n2. Calculate and/or extract the desired feature for each subdivisions on left and right side of the measure.\\n3. For one side of the measure (the '0' side or the '1' side) sum together the features associated with each subdivision with the same type of features in previous subdivisions. This is done to reflect the behavior of a human musician in which their musical response is based on the current and previous musical events from other performing entities.\\n4. multiply all the feature summations by -1 if they are not the associated with the final subdivision for each half of the measure.\\n5. repeat this process for the other side of the measure.\\n6. Organize the data into a PKBSE encoding matrix as at the bottom of figure 12. We negate all summed other than the last subdivision within the respective halves of the measure due to the fact that they cover the entire segment. If we sum all the parts together for a particular sub-band we get a sequence dependent on the qubits themselves being 0 or 1 and remove the smaller segments from the total feature value from the higher segment layers. After we have done this for each feature we organize it into an encoding matrix shown at the bottom of figure 6 in order to organize our parameters to encode onto our quantum circuit. Since we are dealing with a low, mid and high band along with 3 separate parameters, our PKBSE encoding matrix will be 3x3. Each element this matrix will be 2x4 matrix containing each of the summed features for each subdivision.\\n|q 0 H ‚äón U 1 l U 2 l U 3 l U 4 l ... |q 1 U 1m U 2 l U 3m U 4m ... |q 2 U 1 h U 2 h U 3 h U 4 h ... |q 3 H ‚äón X ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ X ... |q 4 X ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ X ... |q 5 X ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ X ... |q 0 U 5 l U 6 l U 7 l U 8 l |q 1 U 5m U 6m U 7m U 8m |q 2 U 5 h U 6 h U 7 h U 8 h |q 3 ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ QF T ‚Ä† |q 4 ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ |q 5 ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢\\nFigure 7 shows the quantum circuit for the PKBSE encoding method. The spinal cord and timbre registers are setup in the same way that they were in static encoding. Each qubit in the timbre register represents one of the sub-band of the input audio signal, while the spinal cord register represent the different subdivisions that are being considered. This is done by entangling the two registers in a particular way. We will use the concept presented in [13] which states that human musicians perceive the attack times of instrument with lower frequency content with less resolution than that of instruments with high frequency content. Here we can say that parameters associated with the low sub-band, encoded on to q 0 , will be entangled with the most significant qubit in the spinal cord register, q 3 . This is due to the fact that the rate at which q 3 changes is less frequent that the other qubits in the register. Following suit, q 1 which deals with mid sub-band sequence parameters will be entangled with the next significant qubit q 4 , and so on and so forth.\\nThe separation between the sequences for the first and the second half of the measure can be observed in the circuit as well. The first half of the measure (as stated previously) is defined by '0' in the most significant spinal cord qubit, and thus its U-gate sequence is enclosed by X gates on the spinal cord register. This sequence of gates will be triggered if any of the spinal cord qubits happen to be '0'. On the other hands, if any of these qubits happen to be '1' then the gate sequence outside of the X gates will be triggered. The encoding process of mapping the extracted features of the signal to parameters on their corresponding controlled U-gates is identical to that for static encoding. However, in the PKBSE circuit we will get a direct phase kick back from the U-gates that were applied to the timbre register, and thus elements from the original signal should have more direct impact on the states for the spinal cord register. Also in contrast to the static encoding method where we considered the effects of features for one subdivision at a time, the PKBSE method allows the system to consider the effects of groups of subdivisions at the same time in superposition.\",\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 5}],\n",
       "  'title': 'QuiKo: A Quantum Beat Generation Application',\n",
       "  'abstract': '',\n",
       "  'paddleOCR': [[[[593.0, 40.0], [762.0, 44.0], [761.0, 80.0], [592.0, 75.0]],\n",
       "    ['Filter Bank', 0.9984560608863831]],\n",
       "   [[[987.0, 50.0], [1268.0, 50.0], [1268.0, 80.0], [987.0, 80.0]],\n",
       "    ['Feature Extraction', 0.9999053478240967]],\n",
       "   [[[1553.0, 48.0], [1811.0, 48.0], [1811.0, 84.0], [1553.0, 84.0]],\n",
       "    ['Quantum Circuit', 0.9998905062675476]],\n",
       "   [[[1560.0, 120.0], [1801.0, 124.0], [1800.0, 160.0], [1559.0, 156.0]],\n",
       "    ['Static encoding', 0.9992708563804626]],\n",
       "   [[[612.0, 181.0], [754.0, 185.0], [753.0, 221.0], [610.0, 216.0]],\n",
       "    ['Low Pass', 0.9997776746749878]],\n",
       "   [[[130.0, 219.0], [313.0, 219.0], [313.0, 255.0], [130.0, 255.0]],\n",
       "    ['Input Audio', 0.9813518524169922]],\n",
       "   [[[1558.0, 305.0], [1807.0, 310.0], [1807.0, 345.0], [1557.0, 341.0]],\n",
       "    ['PKBSE encoding', 0.9946918487548828]],\n",
       "   [[[611.0, 324.0], [768.0, 324.0], [768.0, 360.0], [611.0, 360.0]],\n",
       "    ['Band Pass', 0.9998897910118103]],\n",
       "   [[[602.0, 452.0], [751.0, 452.0], [751.0, 488.0], [602.0, 488.0]],\n",
       "    ['High Pass', 0.9763156175613403]],\n",
       "   [[[1512.0, 644.0], [1870.0, 648.0], [1870.0, 684.0], [1511.0, 680.0]],\n",
       "    ['Probability Distribution', 0.9992691874504089]],\n",
       "   [[[153.0, 663.0], [397.0, 665.0], [397.0, 697.0], [153.0, 694.0]],\n",
       "    ['Audio Database', 0.9999418258666992]],\n",
       "   [[[48.0, 728.0], [101.0, 728.0], [101.0, 751.0], [48.0, 751.0]],\n",
       "    ['39', 0.6485037207603455]],\n",
       "   [[[823.0, 730.0], [1041.0, 730.0], [1041.0, 766.0], [823.0, 766.0]],\n",
       "    ['Compare with', 0.9852883219718933]],\n",
       "   [[[46.0, 760.0], [105.0, 760.0], [105.0, 781.0], [46.0, 781.0]],\n",
       "    ['Favorites', 0.9941890239715576]],\n",
       "   [[[185.0, 758.0], [311.0, 758.0], [311.0, 779.0], [185.0, 779.0]],\n",
       "    ['iCloud storage is full.', 0.9658976793289185]],\n",
       "   [[[55.0, 781.0], [128.0, 781.0], [128.0, 802.0], [55.0, 802.0]],\n",
       "    [' AirDrop', 0.9920810461044312]],\n",
       "   [[[187.0, 783.0], [214.0, 783.0], [214.0, 797.0], [187.0, 797.0]],\n",
       "    ['Name', 0.9903020858764648]],\n",
       "   [[[856.0, 774.0], [1008.0, 774.0], [1008.0, 810.0], [856.0, 810.0]],\n",
       "    ['Database', 0.999902606010437]],\n",
       "   [[[57.0, 804.0], [130.0, 804.0], [130.0, 825.0], [57.0, 825.0]],\n",
       "    ['Recents', 0.9969847798347473]],\n",
       "   [[[206.0, 804.0], [313.0, 804.0], [313.0, 819.0], [206.0, 819.0]],\n",
       "    ['Drums_Database', 0.9994046092033386]],\n",
       "   [[[57.0, 827.0], [130.0, 827.0], [130.0, 848.0], [57.0, 848.0]],\n",
       "    [' Desktop', 0.971966564655304]],\n",
       "   [[[204.0, 819.0], [290.0, 819.0], [290.0, 840.0], [204.0, 840.0]],\n",
       "    ['Drums_high', 0.9996063113212585]],\n",
       "   [[[208.0, 837.0], [281.0, 837.0], [281.0, 852.0], [208.0, 852.0]],\n",
       "    ['Drums_low', 0.9949963092803955]],\n",
       "   [[[59.0, 850.0], [153.0, 850.0], [153.0, 873.0], [59.0, 873.0]],\n",
       "    ['A Applications', 0.9286881685256958]],\n",
       "   [[[208.0, 854.0], [283.0, 854.0], [283.0, 869.0], [208.0, 869.0]],\n",
       "    ['Drums_mid', 0.9949934482574463]],\n",
       "   [[[57.0, 873.0], [147.0, 873.0], [147.0, 896.0], [57.0, 896.0]],\n",
       "    ['O Downloads', 0.9384741187095642]],\n",
       "   [[[197.0, 869.0], [256.0, 869.0], [256.0, 890.0], [197.0, 890.0]],\n",
       "    [' input', 0.9298344254493713]],\n",
       "   [[[58.0, 894.0], [147.0, 899.0], [146.0, 922.0], [56.0, 917.0]],\n",
       "    [' Documents', 0.9581640362739563]],\n",
       "   [[[55.0, 930.0], [86.0, 930.0], [86.0, 945.0], [55.0, 945.0]],\n",
       "    ['Cloud', 0.9995304942131042]]],\n",
       "  'ocr': [[[197.0, 869.0], [256.0, 869.0], [256.0, 890.0], [197.0, 890.0]],\n",
       "   [' input', 0.9298344254493713]]},\n",
       " '2102.05981v1-Figure2-1.png': {'caption': 'Figure 2: High-level overview of RowBlocker (per DRAM rank). An ACT is accompanied by its row address.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2102.05981v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'RowBlocker',\n",
       "    'text': 'RowBlocker\\'s goal is to proactively throttle row activations in an efficient manner to avoid any possibility of a RowHammer attack. RowBlocker achieves this by overcoming two challenges regarding performance and area overheads.\\nFirst, achieving low performance overhead is a key challenge for a throttling mechanism because many benign applications tend to repeatedly activate a DRAM row that they have recently activated [44,45,57,76]. This can potentially cause a throttling mechanism to mistakenly throttle benign applications, thereby degrading system performance. To ensure throttling only applications that might cause RowHammer bit-flips, Row-Blocker throttles the row activations targeting only rows whose activation rates are above a given threshold. To this end, Row-Blocker implements two components as shown in Figure 2: (1) a per-bank blacklisting mechanism, RowBlocker-BL, which blacklists all rows with an activation rate greater than a predefined threshold called the blacklisting threshold (N BL ); and\\n(2) a per-rank activation history buffer, RowBlocker-HB, which tracks the most recently activated rows. RowBlocker enforces a time delay between two consecutive activations targeting a row only if the row is blacklisted. By doing so, RowBlocker is less likely to throttle a benign application\\'s row activations.\\nSecond, achieving low area overhead is a key challenge for a throttling mechanism because throttling requires tracking all row activations throughout an entire refresh window without losing information of any row activation. RowBlocker implements its blacklisting mechanism, RowBlocker-BL, by using area-efficient counting Bloom filters [11,33] to track row activation rates. RowBlocker-BL maintains two counting Bloom filters in a time-interleaved manner to track row activation rates for large time windows without missing any row that should be blacklisted. We explain how counting Bloom filters work and how RowBlocker-BL employs them in Section 3.1.1. High-Level Overview of RowBlocker. RowBlocker modifies the memory request scheduler to temporarily block (i.e., delay) an activation that targets a blacklisted and recently-activated row until the activation can be safely performed. By blocking such row activations, RowBlocker ensures that no row can be activated at a high enough rate to induce RowHammer bit-flips. When the memory request scheduler attempts to schedule a row activation command to a bank, it queries RowBlocker ( 1 ) to check if the row activation is RowHammer-safe. This simultaneously triggers two lookup operations. First, RowBlocker checks the RowBlocker-BL to see if the row to be activated is blacklisted ( 2 ). A row is blacklisted if its activation rate exceeds a given threshold. We discuss how RowBlocker-BL estimates the activation rate of a row in Section 3.1.1. Second, RowBlocker checks RowBlocker-HB to see if the row has been recently activated ( 3 ). If a row is both blacklisted ( 4 ) and recently activated ( 5 ), RowBlocker responds to the memory request scheduler with a RowHammer-unsafe signal ( 6 ), consequently blocking the row activation. Blocking such a row activation is essential because allowing further activations to a blacklisted and recently-activated row could increase the row\\'s overall activation rate and thus result in RowHammer bit-flips. The memory request scheduler does not issue a row activation if RowBlocker returns unsafe. However, it keeps issuing the RowHammer-safe requests. This scheduling decision effectively prioritizes RowHammer-safe memory accesses over unsafe ones. An unsafe row activation becomes safe again as soon as a certain amount of time (t Delay ) passes after its latest activation, effectively limiting the row\\'s average activation rate to a RowHammer-safe value. After t Delay is satisfied, RowBlocker-HB no longer reports that the row has been recently activated ( 5 ), thereby allowing the memory request scheduler to issue the row activation ( 6 ). When the memory request scheduler issues a row activation ( 7 ), it simultaneously updates both RowBlocker-BL ( 8 ) and RowBlocker-HB ( 9 ). We explain how RowBlocker-BL and RowBlocker-HB work in Section 3.1.1 and 3.1.2, respectively. 3.1.1. RowBlocker-BL Mechanism. RowBlocker-BL uses two counting Bloom filters (CBF) in a time-interleaved fashion to decide whether a row should be blacklisted. Each CBF takes turns to make the blacklisting decision. A row is blacklisted when its activation rate exceeds a configurable threshold, which we call the blacklisting threshold (N BL ). When a CBF blacklists a row, any further activations targeting the row are throttled until the end of the CBF\\'s turn. In this subsection, we describe how a CBF works, how we use two CBFs to avoid stale blacklists, and how the two CBFs never fail to blacklist an aggressor row. Bloom Filter. A Bloom filter [11] is a space-efficient probabilistic data structure that is used for testing whether a set contains a particular element. A Bloom filter consists of a set of hash functions and a bit array on which it performs three operations: clear, insert, and test. Clearing a Bloom filter zeroes its bit array. To insert/test an element, each hash function evaluates an index into the bit array for the element, using an identifier for the element. Inserting an element sets the bits that the hash functions point to. Testing for an element checks whether all these bits are set. Since a hash function can yield the same set of indices for different elements (i.e., aliasing), testing a Bloom filter can return true for an element that was never inserted (i.e., false positive). However, the test operation never returns false for an inserted element (i.e., no false negatives). A Bloom filter eventually saturates (i.e., always returns true when tested for any element) if elements are continually inserted, which requires periodically clearing the filter and losing all inserted elements. Unified Bloom Filter (UBF). UBF [86] is a Bloom filter variant that allows a system to continuously track a set of elements that are inserted into a Bloom filter within the most recent time window of a fixed length (i.e., a rolling time window). Using a conventional Bloom filter to track a rolling time window could result in data loss whenever the Bloom filter is cleared, as the clearing eliminates the elements that still fall within the rolling time window. Instead, UBF continuously tracks insertions in a rolling time window by maintaining two Bloom filters and using them in a time-interleaved manner. UBF inserts every element into both filters, while the filters take turns in responding to test queries across consecutive limited time windows (i.e., epochs). UBF clears the filter which responds to test queries at the end of an epoch and redirects the test queries to the other filter for the next epoch. Therefore, each filter is cleared every other epoch (i.e., the filter\\'s lifetime is two epochs). By doing so, UBF ensures no false negatives for the elements that are inserted in a rolling time window of up to two epochs. Counting Bloom Filter (CBF). To track the number of times an element is inserted into the filter, another Bloom filter variant, called counting Bloom filters (CBF) [33], replaces the bit array with a counter array. Inserting an element in a CBF increments all of its corresponding counters. Testing an element returns the minimum value among all of the element\\'s corresponding counters, which represents an upper bound on the number of times an element was inserted into the filter. Due to aliasing, the test result can be larger than the true insertion count, but it cannot be smaller than that because counters are never decremented (i.e., false positives are possible, but false negatives are not). Combining UBF and CBF for Blacklisting. To estimate row activation rates with low area cost, RowBlocker-BL combines the ideas of UBF and CBF to form our dual counting Bloom filter (D-CBF). D-CBF maintains two CBFs in the time-interleaved manner of UBF. On every row activation, RowBlocker-BL inserts the activated row\\'s address into both CBFs. RowBlocker-BL considers a row to be blacklisted when the row\\'s activation count exceeds the blacklisting threshold (N BL ) in a rolling time window.\\nFigure 3 illustrates how RowBlocker-BL uses a D-CBF over time. RowBlocker-BL designates one of the CBFs as active and the other as passive. At any given time, only the active CBF responds to test queries. When a clear signal is received, D-CBF (1) clears only the active filter (e.g., CBF A at 3 ) and (2) swaps the active and passive filters (e.g., CBF A becomes passive and CBF B becomes active at 3 ). RowBlocker-BL blacklists a row if the row\\'s activation count in the active CBF exceeds the blacklisting threshold (N BL ). D-CBF Operation Walk-Through. We walk through D-CBF operation in Figure 3 from the perspective of a DRAM row. The counters that correspond to the row in both filters (CBF A and CBF B ) are initially zero ( 1 ). CBF A is the active filter, while CBF B is the passive filter. As the row\\'s activation count accumulates and reaches N BL ( 2 ), both CBF A and CBF B decide to blacklist the row. RowBlocker applies the active filter\\'s decision (CBF A ) and blacklists the row. As the counter values do not decrease, the row remains blacklisted until the end of Epoch 1. Therefore, a minimum delay is enforced between consecutive activations of this row between 2 and 3 . At the end of Epoch 1 ( 3 ), CBF A is cleared, and CBF B becomes the active filter. Note that CBF B immediately blacklists the row, as the counter values corresponding to the row in CBF B are still larger than N BL . Meanwhile, assuming that the row continues to be activated, the counters in CBF A again reach N BL ( 4 ). At the end of Epoch 2 ( 5 ), CBF A becomes the active filter again and immediately blacklists the row. By following this scheme, D-CBF blacklists the row as long as the row\\'s activation count exceeds N BL in an epoch. Assuming that the row\\'s activation count does not exceed N BL within Epoch 3, starting from 6 , the row is no longer blacklisted. Time-interleaving across the two CBFs ensures that BlockHammer maintains a fresh blacklist that never incorrectly excludes a DRAM row that needs to be blacklisted. Section 5 provides a generalized analytical proof of BlockHammer\\'s security guarantees that comprehensively studies all possible row activation patterns across all epochs.  To prevent any specific row from being repeatedly blacklisted due to its CBF counters aliasing with those of an aggressor row (i.e., due to a false positive), RowBlocker-BL alters the hash functions that each CBF uses whenever the CBF is cleared. To achieve this, RowBlocker-BL replaces the hash function\\'s seed value with a new randomly-generated value, as we explain next. Consequently, an aggressor row aliases with a different set of rows after every clear operation. Implementing Counting Bloom Filters. To periodically send a clear signal to D-CBF, RowBlocker-BL implements a clock register that stores the timestamp of the latest clear operation. In our implementation, each CBF contains 1024 elements of 12-bit saturating counters to count up to the blacklisting threshold N BL . We employ four area-and latency-efficient H3-class hash functions that consist of simple static bit-shift and mask operations [17]. We hardwire the static shift operation, so it does not require any logic gates. The mask operation performs a bitwise exclusive-OR on the shifted element (i.e., row address) and a seed. To alter the hash function when a CBF is cleared, RowBlocker simply replaces the hash function\\'s seed value with a randomly-generated value. 3.1.2. RowBlocker-HB Mechanism. RowBlocker-HB\\'s goal is to ensure that a blacklisted row cannot be activated often enough to cause a bit-flip. To ensure this, RowBlocker-HB delays a subsequent activation to a blacklisted row until the row\\'s last activation becomes older than a certain amount of time that we call t Delay . To do so, RowBlocker-HB maintains a first-in-first-out history buffer that stores a record of all row activations in the last t Delay time window. When RowBlocker queries RowBlocker-HB with a row address (i.e., 3 in Figure 2), RowBlocker-HB searches the row address in the history buffer and sets the \"Recently Activated?\" signal to true if the row address appears in the history buffer. Implementing RowBlocker-HB. We implement a per-DRAMrank history buffer as a circular queue using a head and a tail pointer. Each entry of this buffer stores (1) a row ID (which is unique in the rank), (2) a timestamp of when the entry was inserted into the buffer, and (3) a valid bit. The head and the tail pointers address the oldest and the youngest entries in the history buffer, respectively. When the memory request scheduler issues a row activation ( 7 in Figure 2), RowBlocker-HB inserts a new entry with the activated row address, the current timestamp, and a valid bit set to logic \\'1\\' into the history buffer and updates the tail pointer. RowBlocker-HB checks the timestamp of the oldest entry, indicated by the head pointer, every cycle. When the oldest entry becomes as old as t Delay , RowBlocker-HB invalidates the entry by resetting its valid bit to logic \\'0\\' and updates the head pointer. To test whether a row is recently activated ( 3 in Figure 2), RowBlocker-HB looks up the tested row address in each valid entry (i.e., an entry with a valid bit set to one) in parallel. To search the history buffer with low latency, we keep row addresses in a content-addressable memory array. Any matching valid entry means that the row has been activated within the last t Delay time window, so the new activation should not be issued if the row is blacklisted by RowBlocker-BL. We size the history buffer to be large enough to contain the worst-case number of row activations that need to be tested. The number of activations that can be performed in a DRAM rank is bounded by the timing parameter t FAW [ 1) and the maximum number of rows that RowBlocker must track within each epoch.\\nTo determine suitable values for each of the three parameters, we follow a three-step methodology that minimizes the cost of false positives for a given area budget. First, we empirically choose the CBF size based on false positive rates observed in our experiments (Section 7 discusses our experimental configuration). We choose a CBF size of 1K counters because we observe that reducing the CBF size below 1K significantly increases the false positive rate due to aliasing.\\nSecond, we configure N BL based on three goals: (1) N BL should be smaller than the RowHammer threshold to prevent RowHammer bit-flips; (2) N BL should be significantly larger than the per-row activation counts that benign applications exhibit in order to ensure that RowBlocker does not blacklist benign applications\\' row activations, even when accounting for false positives due to Bloom filter aliasing; and (3) N BL should be as low as possible to minimize t Delay (i.e., the time delay penalty for all activations to blacklisted rows, including those due to false positives) per Equation 1. To balance these three goals, we analyze the memory access patterns of 125 eight-core multiprogrammed workloads, each of which consists of eight randomly-chosen benign threads. We simulate these workloads using cycle-level simulation [77,125] for 200M instructions with a warmup period of 100M instructions on a 3.2 GHz system with 16 MB of last-level cache. We measure per-row activation rates by counting the activations that each row experiences within a 64 ms time window (i.e., one refresh window) starting from the row\\'s first activation. We observe that benign threads reach up to 78, 109, and 314 activations per row in a 64 ms time window for the 95th, 99th, and 100th percentile of the set of DRAM rows that are accessed at least once. Based on these observations, we set N BL to 8K for a RowHammer threshold of 32K, providing (1) RowHammer-safe operation, (2) an ample margin for row activations from benign threads to achieve a low false positive rate (less than 0.01%, as shown in Section 8.3), and (3) a reasonable worst-case t Delay penalty of 7.7 ¬µs for activations to blacklisted rows.\\nThird, we use Equation 1to choose a value for t CBF such that the resulting t Delay does not excessively penalize a mistakenly blacklisted row (i.e., a false positive). Increasing t CBF both (1) decreases t Delay (via Equation 1) and (2) extends the length of time for which a row is blacklisted. Therefore, we set t CBF equal to t REFW , which achieves as low a t Delay as possible without blacklisting a row past the point at which its potential victim rows have already been refreshed.\\nWe present the final values we choose for all BlockHammer parameters in conjunction with the DRAM timing parameters we use in Table 1 ',\n",
       "    'n_publication_ref': 13,\n",
       "    'n_figure_ref': 6},\n",
       "   {'heading': 'Area, Static Power, and Access Energy',\n",
       "    'text': \"Table 4 shows an area, static power, and access energy cost analysis of BlockHammer alongside six state-of-the-art RowHammer mitigation mechanisms [73,84,113,132,137,161], one of which is concurrent work with BlockHammer (Graphene [113]). We perform this analysis at two RowHammer thresholds (N RH ): 32K and 1K. 3 Main Components of BlockHammer. BlockHammer combines two mechanisms: RowBlocker and AttackThrottler. Row-Blocker, as shown in Figure 2, consists of two components (1) RowBlocker-BL, which implements a dual counting Bloom filter for each DRAM bank, and (2) RowBlocker-HB, which implements a row activation history buffer for each DRAM rank. When configured to handle a RowHammer threshold (N RH ) of 32K, as shown in Table 1, each counting Bloom filter has 1024 13-bit counters, stored in an SRAM array. These counters are indexed by four H3-class hash functions [17], which introduce negligible area overhead (discussed in Section 3.1.1). RowBlocker-HB's history buffer holds 887 entries per DRAM rank. Each entry contains 32 bits for a row ID, a timestamp, and a valid bit. AttackThrottler uses two counters per thread per DRAM bank to measure the RHLI of each <thread, bank> pair. We estimate BlockHammer's overall area overhead as 0.14 mm 2 per DRAM rank, for a 16-bank DDR4 memory. For a high-end 28-core Intel Xeon processor system with four memory channels and single-rank DDR4 DIMMs, BlockHammer consumes approximately 0.55 mm 2 , which translates to only 0.06% of the CPU die area [152]. When configured for an N RH of 1K, we reduce BlockHammer's blacklisting threshold (N BL ) from 8K to 512, reducing the CBF counter width from 13 bits to 9 bits. To avoid false positives at the reduced blacklisting threshold, we increase the CBF size to 8K. With this modification, BlockHammer's D-CBF consumes 0.74 mm 2 . Reducing N RH mandates larger time delays between subsequent row activations targeting a blacklisted row, thereby increasing the history buffer's size from 887 to 27.8K entries, which translates to 0.83 mm 2 chip area. Therefore, BlockHammer's total area overhead at an N RH of 1K is 1.57 mm 2 or 0.64% of the CPU die area [152]. Area Comparison. Graphene, TWiCe, and CBT need to store 5.22 kB, 37.12 kB, and 24.50 kB of metadata in the memory controller per DRAM rank, for the same 16-bank DDR4 memory, which translates to similarly low area overheads of 0.02%, 0.06%, and 0.08% of the CPU die area, respectively. Graphene's area overhead per byte of metadata is larger than other mechanisms because Graphene is fully implemented with CAM logic, as shown in Table 4. PARA, PRoHIT, and MRLoc are extremely area efficient compared to other mechanisms because they are probabilistic mechanisms [73,137,161], and thus do not need to store kilobytes of metadata to track row activation rates.  [137] and MRLoc [161] do not provide a concrete discussion on how to adjust their empirically-determined parameters for different NRH values. Therefore, we (1) report their values for a fixed design point that each paper provides for NRH =2K and (2) mark values we cannot estimate using an √ó. We repeat our area overhead analysis for future DRAM chips by scaling the RowHammer threshold down to 1K. While Block-Hammer consumes 1.57 mm 2 of chip area to prevent bit-flips at this lower threshold, TWiCe's and CBT's area overhead increases to 3.3x and 2.5x of BlockHammer's. We conclude that BlockHammer scales better than both CBT and TWiCe in terms of area overhead. Graphene's area overhead does not scale as efficiently as BlockHammer with decreasing RowHammer threshold, and becomes comparable to BlockHammer when configured for a RowHammer threshold of 1K. Static Power and Access Energy Comparison. When configured for an N RH of 32K, BlockHammer consumes 20.30 pJ per access, which is half of Graphene's access energy; and 22.27 mW of static power, which is 63% of CBT's. BlockHammer's static power consumption scales more efficiently than that of CBT and TWiCe as N RH decreases to 1K, whereas CBT and TWiCe consume 2.42x and 2.86x the static power of Block-Hammer, respectively. Similarly, Graphene's access energy and static power drastically increase by 22.56x and 30.2x, respectively, when N RH scales down to 1K. As a result, Graphene consumes 9.21√ó of BlockHammer's access energy.\",\n",
       "    'n_publication_ref': 16,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Latency Analysis',\n",
       "    'text': 'We implement BlockHammer in Verilog HDL and synthesize our design using Synopsys DC [143] with a 65 nm process technology to evaluate the latency impact on memory accesses. According to our RTL model, which we open source [124], BlockHammer responds to an \"Is this ACT RowHammer-safe?\" query ( 1 in Figure 2) in only 0.97 ns. This latency can be hidden because it is one-to-two orders of magnitude smaller than the row access latency (e.g., 45-50 ns) that DRAM standards (e.g., DDRx, LPDDRx, GDDRx) enforce [36,53,55].',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'BlockHammer: Preventing RowHammer at Low Cost by Blacklisting Rapidly-Accessed DRAM Rows',\n",
       "  'abstract': \"Aggressive memory density scaling causes modern DRAM devices to suffer from RowHammer, a phenomenon where rapidly activating (i.e., hammering) a DRAM row can cause bit-flips in physically-nearby rows. Recent studies demonstrate that modern DDR4/LPDDR4 DRAM chips, including chips previously marketed as RowHammer-safe, are even more vulnerable to RowHammer than older DDR3 DRAM chips. Many works show that attackers can exploit RowHammer bit-flips to reliably mount system-level attacks to escalate privilege and leak private data. Therefore, it is critical to ensure RowHammersafe operation on all DRAM-based systems as they become increasingly more vulnerable to RowHammer. Unfortunately, state-of-the-art RowHammer mitigation mechanisms face two major challenges. First, they incur increasingly higher performance and/or area overheads when applied to more vulnerable DRAM chips. Second, they require either closely-guarded proprietary information about the DRAM chips' physical circuit layouts or modifications to the DRAM chip design. In this paper, we show that it is possible to efficiently and scalably prevent RowHammer bit-flips without knowledge of or modification to DRAM internals. To this end, we introduce BlockHammer, a low-cost, effective, and easy-to-adopt Row-Hammer mitigation mechanism that prevents all RowHammer bit-flips while overcoming the two key challenges. BlockHammer selectively throttles memory accesses that could otherwise potentially cause RowHammer bit-flips. The key idea of Block-Hammer is to (1) track row activation rates using area-efficient Bloom filters, and (2) use the tracking data to ensure that no row is ever activated rapidly enough to induce RowHammer bit-flips. By guaranteeing that no DRAM row ever experiences a RowHammer-unsafe activation rate, BlockHammer (1) makes it impossible for a RowHammer bit-flip to occur and (2) greatly reduces a RowHammer attack's impact on the performance of co-running benign applications. Our evaluations across a comprehensive range of 280 workloads show that, compared to the best of six state-of-the-art RowHammer mitigation mechanisms (all of which require knowledge of or modification to DRAM internals), BlockHammer provides (1) competitive performance and energy when the system is not under a RowHammer attack and (2) significantly better performance and energy when the system is under a RowHammer attack.\",\n",
       "  'paddleOCR': [[[[340.0, 22.0], [525.0, 22.0], [525.0, 47.0], [340.0, 47.0]],\n",
       "    ['RowBlocker', 0.9986797571182251]],\n",
       "   [[[598.0, 56.0], [808.0, 54.0], [809.0, 76.0], [599.0, 79.0]],\n",
       "    ['RowBlocker-BL', 0.99881911277771]],\n",
       "   [[[888.0, 57.0], [1024.0, 57.0], [1024.0, 80.0], [888.0, 80.0]],\n",
       "    ['Blacklisted?', 0.9999203681945801]],\n",
       "   [[[427.0, 81.0], [491.0, 81.0], [491.0, 100.0], [427.0, 100.0]],\n",
       "    ['Test ', 0.997010350227356]],\n",
       "   [[[591.0, 84.0], [610.0, 84.0], [608.0, 181.0], [589.0, 181.0]],\n",
       "    ['Functions', 0.9982001185417175]],\n",
       "   [[[635.0, 82.0], [773.0, 81.0], [773.0, 99.0], [635.0, 100.0]],\n",
       "    ['(per DRAM bank)', 0.9992229342460632]],\n",
       "   [[[566.0, 105.0], [586.0, 106.0], [583.0, 164.0], [564.0, 163.0]],\n",
       "    ['Hash', 0.9989087581634521]],\n",
       "   [[[178.0, 117.0], [300.0, 117.0], [300.0, 140.0], [178.0, 140.0]],\n",
       "    ['Is this ACT', 0.9818263649940491]],\n",
       "   [[[187.0, 150.0], [292.0, 150.0], [292.0, 172.0], [187.0, 172.0]],\n",
       "    ['RH-Safe?', 0.999592125415802]],\n",
       "   [[[422.0, 146.0], [494.0, 146.0], [494.0, 169.0], [422.0, 169.0]],\n",
       "    ['Insert', 0.9998384118080139]],\n",
       "   [[[23.0, 158.0], [145.0, 163.0], [144.0, 190.0], [22.0, 185.0]],\n",
       "    ['Memory', 0.999779999256134]],\n",
       "   [[[365.0, 159.0], [388.0, 159.0], [388.0, 185.0], [365.0, 185.0]],\n",
       "    ['8', 0.9998180270195007]],\n",
       "   [[[432.0, 178.0], [481.0, 178.0], [481.0, 202.0], [432.0, 202.0]],\n",
       "    ['ACT', 0.807957112789154]],\n",
       "   [[[26.0, 200.0], [143.0, 200.0], [143.0, 227.0], [26.0, 227.0]],\n",
       "    ['Request', 0.9997625350952148]],\n",
       "   [[[937.0, 208.0], [962.0, 208.0], [962.0, 234.0], [937.0, 234.0]],\n",
       "    ['&', 0.9992585778236389]],\n",
       "   [[[13.0, 237.0], [153.0, 239.0], [153.0, 266.0], [13.0, 264.0]],\n",
       "    ['Scheduler', 0.9996720552444458]],\n",
       "   [[[177.0, 254.0], [300.0, 254.0], [300.0, 277.0], [177.0, 277.0]],\n",
       "    ['RH-Unsafe', 0.9994061589241028]],\n",
       "   [[[424.0, 257.0], [511.0, 257.0], [511.0, 280.0], [424.0, 280.0]],\n",
       "    ['Search ', 0.9969345927238464]],\n",
       "   [[[596.0, 267.0], [812.0, 267.0], [812.0, 290.0], [596.0, 290.0]],\n",
       "    ['RowBlocker-HB', 0.9983662962913513]],\n",
       "   [[[428.0, 326.0], [500.0, 326.0], [500.0, 349.0], [428.0, 349.0]],\n",
       "    ['Insert', 0.9998529553413391]],\n",
       "   [[[439.0, 358.0], [488.0, 358.0], [488.0, 381.0], [439.0, 381.0]],\n",
       "    ['ACT', 0.953436553478241]],\n",
       "   [[[593.0, 365.0], [659.0, 365.0], [659.0, 384.0], [593.0, 384.0]],\n",
       "    ['Row ID', 0.9895732998847961]],\n",
       "   [[[689.0, 366.0], [797.0, 366.0], [797.0, 384.0], [689.0, 384.0]],\n",
       "    ['Timestamp', 0.9995329976081848]],\n",
       "   [[[891.0, 360.0], [994.0, 360.0], [994.0, 386.0], [891.0, 386.0]],\n",
       "    ['Recently', 0.9997985363006592]],\n",
       "   [[[202.0, 380.0], [276.0, 380.0], [276.0, 404.0], [202.0, 404.0]],\n",
       "    ['Issued', 0.9997894167900085]],\n",
       "   [[[882.0, 391.0], [1003.0, 391.0], [1003.0, 414.0], [882.0, 414.0]],\n",
       "    ['Activated?', 0.9999486804008484]],\n",
       "   [[[196.0, 414.0], [280.0, 411.0], [281.0, 435.0], [197.0, 437.0]],\n",
       "    ['an ACT', 0.9913084506988525]]],\n",
       "  'ocr': [[[422.0, 146.0], [494.0, 146.0], [494.0, 169.0], [422.0, 169.0]],\n",
       "   ['Insert', 0.9998384118080139]]},\n",
       " '2102.05444v2-Figure2-1.png': {'caption': 'Figure 2: An overview of the approach with exemplary outputs of the individual phases.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2102.05444v2-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Approach Overview',\n",
       "    'text': 'Fig. 2 gives an overview of our extraction approach. The input of the approach is a dump of Wikipedia as well as an associated knowledge graph. In the Subject Entity Discovery phase, listings and their context are extracted from the Wikipedia dump and subject entities are identified (Sec. 4.3). Subsequently, the existing information in the knowledge graph is used to mine descriptive rules from the extracted listings (Sec. 4.4). Finally, the rules are applied to all the listings in Wikipedia in order to extract new type and relation assertions (Sec. 4.5).',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Information Extraction From Co-Occurring Similar Entities',\n",
       "  'abstract': \"Knowledge about entities and their interrelations is a crucial factor of success for tasks like question answering or text summarization. Publicly available knowledge graphs like Wikidata or DBpedia are, however, far from being complete. In this paper, we explore how information extracted from similar entities that co-occur in structures like tables or lists can help to increase the coverage of such knowledge graphs. In contrast to existing approaches, we do not focus on relationships within a listing (e.g., between two entities in a table row) but on the relationship between a listing's subject entities and the context of the listing. To that end, we propose a descriptive rule mining approach that uses distant supervision to derive rules for these relationships based on a listing's context. Extracted from a suitable data corpus, the rules can be used to extend a knowledge graph with novel entities and assertions. In our experiments we demonstrate that the approach is able to extract up to 3M novel entities and 30M additional assertions from listings in Wikipedia. We find that the extracted information is of high quality and thus suitable to extend Wikipedia-based knowledge graphs like DBpedia, YAGO, and CaLiGraph. For the case of DBpedia, this would result in an increase of covered entities by roughly 50%.\",\n",
       "  'paddleOCR': [[[[690.0, 107.0],\n",
       "     [888.0, 107.0],\n",
       "     [888.0, 139.0],\n",
       "     [690.0, 139.0]],\n",
       "    ['Subject Entity', 0.9835385084152222]],\n",
       "   [[[1246.0, 100.0], [1414.0, 105.0], [1413.0, 144.0], [1245.0, 138.0]],\n",
       "    ['Descriptive', 0.9994191527366638]],\n",
       "   [[[1694.0, 107.0], [1998.0, 107.0], [1998.0, 136.0], [1694.0, 136.0]],\n",
       "    ['Assertion Generation', 0.9817914962768555]],\n",
       "   [[[89.0, 136.0], [242.0, 141.0], [241.0, 179.0], [88.0, 174.0]],\n",
       "    ['Knowledge', 0.9998798370361328]],\n",
       "   [[[291.0, 143.0], [425.0, 143.0], [425.0, 174.0], [291.0, 174.0]],\n",
       "    ['Wikipedia', 0.9988391399383545]],\n",
       "   [[[712.0, 138.0], [865.0, 146.0], [863.0, 184.0], [711.0, 176.0]],\n",
       "    ['Discovery', 0.9995227456092834]],\n",
       "   [[[1240.0, 136.0], [1419.0, 144.0], [1417.0, 182.0], [1239.0, 174.0]],\n",
       "    ['Rule Mining', 0.9998703598976135]],\n",
       "   [[[1751.0, 136.0], [1938.0, 144.0], [1936.0, 184.0], [1750.0, 176.0]],\n",
       "    ['and Filtering', 0.9967463612556458]],\n",
       "   [[[123.0, 181.0], [213.0, 181.0], [213.0, 212.0], [123.0, 212.0]],\n",
       "    ['Graph', 0.9991218447685242]],\n",
       "   [[[312.0, 178.0], [399.0, 184.0], [397.0, 215.0], [310.0, 210.0]],\n",
       "    ['Dump', 0.998917818069458]],\n",
       "   [[[547.0, 237.0], [763.0, 237.0], [763.0, 268.0], [547.0, 268.0]],\n",
       "    ['(1) Page Entity', 0.9910427331924438]],\n",
       "   [[[790.0, 230.0], [940.0, 235.0], [939.0, 273.0], [788.0, 268.0]],\n",
       "    ['(4) Listing', 0.9641700387001038]],\n",
       "   [[[547.0, 275.0], [765.0, 275.0], [765.0, 306.0], [547.0, 306.0]],\n",
       "    ['(2) Top Section', 0.9978256225585938]],\n",
       "   [[[795.0, 271.0], [1037.0, 271.0], [1037.0, 309.0], [795.0, 309.0]],\n",
       "    ['[ Subject Entity', 0.9229423403739929]],\n",
       "   [[[547.0, 313.0], [705.0, 313.0], [705.0, 344.0], [547.0, 344.0]],\n",
       "    ['(3) Section', 0.9962784051895142]],\n",
       "   [[[1843.0, 340.0], [1955.0, 340.0], [1955.0, 371.0], [1843.0, 371.0]],\n",
       "    ['RULES', 0.9968582987785339]],\n",
       "   [[[101.0, 387.0], [341.0, 387.0], [341.0, 432.0], [101.0, 432.0]],\n",
       "    ['Gilby Clarke', 0.9839641451835632]],\n",
       "   [[[565.0, 391.0], [797.0, 391.0], [797.0, 429.0], [565.0, 429.0]],\n",
       "    ['Gilby Clarke', 0.9743983745574951]],\n",
       "   [[[983.0, 391.0], [1028.0, 391.0], [1028.0, 429.0], [983.0, 429.0]],\n",
       "    ['(1)', 0.9981333613395691]],\n",
       "   [[[95.0, 469.0], [300.0, 474.0], [299.0, 512.0], [95.0, 507.0]],\n",
       "    ['Discography', 0.999634325504303]],\n",
       "   [[[557.0, 469.0], [757.0, 474.0], [756.0, 512.0], [556.0, 507.0]],\n",
       "    ['Discography', 0.9976358413696289]],\n",
       "   [[[985.0, 474.0], [1028.0, 474.0], [1028.0, 514.0], [985.0, 514.0]],\n",
       "    ['(2)', 0.9992432594299316]],\n",
       "   [[[106.0, 550.0], [468.0, 552.0], [468.0, 584.0], [106.0, 581.0]],\n",
       "    [\"Albums with Guns N'Roses\", 0.9506769180297852]],\n",
       "   [[[567.0, 550.0], [927.0, 552.0], [927.0, 584.0], [567.0, 581.0]],\n",
       "    ['Albums with Guns N Roses', 0.9520375728607178]],\n",
       "   [[[985.0, 550.0], [1026.0, 550.0], [1026.0, 590.0], [985.0, 590.0]],\n",
       "    ['(3)', 0.9993863701820374]],\n",
       "   [[[1125.0, 552.0], [1509.0, 552.0], [1509.0, 584.0], [1125.0, 584.0]],\n",
       "    ['StopSection.{Discography}', 0.958130955696106]],\n",
       "   [[[1653.0, 548.0], [1981.0, 548.0], [1981.0, 579.0], [1653.0, 579.0]],\n",
       "    ['TheSpaghettiIncident?', 0.9992846250534058]],\n",
       "   [[[110.0, 593.0], [444.0, 588.0], [444.0, 617.0], [110.0, 622.0]],\n",
       "    ['- The Spaghetti Incident? --', 0.9640238881111145]],\n",
       "   [[[573.0, 590.0], [892.0, 590.0], [892.0, 622.0], [573.0, 622.0]],\n",
       "    [\"Che' Spagnetl Incident??\", 0.8426106572151184]],\n",
       "   [[[1216.0, 593.0], [1422.0, 593.0], [1422.0, 622.0], [1216.0, 622.0]],\n",
       "    ['MusicalWork', 0.956798791885376]],\n",
       "   [[[1703.0, 590.0], [1933.0, 590.0], [1933.0, 620.0], [1703.0, 620.0]],\n",
       "    ['is-aMusicalWork', 0.9831485152244568]],\n",
       "   [[[985.0, 608.0], [1026.0, 608.0], [1026.0, 649.0], [985.0, 649.0]],\n",
       "    ['(4)', 0.9991981983184814]],\n",
       "   [[[112.0, 628.0], [321.0, 628.0], [321.0, 658.0], [112.0, 658.0]],\n",
       "    ['- Greatest Hits --', 0.938691258430481]],\n",
       "   [[[580.0, 633.0], [763.0, 628.0], [763.0, 653.0], [580.0, 658.0]],\n",
       "    ['Greatest Hits!', 0.9756194949150085]],\n",
       "   [[[1608.0, 635.0], [2028.0, 635.0], [2028.0, 664.0], [1608.0, 664.0]],\n",
       "    ['GreatestHits is-aMusicalyork', 0.9688858389854431]],\n",
       "   [[[104.0, 664.0], [457.0, 667.0], [457.0, 698.0], [103.0, 696.0]],\n",
       "    ['Albums with NancySinatra', 0.9751297831535339]],\n",
       "   [[[567.0, 664.0], [918.0, 667.0], [918.0, 698.0], [567.0, 696.0]],\n",
       "    ['Albums with Nancy Slnatra', 0.9636194109916687]],\n",
       "   [[[985.0, 664.0], [1026.0, 664.0], [1026.0, 705.0], [985.0, 705.0]],\n",
       "    ['(3)', 0.9987582564353943]],\n",
       "   [[[1144.0, 664.0], [1491.0, 664.0], [1491.0, 693.0], [1144.0, 693.0]],\n",
       "    ['3iopSection.{Discography\"}', 0.883807897567749]],\n",
       "   [[[1593.0, 678.0], [2043.0, 678.0], [2043.0, 707.0], [1593.0, 707.0]],\n",
       "    ['CaliforniaGirlis-aMusicalWork', 0.9694995880126953]],\n",
       "   [[[572.0, 693.0], [765.0, 698.0], [764.0, 743.0], [571.0, 738.0]],\n",
       "    ['(Calltornia Gir!', 0.809779167175293]],\n",
       "   [[[112.0, 705.0], [325.0, 705.0], [325.0, 734.0], [112.0, 734.0]],\n",
       "    ['- California Gir1 --', 0.9453360438346863]],\n",
       "   [[[985.0, 705.0], [1026.0, 705.0], [1026.0, 745.0], [985.0, 745.0]],\n",
       "    ['(4)', 0.9994209408760071]],\n",
       "   [[[1056.0, 702.0], [1573.0, 702.0], [1573.0, 731.0], [1056.0, 731.0]],\n",
       "    ['3scction.{Albnms with GnnsNRoses}', 0.9176843762397766]],\n",
       "   [[[112.0, 740.0], [306.0, 740.0], [306.0, 772.0], [112.0, 772.0]],\n",
       "    ['Solo albums', 0.962721049785614]],\n",
       "   [[[571.0, 743.0], [765.0, 743.0], [765.0, 772.0], [571.0, 772.0]],\n",
       "    ['Solo albums', 0.9755094051361084]],\n",
       "   [[[985.0, 745.0], [1026.0, 745.0], [1026.0, 785.0], [985.0, 785.0]],\n",
       "    ['(3)', 0.9993185997009277]],\n",
       "   [[[1265.0, 738.0], [1371.0, 738.0], [1371.0, 769.0], [1265.0, 769.0]],\n",
       "    ['Album', 0.8489971160888672]],\n",
       "   [[[1580.0, 760.0], [2052.0, 760.0], [2052.0, 792.0], [1580.0, 792.0]],\n",
       "    ['TheSpaghettiIncident? is-a Albua', 0.9707218408584595]],\n",
       "   [[[119.0, 781.0], [207.0, 781.0], [207.0, 812.0], [119.0, 812.0]],\n",
       "    ['Name', 0.9985716342926025]],\n",
       "   [[[278.0, 783.0], [334.0, 783.0], [334.0, 810.0], [278.0, 810.0]],\n",
       "    ['Year', 0.99649578332901]],\n",
       "   [[[586.0, 781.0], [664.0, 781.0], [664.0, 812.0], [586.0, 812.0]],\n",
       "    ['Name', 0.9981132745742798]],\n",
       "   [[[739.0, 783.0], [800.0, 783.0], [800.0, 812.0], [739.0, 812.0]],\n",
       "    ['Year', 0.9986870288848877]],\n",
       "   [[[1655.0, 798.0], [1983.0, 798.0], [1983.0, 828.0], [1655.0, 828.0]],\n",
       "    ['GreatestHits is-a Album', 0.982488214969635]],\n",
       "   [[[121.0, 819.0], [222.0, 819.0], [222.0, 850.0], [121.0, 850.0]],\n",
       "    ['Rubber', 0.9987773895263672]],\n",
       "   [[[987.0, 821.0], [1024.0, 821.0], [1024.0, 852.0], [987.0, 852.0]],\n",
       "    ['(4)', 0.9844379425048828]],\n",
       "   [[[1119.0, 812.0], [1504.0, 812.0], [1504.0, 843.0], [1119.0, 843.0]],\n",
       "    ['topSection.{Discography\"}', 0.9591882228851318]],\n",
       "   [[[121.0, 857.0], [198.0, 857.0], [198.0, 888.0], [121.0, 888.0]],\n",
       "    ['Swag', 0.9997264742851257]],\n",
       "   [[[1125.0, 848.0], [1496.0, 850.0], [1496.0, 888.0], [1125.0, 886.0]],\n",
       "    ['3section.{Solo albums}', 0.955313503742218]],\n",
       "   [[[1640.0, 868.0], [1994.0, 868.0], [1994.0, 897.0], [1640.0, 897.0]],\n",
       "    ['Rubber artist GilbyClarke', 0.979137659072876]],\n",
       "   [[[1125.0, 892.0], [1498.0, 895.0], [1498.0, 933.0], [1125.0, 930.0]],\n",
       "    ['3artist.{<PageEntity >}', 0.9627289772033691]],\n",
       "   [[[1653.0, 906.0], [1978.0, 904.0], [1979.0, 935.0], [1653.0, 937.0]],\n",
       "    ['Swag artist GilbyClarke', 0.9751940965652466]],\n",
       "   [[[1306.0, 946.0], [1330.0, 946.0], [1330.0, 964.0], [1306.0, 964.0]],\n",
       "    ['...', 0.6866784691810608]],\n",
       "   [[[1804.0, 946.0], [1830.0, 946.0], [1830.0, 964.0], [1804.0, 964.0]],\n",
       "    ['...', 0.72178053855896]]],\n",
       "  'ocr': [[[557.0, 469.0], [757.0, 474.0], [756.0, 512.0], [556.0, 507.0]],\n",
       "   ['Discography', 0.9976358413696289]]},\n",
       " '2101.09491v3-Figure14-1.png': {'caption': 'Figure 14 Stages of a DT indicating Stage 4 as the current model presented within this publication.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2101.09491v3-Figure14-1.png',\n",
       "  'sections': [{'heading': 'Digital Twin',\n",
       "    'text': 'A DT is defined as \"digital replications of living as well as non-living entities that enable data to be seamlessly transmitted between the physical and virtual worlds\" [97]. We report a \"Stage 4\" DT with extended data analytics and simulation capabilities, in particular leveraging edgeprocessing in real-time to predict future behaviors (Figure 14). A DT designed according to this paradigm ensures positive interdependency across its internal and external functions, allowing integration of real-time sensor data streaming and processing with other operational RAS/I inputs and services. It ensures legitimacy is maintained in and across existing technology ecosystems.\\nHastie et al. cite three main challenges for human-robot collaboration, which this work addresses: planning in human-robot teams, executing and monitoring a task and adaptivity of the human-robot partnership [89]. Meeting these challenges requires pre-mission planning, situation monitoring with the ability to manually assume control, if necessary, and the ability to re-synchronize with the robot if communications are lost.\\nThe common prevalence of internet connectivity and the increasing number of cloud computing solutions available have enabled the rapid development of cloud robotics [98]. The technology is fundamental to DTs and offers an extremely powerful computing platform without the associated hardware costs. Importantly, it allows ease of integration and communication with edge-devices and robots, including human-robot interfacing. The following subsections describe the functionality of the DT used for this mission evaluation.',\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Symbiotic System of Systems Design for Safe and Resilient Autonomous Robotics in Offshore Wind Farms',\n",
       "  'abstract': 'To reduce Operation and Maintenance (O&M) costs on offshore wind farms, wherein 80% of the O&M cost relates to deploying personnel, the offshore wind sector looks to Robotics and Artificial Intelligence (RAI) for solutions. Barriers to Beyond Visual Line of Sight (BVLOS) robotics include operational safety compliance and resilience, inhibiting the commercialization of autonomous services offshore. To address safety and resilience challenges we propose a Symbiotic System Of Systems Approach (SSOSA), reflecting the lifecycle learning and co-evolution with knowledge sharing for mutual gain of robotic platforms and remote human operators. Our novel methodology enables the run-time verification of safety, reliability and resilience during autonomous missions. To achieve this, a Symbiotic Digital Architecture (SDA) was developed to synchronize digital models of the robot, environment, infrastructure, and integrate front-end analytics and bidirectional communication for autonomous adaptive mission planning and situation reporting to a remote operator. A reliability ontology for the deployed robot, based on our holistic hierarchicalrelational model, supports computationally efficient platform data analysis. We demonstrate an asset inspection mission within a confined space through Cooperative, Collaborative and Corroborative (C 3 ) governance (internal and external symbiosis) via decision-making processes and the associated structures. We create a hyper enabled human interaction capability to analyze the mission status, diagnostics of critical sub-systems within the robot to provide automatic updates to our AI-driven run-time reliability ontology. This enables faults to be translated into failure modes for decision-making during the mission. Our results demonstrate that the symbiotic system of systems methodology provides enhanced run-time operational resilience and safety compliance to BVLOS autonomous missions. The SSOSA is highly transferable to other mission scenarios and technologies, providing a pathway to implementing scalable and diverse autonomous services.',\n",
       "  'paddleOCR': [[[[190.0, 36.0], [327.0, 36.0], [327.0, 60.0], [190.0, 60.0]],\n",
       "    ['Status Twin', 0.9762125611305237]],\n",
       "   [[[191.0, 71.0], [423.0, 71.0], [423.0, 94.0], [191.0, 94.0]],\n",
       "    ['Status of a fixed size', 0.9491228461265564]],\n",
       "   [[[43.0, 99.0], [137.0, 105.0], [135.0, 141.0], [41.0, 135.0]],\n",
       "    ['Stage', 0.999596893787384]],\n",
       "   [[[190.0, 106.0], [440.0, 109.0], [440.0, 132.0], [190.0, 130.0]],\n",
       "    ['data set.manual setup', 0.9486055970191956]],\n",
       "   [[[188.0, 201.0], [412.0, 199.0], [413.0, 226.0], [189.0, 229.0]],\n",
       "    ['Operational Twins', 0.9806481599807739]],\n",
       "   [[[768.0, 218.0], [951.0, 218.0], [951.0, 257.0], [768.0, 257.0]],\n",
       "    ['Completed', 0.9338181614875793]],\n",
       "   [[[189.0, 236.0], [413.0, 238.0], [412.0, 261.0], [188.0, 260.0]],\n",
       "    ['Flexible data model', 0.9497065544128418]],\n",
       "   [[[41.0, 266.0], [140.0, 273.0], [137.0, 306.0], [38.0, 299.0]],\n",
       "    ['Stage', 0.9995476007461548]],\n",
       "   [[[188.0, 273.0], [441.0, 276.0], [441.0, 303.0], [187.0, 300.0]],\n",
       "    ['storing past processes', 0.9732561111450195]],\n",
       "   [[[767.0, 269.0], [881.0, 273.0], [879.0, 313.0], [766.0, 308.0]],\n",
       "    ['Stages', 0.9974663853645325]],\n",
       "   [[[190.0, 370.0], [577.0, 369.0], [577.0, 396.0], [190.0, 397.0]],\n",
       "    ['Operational Twins With Events', 0.9581071138381958]],\n",
       "   [[[188.0, 405.0], [487.0, 405.0], [487.0, 431.0], [188.0, 431.0]],\n",
       "    ['ML to identify events and', 0.9526296854019165]],\n",
       "   [[[42.0, 431.0], [144.0, 441.0], [140.0, 474.0], [39.0, 464.0]],\n",
       "    ['Stage.', 0.9632167220115662]],\n",
       "   [[[186.0, 443.0], [423.0, 442.0], [423.0, 466.0], [186.0, 467.0]],\n",
       "    ['predict future events', 0.9723328351974487]],\n",
       "   [[[188.0, 532.0], [619.0, 531.0], [619.0, 554.0], [188.0, 555.0]],\n",
       "    ['Operational Twins With Simulation', 0.9604132771492004]],\n",
       "   [[[768.0, 555.0], [899.0, 560.0], [897.0, 600.0], [766.0, 595.0]],\n",
       "    ['Cutting', 0.9865028262138367]],\n",
       "   [[[190.0, 570.0], [617.0, 570.0], [617.0, 593.0], [190.0, 593.0]],\n",
       "    ['Extended virtualization with process', 0.9617219567298889]],\n",
       "   [[[40.0, 599.0], [170.0, 603.0], [169.0, 644.0], [39.0, 640.0]],\n",
       "    ['Stage4', 0.9969368577003479]],\n",
       "   [[[187.0, 605.0], [613.0, 603.0], [614.0, 630.0], [187.0, 632.0]],\n",
       "    ['system simulation and edge analytics', 0.9826570153236389]],\n",
       "   [[[765.0, 605.0], [859.0, 610.0], [857.0, 653.0], [762.0, 648.0]],\n",
       "    ['Edge', 0.9996048808097839]],\n",
       "   [[[191.0, 701.0], [537.0, 704.0], [536.0, 730.0], [191.0, 727.0]],\n",
       "    ['Twins with Business Models', 0.9520379304885864]],\n",
       "   [[[192.0, 739.0], [481.0, 739.0], [481.0, 765.0], [192.0, 765.0]],\n",
       "    ['Operational improvement', 0.9879671335220337]],\n",
       "   [[[40.0, 768.0], [169.0, 772.0], [168.0, 809.0], [39.0, 805.0]],\n",
       "    ['Stage5', 0.8949351906776428]],\n",
       "   [[[189.0, 775.0], [461.0, 777.0], [461.0, 800.0], [188.0, 797.0]],\n",
       "    ['with financial outcomes', 0.9424342513084412]],\n",
       "   [[[765.0, 802.0], [883.0, 807.0], [881.0, 843.0], [764.0, 838.0]],\n",
       "    ['Future', 0.9965893626213074]],\n",
       "   [[[769.0, 855.0], [866.0, 855.0], [866.0, 893.0], [769.0, 893.0]],\n",
       "    ['Work', 0.9990044832229614]],\n",
       "   [[[191.0, 871.0], [420.0, 870.0], [421.0, 894.0], [191.0, 895.0]],\n",
       "    ['AutonomousTwins', 0.9945814609527588]],\n",
       "   [[[191.0, 905.0], [517.0, 905.0], [517.0, 931.0], [191.0, 931.0]],\n",
       "    ['Automatic self-improvement', 0.9849296808242798]],\n",
       "   [[[40.0, 932.0], [173.0, 937.0], [171.0, 978.0], [39.0, 972.0]],\n",
       "    ['Stage 6', 0.9477539658546448]],\n",
       "   [[[194.0, 941.0], [491.0, 941.0], [491.0, 967.0], [194.0, 967.0]],\n",
       "    ['through analytical models', 0.9714694023132324]]],\n",
       "  'ocr': [[[191.0, 701.0], [537.0, 704.0], [536.0, 730.0], [191.0, 727.0]],\n",
       "   ['Twins with Business Models', 0.9520379304885864]]},\n",
       " '2011.14347v2-Figure1-1.png': {'caption': 'Fig. 1: Illustration of the proposed semi-supervised ssGAN model. As opposed to fully-supervised models that demand Nyquist-sampled acquisitions for training (1), ssGAN learns to synthesize high-quality images given a dataset of undersampled source and target acquisitions (2). ssGAN initially synthesizes a coil-combined target image that is backprojected onto individual coils via sensitivity maps. These multi-coil target images are subsampled in Fourier domain with the target acquisition mask in order to define the selective multi-coil tensor losses in image, k-space and adversarial domains (3).',\n",
       "  'imageText': [],\n",
       "  'image_file': '2011.14347v2-Figure1-1.png',\n",
       "  'sections': [{'heading': 'B. Semi-Supervised Generative Adversarial Networks',\n",
       "    'text': 'Here, we propose a novel semi-supervised GAN model, namely ssGAN, to mitigate the dependency of MRI synthesis models on supervised training with Nyquist-sampled source and target acquisitions. ssGAN is trained on undersampled acquisitions of source and target contrasts, and it synthesizes multi-coil target images directly from undersampled multicoil acquisitions of the source contrast. To do this, ssGAN introduces novel selective loss functions expressed based on only the acquired subset of k-space samples in the target contrast (Fig. 1). Details regarding the optimization objectives of ssGAN are provided in the remainder of this section.  (1), ssGAN learns to synthesize high-quality images given a dataset of undersampled source and target acquisitions (2). ssGAN initially synthesizes a coil-combined target image that is backprojected onto individual coils via sensitivity maps. These multi-coil target images are subsampled in Fourier domain with the target acquisition mask in order to define the selective multi-coil tensor losses in image, k-space and adversarial domains (3). ssGAN receives as input Fourier reconstructions of either fully-sampled or undersampled acquisitions of the source contrast, and learns to synthesize high-quality images of the target contrast. The generator G in ssGAN produces target contrast images via a forward mapping:\\nG(X n Œõ ) =≈∑, with X n Œõ = {x 1 Œõ , . . . , x n Œõ } (4)\\nwhere X n Œõ denotes multi-coil source contrast images acquired with a k-space sampling mask Œõ, n denotes the number of receive coils with sensitivity mapsƒà n X computed via ESPIRiT [48], and≈∑ denotes the synthesized coil-combined target contrast image. Note that ssGAN considers that only undersampled acquisitions of the target contrast are available, where Y m ‚Ñ¶ = {y 1 ‚Ñ¶ , . . . , y m ‚Ñ¶ } denotes Fourier reconstructions of multi-coil target acquisitions collected with a sampling mask ‚Ñ¶ and m receive coils of true coil sensitivities C m Y . As no high-quality reference for the target contrast image is assumed, ssGAN expresses novel selective loss functions based on only the acquired subset of k-space samples. To do this, the synthesized coil-combined image is first projected onto individual coils as follows:\\nY m = P (≈∑,ƒà m Y ) =≈∑ ‚Ä¢ƒà m Y (5)\\nwhere≈∂ m denotes the synthesized multi-coil target contrast images,ƒà m Y denotes estimated coil sensitivity maps computed via ESPIRiT [48], and P is the operator that performs the coil projection in the image domain as dot product takes vectors and outputs a scalar, element-wise multiplication between the input image and coil sensitivity maps. The multi-coil target image projections are then subjected to the binary sampling mask in Fourier domain:\\nk Y m ‚Ñ¶ = M (F(≈∂ m ), ‚Ñ¶) = F(≈∂ m ) ‚Ä¢ Œ© Y m ‚Ñ¶ = F ‚àí1 (k Y m ‚Ñ¶ )(6)\\nwhere F denotes the forward and F ‚àí1 denotes the inverse Fourier transform, M is the operator that performs binary masking in k-space to with a given sampling mask. In Eq. (6)k Y m ‚Ñ¶ and≈∂ m ‚Ñ¶ denote undersampled multi-coil data respectively in k-space and image domain for the synthesized target contrast image. The selective loss function in ssGAN is then defined between undersampled synthesized and undersampled ground truth data for the target contrast, based on three loss components: multi-coil tensor image loss, multi-coil tensor kspace loss, and multi-coil tensor adversarial loss. Each loss term is described below.\\n1) Multi-Coil Tensor Image Loss: The first component of the selective loss function is a multi-coil tensor image loss defined based on undersampled multi-coil data in image domain, between synthesized and ground truth target images:\\nL i = E X n Œõ ,Y m ‚Ñ¶ [||≈∂ m ‚Ñ¶ ‚àí Y m ‚Ñ¶ || 1 ](7)\\nwhere Y m ‚Ñ¶ denotes the multi-coil ground truth target images from accelerated acquisitions, and≈∂ m ‚Ñ¶ denotes the undersampled target images generated by ssGAN.\\n2) Multi-Coil Tensor k-space Loss: The quality of the synthesized images in ssGAN is further enhanced via a multi-coil tensor k-space loss expressed between the Fourierdomain data of the synthesized and ground truth images.\\nL k = E X n Œõ ,Y m ‚Ñ¶ [||h(F(≈∂ m ‚Ñ¶ )/Œ≤) ‚àí h(F(Y m ‚Ñ¶ )/Œ≤)|| 1 ] (8)\\nwhere h is a tanh function with a normalization constant Œ≤ to provide a comparable signal intensities across k-space, and\\nF(Y m ‚Ñ¶ )-F(≈∂ m ‚Ñ¶\\n) stand for k-space data of the ground truth and synthesized multi-coil images, respectively.\\n3) Multi-Coil Tensor Adversarial Loss: The level of realism in the synthesized images is advanced via a multi-coil adversarial loss function evaluated between image-domain data of the synthesized and ground truth multi-coil images:\\nL a = ‚àíE Y m ‚Ñ¶ [(D(Y m ‚Ñ¶ ) ‚àí 1) 2 ] ‚àí E X n Œõ [D(≈∂ m ‚Ñ¶ ) 2 ] (9\\n)\\nwhere D denotes the discriminator that distinguishes between undersampled ground truth and synthesized images. The final selective loss function for ssGAN is constructed as a weighted combination of the three multi-coil tensor loss terms described as L ssGAN = Œª k L k +Œª i L i +Œª a L a , where Œª k , Œª i , and Œª a denote the relative weighting of the tensor k-space, image, and adversarial losses. Note that the selective loss function in along with randomization of the k-space sampling masks across training subjects enables ssGAN to effectively capture complex relationships between acquired and nonacquired k-space coefficients. In turn, ssGAN can successfully recover high-quality target images without requiring Nyquistsampled acquisitions of the target contrast.',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Semi-Supervised Learning of Mutually Accelerated MRI Synthesis without Fully-Sampled Ground Truths',\n",
       "  'abstract': 'Learning-based synthetic multi-contrast MRI commonly involves deep models trained using high-quality images of source and target contrasts, regardless of whether source and target domain samples are paired or unpaired. This results in undesirable reliance on fullysampled acquisitions of all MRI contrasts, which might prove impractical due to limitations on scan costs and time. Here, we propose a novel semi-supervised deep generative model that instead learns to recover high-quality target images directly from accelerated acquisitions of source and target contrasts. To achieve this, the proposed model introduces novel multi-coil tensor losses in image, k-space and adversarial domains. These selective losses are based only on acquired k-space samples, and randomized sampling masks are used across subjects to capture relationships among acquired and non-acquired k-space regions. Comprehensive experiments on multi-contrast neuroimaging datasets demonstrate that our semi-supervised approach yields equivalent performance to gold-standard fully-supervised models, while outperforming a cascaded approach that learns to synthesize based on reconstructions of undersampled data. Therefore, the proposed approach holds great promise to improve the feasibility and utility of accelerated MRI acquisitions mutually undersampled across both contrast sets and k-space.',\n",
       "  'paddleOCR': [[[[30.0, 7.0], [1016.0, 3.0], [1016.0, 37.0], [31.0, 40.0]],\n",
       "    [' Fully-Supervised Generative Adversarial Networks', 0.987217903137207]],\n",
       "   [[[72.0, 59.0], [178.0, 63.0], [177.0, 93.0], [71.0, 89.0]],\n",
       "    ['Source', 0.9994438290596008]],\n",
       "   [[[726.0, 57.0], [942.0, 61.0], [941.0, 95.0], [725.0, 91.0]],\n",
       "    ['Ground Truth', 0.9994171261787415]],\n",
       "   [[[323.0, 84.0], [553.0, 84.0], [553.0, 120.0], [323.0, 120.0]],\n",
       "    ['Generator (G)', 0.9998543858528137]],\n",
       "   [[[150.0, 116.0], [179.0, 116.0], [179.0, 137.0], [150.0, 137.0]],\n",
       "    ['x1', 0.8796478509902954]],\n",
       "   [[[685.0, 111.0], [710.0, 111.0], [710.0, 137.0], [685.0, 137.0]],\n",
       "    ['y1', 0.7464038133621216]],\n",
       "   [[[867.0, 115.0], [894.0, 115.0], [894.0, 138.0], [867.0, 138.0]],\n",
       "    ['Y1', 0.9373290538787842]],\n",
       "   [[[5.0, 374.0], [1009.0, 376.0], [1009.0, 416.0], [5.0, 415.0]],\n",
       "    ['2) Semi-Supervised Generative Adversarial Networks',\n",
       "     0.9924196004867554]],\n",
       "   [[[106.0, 434.0], [213.0, 439.0], [212.0, 469.0], [104.0, 465.0]],\n",
       "    ['Source', 0.9996695518493652]],\n",
       "   [[[1327.0, 435.0], [1547.0, 435.0], [1547.0, 469.0], [1327.0, 469.0]],\n",
       "    ['Ground Truth.', 0.9756412506103516]],\n",
       "   [[[384.0, 457.0], [614.0, 457.0], [614.0, 492.0], [384.0, 492.0]],\n",
       "    ['Generator (G)', 0.998084306716919]],\n",
       "   [[[79.0, 499.0], [96.0, 499.0], [96.0, 517.0], [79.0, 517.0]],\n",
       "    ['A', 0.9954581260681152]],\n",
       "   [[[183.0, 492.0], [220.0, 492.0], [220.0, 522.0], [183.0, 522.0]],\n",
       "    ['XR', 0.6880919933319092]],\n",
       "   [[[1097.0, 497.0], [1123.0, 497.0], [1123.0, 519.0], [1097.0, 519.0]],\n",
       "    ['M', 0.9975389242172241]],\n",
       "   [[[1356.0, 497.0], [1378.0, 497.0], [1378.0, 521.0], [1356.0, 521.0]],\n",
       "    ['2', 0.708991289138794]],\n",
       "   [[[1461.0, 497.0], [1492.0, 501.0], [1489.0, 523.0], [1458.0, 519.0]],\n",
       "    ['Y', 0.9784857034683228]],\n",
       "   [[[7.0, 760.0], [489.0, 760.0], [489.0, 794.0], [7.0, 794.0]],\n",
       "    ['3) Selective Loss Function', 0.9972324967384338]],\n",
       "   [[[17.0, 836.0], [347.0, 838.0], [346.0, 871.0], [17.0, 870.0]],\n",
       "    ['Tensor k-space Loss', 0.9995786547660828]],\n",
       "   [[[817.0, 838.0], [1207.0, 841.0], [1207.0, 870.0], [817.0, 866.0]],\n",
       "    ['Tensor Adversarial Loss', 0.9781962633132935]],\n",
       "   [[[1089.0, 1033.0], [1373.0, 1033.0], [1373.0, 1067.0], [1089.0, 1067.0]],\n",
       "    ['Discriminator (D', 0.9827933311462402]],\n",
       "   [[[46.0, 1131.0], [718.0, 1129.0], [719.0, 1170.0], [46.0, 1171.0]],\n",
       "    ['Lk =Exz,Yn[||h(F(Ya)/)-h(F(Y)/)|l]', 0.8957088589668274]],\n",
       "   [[[1375.0, 1155.0], [1484.0, 1155.0], [1484.0, 1183.0], [1375.0, 1183.0]],\n",
       "    ['Fake/Real', 0.9922493696212769]],\n",
       "   [[[17.0, 1250.0], [327.0, 1256.0], [326.0, 1290.0], [17.0, 1284.0]],\n",
       "    ['Tensor Image Loss', 0.9991716742515564]],\n",
       "   [[[947.0, 1448.0], [1359.0, 1446.0], [1359.0, 1486.0], [947.0, 1488.0]],\n",
       "    ['La =-Eyg[(D(Y)-1)2]', 0.8332540392875671]],\n",
       "   [[[1062.0, 1500.0], [1297.0, 1500.0], [1297.0, 1547.0], [1062.0, 1547.0]],\n",
       "    ['-Exz[D(Y)]', 0.9120998382568359]]],\n",
       "  'ocr': [[[726.0, 57.0], [942.0, 61.0], [941.0, 95.0], [725.0, 91.0]],\n",
       "   ['Ground Truth', 0.9994171261787415]]},\n",
       " '210157154-Figure2-1.png': {'caption': 'Figure 2: An overview of the proposed model as a Bayesian network. The boxes are ‚Äúplates‚Äù representing structures in the data. The plates marked by k, n and T represent products, regions, and time, respectively. Circles denote random variables and squares are deterministic quantities. The model decomposes quantity as a function of region, product, time, and auto-regressive weights.',\n",
       "  'imageText': ['regions:',\n",
       "   'k',\n",
       "   'products:',\n",
       "   'n',\n",
       "   'time:',\n",
       "   'T',\n",
       "   '+,Autoregressive',\n",
       "   'weight',\n",
       "   'Price',\n",
       "   'RevenueQuantity',\n",
       "   'Product',\n",
       "   'weight',\n",
       "   'Region',\n",
       "   'weight',\n",
       "   'Temporal',\n",
       "   'weight',\n",
       "   '-6',\n",
       "   '45',\n",
       "   '3.',\n",
       "   '2',\n",
       "   '-(/',\n",
       "   '*(',\n",
       "   \"-'.\",\n",
       "   'k',\n",
       "   'T',\n",
       "   'n',\n",
       "   \"))'(&'(\"],\n",
       "  'image_file': '210157154-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Stochastic Model of Spatial Demand',\n",
       "    'text': \"We propose the following stochastic model of spatial demand in physical retail. See Figure 2 for an overview. In the current work, the stochastic model is used as a 'simulator' to enable offline policy learning. There are many advantages of using a probabilistic model in the optimal product allocation problem. First, we are able to incorporate prior knowledge about the data generating process, which can again improve data efficiency and model effectiveness. Second, it provides a natural framework for simulating future scenarios through Monte Carlo roll-outs.\\nOur ultimate objective is to maximize total revenue at time, œÅ (t) , which is defined as:\\nœÅ (t) = n i=1 œÅ (t) i (5) where œÅ (t)\\ni is the revenue for region, r i . Region-level revenue is calculated over products, m j :\\nœÅ (t) i = k j=1 p j q (t) ij (6)\\nThe key variable of interest is, q (t) ij , the quantity sold for product, m j , region, r i , at time, t. We model q (t) ij as a truncated normal random variable:\\nq (t) ij ‚àº œà(¬µ, œÉ, a, b) (7)\\nwhere, œà(¬µ, œÉ, a, b) is the pdf of the truncated normal distribution. The term, œÜ(z) is the standard normal pdf, and Œ¶(z) is its cumulative distribution function. See (Burkardt 2014) for more details. We set a = 0 and b = +‚àû, which forces Œ¶(¬µ, œÉ 2 ; b) = 1 and constrains quantity,\\nq (t) ij ‚àà R + . The prior for q (t)\\nij is characterized by the mean, ¬µ q , which is a linear function of environment features, x and learned weights, w, and the inverse gamma distribution for the variance, œÉ q :\\n¬µ q = x w + b (8) œÉ q ‚àº IG(Œ± q , Œ≤ q )(9)\\nIn our environment, we observe temporal features, x t , region features, x r , product features, x p , and autoregressive features, x s : x = [x t , x r , x p , x s ] . We discuss our feature extraction approach more in section Region-level Weights We initially model the weights for each spatial region with a multivariate normal distribution, with mean vector, ¬µ r and covariance matrix, Q r :\\nw r ‚àº N (¬µ r , Q r )(10)\\nProduct-level Weights We also define weights for each product, m j , as follows:\\nw p ‚àº N (¬µ p , Œ£ p )(11)\\n¬µ p ‚àº N (Œ¥ p , Œì p )(12)\\nŒ£ p = LL ‚àº LKJ(œÉ p )(13)\\nWe put a multivariate normal prior over the mean vector, ¬µ p which has hyperparameters ¬µ t and Œ£ t . Additionally, we put an LKJ prior over the covariance matrix, Œ£ p . We reparameterize Œ£ t as its cholesky decomposition, LL , so that the underlying correlation matrices follows an LKJ distribution (Lewandowski, Kurowicka, and Joe 2009). The standard deviations, œÉ p , follow a half-cauchy distribution. The advantage of the LKJ prior is that is more computationally tractable than other covariance priors (Lewandowski, Kurowicka, and Joe 2009). Temporal weights The temporal features capture the long-term and short-term seasonality of the environment. The temporal weights are defined similar to the product weights. Namely, the temporal weights, w t , follow a multivariate normal distribution, with a normal prior over the mean, and the LKJ prior for the covariance matrix:\\nw t ‚àº N (¬µ t , Œ£ t ) (14\\n)\\n¬µ t ‚àº N (Œ¥ t , Œì t )(15)\\nŒ£ t = LL ‚àº LKJ(œÉ t )(16)\\nAutoregressive weight Finally, we specify the weight of previously observed revenue values on q (t) ij . The feature, x s is an autoregressive feature denoting the previous k values of product-level revenue, œÅ t j = n j=i p j q (t) ij . We assume truncated normal prior for w s , and half cauchy priors for the location, ¬µ s and scale, œÉ s :\\nw s ‚àº œà(¬µ s , œÉ s , a, b) (17) ¬µ s ‚àº HalfCauchy(œÜ s )(18)\\nœÉ s ‚àº HalfCauchy(œà s )(19)\\nWe again set a = 0 and b = +‚àû such that w s ‚àà R + .\\nw r ij ‚àº N (w r , Q r )(20)\\nw r ‚àº N (¬µ i , Q r )(21)\\nNote that both w r and w r ij share the same same covariance structure. Thus, the region weights are only hierarchical in their means. Additionally, we treat the upper-level mean vector, ¬µ r as hyperparameter. In Section we test which environment model is more effective at predicting revenue on a test set.\",\n",
       "    'n_publication_ref': 3,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'A Probabilistic Simulator of Spatial Demand for Product Allocation',\n",
       "  'abstract': 'Connecting consumers with relevant products is a very important problem in both online and offline commerce. In physical retail, product placement is an effective way to connect consumers with products. However, selecting product locations within a store can be a tedious process. Moreover, learning important spatial patterns in offline retail is challenging due to the scarcity of data and the high cost of exploration and experimentation in the physical world. To address these challenges, we propose a stochastic model of spatial demand in physical retail. We show that the proposed model is more predictive of demand than existing baselines. We also perform a preliminary study into different automation techniques and show that an optimal product allocation policy can be learned through Deep Q-Learning.',\n",
       "  'paddleOCR': [[[[436.0, 14.0], [570.0, 14.0], [570.0, 34.0], [436.0, 34.0]],\n",
       "    ['Product weight', 0.9725620150566101]],\n",
       "   [[[625.0, 15.0], [670.0, 17.0], [669.0, 37.0], [624.0, 34.0]],\n",
       "    ['Price', 0.9995210766792297]],\n",
       "   [[[33.0, 79.0], [230.0, 78.0], [230.0, 98.0], [33.0, 99.0]],\n",
       "    ['Autoregressive weight', 0.999944806098938]],\n",
       "   [[[472.0, 77.0], [491.0, 77.0], [491.0, 94.0], [472.0, 94.0]],\n",
       "    ['p', 0.6619783043861389]],\n",
       "   [[[614.0, 78.0], [643.0, 84.0], [638.0, 110.0], [608.0, 104.0]],\n",
       "    ['P j', 0.9129095673561096]],\n",
       "   [[[455.0, 88.0], [482.0, 88.0], [482.0, 106.0], [455.0, 106.0]],\n",
       "    ['W', 0.8588224053382874]],\n",
       "   [[[117.0, 147.0], [153.0, 156.0], [147.0, 179.0], [111.0, 170.0]],\n",
       "    ['Ws', 0.9794143438339233]],\n",
       "   [[[48.0, 226.0], [194.0, 226.0], [194.0, 246.0], [48.0, 246.0]],\n",
       "    ['Temporal weight', 0.9999148845672607]],\n",
       "   [[[475.0, 220.0], [551.0, 222.0], [550.0, 244.0], [475.0, 241.0]],\n",
       "    ['Quantity', 0.9419327974319458]],\n",
       "   [[[755.0, 219.0], [832.0, 222.0], [832.0, 241.0], [754.0, 237.0]],\n",
       "    ['Revenue', 0.9990218281745911]],\n",
       "   [[[119.0, 281.0], [157.0, 290.0], [151.0, 314.0], [113.0, 306.0]],\n",
       "    ['Wt', 0.9984219670295715]],\n",
       "   [[[458.0, 290.0], [493.0, 296.0], [489.0, 320.0], [454.0, 314.0]],\n",
       "    ['qij', 0.9956157207489014]],\n",
       "   [[[619.0, 286.0], [651.0, 292.0], [647.0, 313.0], [615.0, 307.0]],\n",
       "    ['Pij', 0.9917842745780945]],\n",
       "   [[[248.0, 359.0], [374.0, 362.0], [374.0, 383.0], [248.0, 381.0]],\n",
       "    ['Region weight', 0.9880605936050415]],\n",
       "   [[[140.0, 404.0], [162.0, 404.0], [162.0, 420.0], [140.0, 420.0]],\n",
       "    ['ur', 0.6853213906288147]],\n",
       "   [[[291.0, 426.0], [325.0, 426.0], [325.0, 456.0], [291.0, 456.0]],\n",
       "    ['w[', 0.5524094104766846]],\n",
       "   [[[636.0, 500.0], [653.0, 500.0], [653.0, 523.0], [636.0, 523.0]],\n",
       "    ['K', 0.5515100955963135]],\n",
       "   [[[686.0, 529.0], [705.0, 529.0], [705.0, 548.0], [686.0, 548.0]],\n",
       "    ['n', 0.9976435303688049]],\n",
       "   [[[0.0, 546.0], [88.0, 542.0], [89.0, 562.0], [0.0, 566.0]],\n",
       "    ['regions: k', 0.9819639921188354]],\n",
       "   [[[0.0, 569.0], [100.0, 569.0], [100.0, 589.0], [0.0, 589.0]],\n",
       "    ['products: n', 0.9994361400604248]],\n",
       "   [[[1.0, 596.0], [65.0, 596.0], [65.0, 613.0], [1.0, 613.0]],\n",
       "    ['time: T', 0.9951580762863159]]],\n",
       "  'ocr': [[[619.0, 286.0], [651.0, 292.0], [647.0, 313.0], [615.0, 307.0]],\n",
       "   ['Pij', 0.9917842745780945]]},\n",
       " '2202.06593v2-Figure3-1.png': {'caption': 'Figure 3: Schematic illustration of the construction of Z.',\n",
       "  'imageText': ['ùíµ\"',\n",
       "   'Section',\n",
       "   '4.2ùíµ',\n",
       "   '=',\n",
       "   'ùíµ!',\n",
       "   '‚à©',\n",
       "   'ùíµ\"',\n",
       "   'of',\n",
       "   'ùíµ!',\n",
       "   'ùíµ!',\n",
       "   'Construction',\n",
       "   'Section',\n",
       "   '4.1.1',\n",
       "   'Section',\n",
       "   '4.1.2',\n",
       "   'Section',\n",
       "   '4.1.3',\n",
       "   'Proposed',\n",
       "   'Computational',\n",
       "   'Method',\n",
       "   'Parametrization',\n",
       "   'Parametric',\n",
       "   'DTW',\n",
       "   'Parametric',\n",
       "   'Optimal',\n",
       "   'Alignment',\n",
       "   'Parametrized',\n",
       "   'Data',\n",
       "   'Standard',\n",
       "   'DTW',\n",
       "   'Observed',\n",
       "   'Data',\n",
       "   'Optimal',\n",
       "   'Alignment'],\n",
       "  'image_file': '2202.06593v2-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Computational Method for Computing Z',\n",
       "    'text': 'In this section, we present our second contribution of introducing novel computational method, called parametric DTW, to compute Z. The basic idea is illustrated in Fig. 3.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Statistical Inference for the Dynamic Time Warping Distance, with Application to Abnormal Time-Series Detection',\n",
       "  'abstract': 'We study statistical inference on the similarity/distance between two time-series under uncertain environment by considering a statistical hypothesis test on the distance obtained from Dynamic Time Warping (DTW) algorithm. The sampling distribution of the DTW distance is too difficult to derive because it is obtained based on the solution of the DTW algorithm, which is complicated. To circumvent this difficulty, we propose to employ the conditional selective inference framework, which enables us to derive a valid inference method on the DTW distance. To our knowledge, this is the first method that can provide a valid p-value to quantify the statistical significance of the DTW distance, which is helpful for high-stake decision making such as abnormal time-series detection problems. We evaluate the performance of the proposed inference method on both synthetic and real-world datasets.',\n",
       "  'paddleOCR': [[[[66.0, 24.0], [197.0, 24.0], [197.0, 54.0], [66.0, 54.0]],\n",
       "    ['Observed', 0.9997820258140564]],\n",
       "   [[[656.0, 24.0], [782.0, 24.0], [782.0, 54.0], [656.0, 54.0]],\n",
       "    ['Standard', 0.9998553991317749]],\n",
       "   [[[293.0, 46.0], [546.0, 46.0], [546.0, 75.0], [293.0, 75.0]],\n",
       "    ['Optimal Alignment', 0.9888952970504761]],\n",
       "   [[[92.0, 62.0], [162.0, 66.0], [161.0, 97.0], [90.0, 93.0]],\n",
       "    ['Data', 0.9996694922447205]],\n",
       "   [[[683.0, 63.0], [755.0, 63.0], [755.0, 94.0], [683.0, 94.0]],\n",
       "    ['DTW', 0.9934041500091553]],\n",
       "   [[[25.0, 167.0], [226.0, 167.0], [226.0, 191.0], [25.0, 191.0]],\n",
       "    ['Parametrization', 0.9999131560325623]],\n",
       "   [[[415.0, 169.0], [877.0, 169.0], [877.0, 198.0], [415.0, 198.0]],\n",
       "    ['Proposed Computational Method', 0.9971448183059692]],\n",
       "   [[[36.0, 248.0], [216.0, 248.0], [216.0, 276.0], [36.0, 276.0]],\n",
       "    ['Parametrized', 0.9997487664222717]],\n",
       "   [[[343.0, 246.0], [494.0, 246.0], [494.0, 276.0], [343.0, 276.0]],\n",
       "    ['Parametric', 0.999883770942688]],\n",
       "   [[[641.0, 246.0], [796.0, 246.0], [796.0, 276.0], [641.0, 276.0]],\n",
       "    ['Parametric', 0.9999162554740906]],\n",
       "   [[[1135.0, 248.0], [1311.0, 248.0], [1311.0, 276.0], [1135.0, 276.0]],\n",
       "    ['Construction', 0.9997043013572693]],\n",
       "   [[[92.0, 284.0], [162.0, 288.0], [161.0, 319.0], [90.0, 316.0]],\n",
       "    ['Data', 0.9997355937957764]],\n",
       "   [[[292.0, 289.0], [547.0, 289.0], [547.0, 317.0], [292.0, 317.0]],\n",
       "    ['Optimal Alignment', 0.9991872310638428]],\n",
       "   [[[683.0, 286.0], [755.0, 286.0], [755.0, 317.0], [683.0, 317.0]],\n",
       "    ['DTW', 0.9544463753700256]],\n",
       "   [[[1182.0, 283.0], [1260.0, 283.0], [1260.0, 322.0], [1182.0, 322.0]],\n",
       "    ['0f Z2', 0.9110420942306519]],\n",
       "   [[[329.0, 352.0], [505.0, 352.0], [505.0, 376.0], [329.0, 376.0]],\n",
       "    ['Section 4.1.2', 0.9676368236541748]],\n",
       "   [[[630.0, 349.0], [805.0, 349.0], [805.0, 373.0], [630.0, 373.0]],\n",
       "    ['Section 4.1.3', 0.9762502908706665]],\n",
       "   [[[881.0, 343.0], [1082.0, 346.0], [1081.0, 382.0], [881.0, 379.0]],\n",
       "    ['Z = Z1 n Z2', 0.832814633846283]],\n",
       "   [[[38.0, 356.0], [214.0, 356.0], [214.0, 380.0], [38.0, 380.0]],\n",
       "    ['Section 4.1.1', 0.9975082874298096]],\n",
       "   [[[1152.0, 350.0], [1305.0, 350.0], [1305.0, 379.0], [1152.0, 379.0]],\n",
       "    ['Section 4.2', 0.9998405575752258]]],\n",
       "  'ocr': [[[292.0, 289.0], [547.0, 289.0], [547.0, 317.0], [292.0, 317.0]],\n",
       "   ['Optimal Alignment', 0.9991872310638428]]},\n",
       " '2110.06467v3-Figure1-1.png': {'caption': 'Fig. 1. The diagram of the proposed DB-AIAT. (a) The overall diagram of the proposed system. (b) The detailed architecture of mask decoder.',\n",
       "  'imageText': ['2D-Conv',\n",
       "   'Sigmoid',\n",
       "   '2D-Conv',\n",
       "   'Sigmoid',\n",
       "   'Tanh',\n",
       "   '2D-Conv',\n",
       "   '2D-Conv',\n",
       "   'LN',\n",
       "   'DenseNet',\n",
       "   'Sub-pixel',\n",
       "   '2D-Conv',\n",
       "   'Dilated',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'Magnitude',\n",
       "   'Masking',\n",
       "   'Branch',\n",
       "   'cos(.)sin(.)',\n",
       "   'Phase',\n",
       "   'er',\n",
       "   'o',\n",
       "   'rm',\n",
       "   'n',\n",
       "   'sf',\n",
       "   'T',\n",
       "   'ra',\n",
       "   '-',\n",
       "   'F',\n",
       "   'A',\n",
       "   'A',\n",
       "   'T',\n",
       "   'er',\n",
       "   'o',\n",
       "   'rm',\n",
       "   'n',\n",
       "   'sf',\n",
       "   'T',\n",
       "   'ra',\n",
       "   '-',\n",
       "   'F',\n",
       "   'A',\n",
       "   'A',\n",
       "   'T',\n",
       "   'Dense-encoder',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'et',\n",
       "   'se',\n",
       "   'N',\n",
       "   'D',\n",
       "   'en',\n",
       "   'at',\n",
       "   'ed',\n",
       "   'D',\n",
       "   'il',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'L',\n",
       "   'N',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'L',\n",
       "   'N',\n",
       "   'Dense-encoder',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'et',\n",
       "   'se',\n",
       "   'N',\n",
       "   'D',\n",
       "   'en',\n",
       "   'at',\n",
       "   'ed',\n",
       "   'D',\n",
       "   'il',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'L',\n",
       "   'N',\n",
       "   'el',\n",
       "   'u',\n",
       "   'P',\n",
       "   'R',\n",
       "   'n',\n",
       "   'v',\n",
       "   '-C',\n",
       "   'o',\n",
       "   '2',\n",
       "   'D',\n",
       "   'L',\n",
       "   'N',\n",
       "   'Merge',\n",
       "   'Module',\n",
       "   'Complex',\n",
       "   'Refining',\n",
       "   'Branch',\n",
       "   'Decouple',\n",
       "   '(',\n",
       "   ',',\n",
       "   'Ôºâ',\n",
       "   '(',\n",
       "   ',',\n",
       "   'Ôºâ',\n",
       "   'o',\n",
       "   'd',\n",
       "   'u',\n",
       "   'le',\n",
       "   'A',\n",
       "   'M',\n",
       "   'A',\n",
       "   'H',\n",
       "   'Decoder',\n",
       "   'Real',\n",
       "   'er',\n",
       "   'o',\n",
       "   'rm',\n",
       "   'n',\n",
       "   'sf',\n",
       "   'T',\n",
       "   'ra',\n",
       "   '-',\n",
       "   'F',\n",
       "   'A',\n",
       "   'A',\n",
       "   'T',\n",
       "   'Decoder',\n",
       "   'Mask',\n",
       "   'o',\n",
       "   'd',\n",
       "   'u',\n",
       "   'le',\n",
       "   'A',\n",
       "   'M',\n",
       "   'A',\n",
       "   'H',\n",
       "   'er',\n",
       "   'o',\n",
       "   'rm',\n",
       "   'n',\n",
       "   'sf',\n",
       "   'T',\n",
       "   'ra',\n",
       "   '-',\n",
       "   'F',\n",
       "   'A',\n",
       "   'A',\n",
       "   'T',\n",
       "   '(',\n",
       "   ',',\n",
       "   'Ôºâ',\n",
       "   'Decoder',\n",
       "   'Imag',\n",
       "   '(a)',\n",
       "   'Overall',\n",
       "   'diagram',\n",
       "   '(b)',\n",
       "   'Mask',\n",
       "   'Decoder'],\n",
       "  'image_file': '2110.06467v3-Figure1-1.png',\n",
       "  'sections': [{'heading': 'INTRODUCTION',\n",
       "    'text': 'In real acoustic scenarios, various types of environmental interference may severely degrade the performance of telecommunication and hearing aids. Monaural speech enhancement (SE) technique aims at recovering clean speech from its noise-corrupted mixture to improve speech quality and/or intelligibility when only one channel recording is available [1]. Recently, deep neural networks (DNNs) have ignited the development of SE algorithms for their more powerful capability in dealing with non-stationary noise than conventional statistical signal-processing based approaches [2].\\nIn a typical supervised SE paradigm, DNNs are usually leveraged to estimate the mask functions or directly predict the magnitude spectra of clean speech in the time-frequency (T-F) domain [3,4], where the noisy phase is unchanged and involved in waveform reconstruction. Recently, the importance of phase has been fully illustrated in improving the speech perceptual quality, especially under low signal-to-noise ratio (SNR) conditions [5]. In this regard, more recent approaches attempt to recover the phase information either This work was supported in part by the National Natural Science Foundation of China under Grant 61631016 and Grant 61501410.\\nexplicitly or implicitly [6,7,8,9,10,11]. For the first class, the network is employed to estimate the complex ratio masks (CRMs) or the real and imaginary (RI) spectra, which facilitate both magnitude and phase information recovery simultaneously in the T-F domain. For the latter, the time-domain waveform is directly regenerated, which diverts around the phase estimation process. More recently, decoupling-style phase-aware methods are proposed, where the original complex-spectrum estimation problem is decomposed into two sub-stages [7,12,13]. Specifically, only the magnitude estimation is involved in the first stage, followed by the spectrum refinement with residual learning in the later stage. In this way, the optimization w.r.t. magnitude and phase can be effectively decoupled, which alleviates the implicit compensation effect between two targets [14].\\nIn this paper, we propose a dual-branch SE structure dubbed DB-AIAT, to explore the complex spectrum recovery from the complementary perspective. Specifically, two core branches are elaborately designed in parallel, namely a magnitude masking branch (MMB) and a complex refining branch (CRB). In MMB, we seek to construct the filtering system which only applies to the magnitude domain. In this branch, most of the noise can be effectively suppressed. In CRB, it is established as the decorating system to compensate for the lost spectral details and phase mismatch effect. Two branches work collaboratively to facilitate the overall spectrum recovery. Besides, despite temporal convolutional networks (TCNs) [15] and LSTM layers are widely adopted for long-range sequence modeling in the SE area, they still lack sufficient capacity to capture the global context information [10,16]. In addition, they usually only apply in the time axis, which neglects the correlations among different frequency subbands. To this end, we propose an attention-in-attention transformer (AIAT) to funnel the global sequence modeling process, which captures long-range dependencies along both time and frequency axes, and meanwhile, aggregates global hierarchical contextual information. Experimental results on Voice Bank + DEMAND dataset show that our proposed method achieves remarkable results and consistently outperforms most state-of-the-art baselines with a relatively light model size.\\nThe remainder of the paper is organized as follows. In Section 2, the proposed framework is described in detail. The experimental setup is presented in Section 3, while Section 4 gives the results and analysis. Finally, some conclusions are drawn in Section 5.   masking branch (MMB) and a complex spectrum refining branch (CRB), which aim at collaboratively estimating the magnitude and phase information of clean speech in parallel. To be specific, in the MMB path, the input is the magnitude of the noisy spectrum, and the network estimates the magnitude mask M mmb to coarsely recover the magnitude of the target speech, i.e., | S mmb |. Subsequently, the coarsely estimated spectral magnitude is coupled with the noisy phase Œ∏X to derive the coarse-denoised complex spectrum. Note that we omit the time and frequency indices for brevity.\\nAs the complement, CRB receives noisy RI component {Xr, Xi} as the input and focuses on the fine-grained spectral structures which may be lost in the MMB path and further suppressing the residual noise components. Note that we only estimate the residual details instead of explicitly estimating the whole complex spectrum, which alleviates the overall burden of the network. The alternate interconnections are adopted to exchange information between the two branches, enabling better feature representation. Finally, we sum the coarse-denoised complex spectrum and the fine-grained complex spectral details together to reconstruct the clean complex spectrum. In a nutshell, the whole procedure can be formulated as:\\n| S mmb | = |X t,f | ‚äó M mmb ,(1)\\nS mmb r = | S mmb | ‚äó cos (Œ∏X ) ,(2)\\nS mmb i = | S mmb | ‚äó sin (Œ∏X ) ,(3)\\nSr = S mmb r + S crb r ,(4)\\nSi = S mmb i + S crb i (5)\\nwhere S crb r , S crb i denote the output RI components of CRB and Sr, Si denote the final merged clean RI components. Tilde denotes the estimated variable. ‚äó is the element-wise multiplication operator. The input features of MMB and CRB are denoted as |X| ‚àà R T √óF √ó1 and Cat(Xr, Xi) ‚àà R T √óF √ó2 , respectively. Here T is the number of frames and F is the number of frequency bins. As shown in Fig. 1, MMB consists of a densely-connected convolutional encoder, an AIAT and a mask decoder. Analogously, CRB is composed of a dense-encoder, an AIAT and two complex decoders.',\n",
       "    'n_publication_ref': 18,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'DUAL-BRANCH ATTENTION-IN-ATTENTION TRANSFORMER FOR SPEECH ENHANCEMENT',\n",
       "  'abstract': 'Curriculum learning begins to thrive in the speech enhancement area, which decouples the original spectrum estimation task into multiple easier sub-tasks to achieve better performance. Motivated by that, we propose a dual-branch attention-in-attention transformer dubbed DB-AIAT to handle both coarse-and fine-grained regions of the spectrum in parallel. From a complementary perspective, a magnitude masking branch is proposed to coarsely estimate the overall magnitude spectrum, and simultaneously a complex refining branch is elaborately designed to compensate for the missing spectral details and implicitly derive phase information. Within each branch, we propose a novel attention-in-attention transformer-based module to replace the conventional RNNs and temporal convolutional networks for temporal sequence modeling. Specifically, the proposed attention-in-attention transformer consists of adaptive temporalfrequency attention transformer blocks and an adaptive hierarchical attention module, aiming to capture long-term temporal-frequency dependencies and further aggregate global hierarchical contextual information. Experimental results on Voice Bank + DEMAND demonstrate that DB-AIAT yields state-of-the-art performance (e.g., 3.31 PESQ, 95.6% STOI and 10.79dB SSNR) over previous advanced systems with a relatively small model size (2.81M).',\n",
       "  'paddleOCR': [[[[405.0, 2.0], [792.0, 2.0], [792.0, 30.0], [405.0, 30.0]],\n",
       "    ['Magnitude Masking Branch', 0.997005045413971]],\n",
       "   [[[1232.0, 0.0], [1438.0, 0.0], [1438.0, 24.0], [1232.0, 24.0]],\n",
       "    ['Merge Module', 0.9644677639007568]],\n",
       "   [[[1269.0, 36.0], [1396.0, 51.0], [1391.0, 92.0], [1265.0, 78.0]],\n",
       "    ['Phase Oxt,f', 0.9012689590454102]],\n",
       "   [[[1649.0, 57.0], [1723.0, 57.0], [1723.0, 79.0], [1649.0, 79.0]],\n",
       "    ['Dilated', 0.9988264441490173]],\n",
       "   [[[235.0, 83.0], [375.0, 85.0], [375.0, 107.0], [235.0, 104.0]],\n",
       "    ['Dense-encoder', 0.9996010065078735]],\n",
       "   [[[880.0, 91.0], [901.0, 91.0], [901.0, 231.0], [880.0, 231.0]],\n",
       "    [' AHA Module', 0.960083544254303]],\n",
       "   [[[1637.0, 83.0], [1735.0, 83.0], [1735.0, 105.0], [1637.0, 105.0]],\n",
       "    ['DenseNet', 0.9995464086532593]],\n",
       "   [[[1639.0, 114.0], [1731.0, 114.0], [1731.0, 136.0], [1639.0, 136.0]],\n",
       "    ['Sub-pixel', 0.9993665218353271]],\n",
       "   [[[578.0, 126.0], [604.0, 126.0], [604.0, 250.0], [578.0, 250.0]],\n",
       "    [' Transformer', 0.9748560786247253]],\n",
       "   [[[742.0, 126.0], [763.0, 127.0], [759.0, 249.0], [738.0, 248.0]],\n",
       "    ['Transformer', 0.9988036751747131]],\n",
       "   [[[300.0, 136.0], [321.0, 136.0], [321.0, 229.0], [300.0, 229.0]],\n",
       "    ['DenseNet', 0.9994724988937378]],\n",
       "   [[[708.0, 142.0], [736.0, 142.0], [736.0, 227.0], [708.0, 227.0]],\n",
       "    ['ATFA-', 0.9935487508773804]],\n",
       "   [[[1636.0, 134.0], [1735.0, 138.0], [1734.0, 166.0], [1635.0, 161.0]],\n",
       "    ['2D-Conv', 0.9935539960861206]],\n",
       "   [[[193.0, 152.0], [214.0, 152.0], [214.0, 229.0], [193.0, 229.0]],\n",
       "    ['2D-Conv', 0.9544821381568909]],\n",
       "   [[[245.0, 149.0], [272.0, 151.0], [267.0, 220.0], [240.0, 217.0]],\n",
       "    ['PRelu', 0.9634067416191101]],\n",
       "   [[[274.0, 149.0], [301.0, 151.0], [296.0, 223.0], [269.0, 222.0]],\n",
       "    ['Dilated', 0.9992061257362366]],\n",
       "   [[[331.0, 144.0], [350.0, 144.0], [350.0, 229.0], [331.0, 229.0]],\n",
       "    ['2D-Conv', 0.9987601041793823]],\n",
       "   [[[457.0, 144.0], [478.0, 144.0], [478.0, 225.0], [457.0, 225.0]],\n",
       "    ['2D-Conv', 0.9994542002677917]],\n",
       "   [[[484.0, 150.0], [505.0, 150.0], [505.0, 225.0], [484.0, 225.0]],\n",
       "    ['PReIu', 0.9258869886398315]],\n",
       "   [[[547.0, 150.0], [574.0, 150.0], [574.0, 227.0], [547.0, 227.0]],\n",
       "    [' ATFA-', 0.9533031582832336]],\n",
       "   [[[1253.0, 144.0], [1316.0, 144.0], [1316.0, 172.0], [1253.0, 172.0]],\n",
       "    ['sin(.)', 0.9984307289123535]],\n",
       "   [[[1346.0, 146.0], [1413.0, 146.0], [1413.0, 174.0], [1346.0, 174.0]],\n",
       "    ['cos(.)', 0.9932754635810852]],\n",
       "   [[[52.0, 165.0], [119.0, 170.0], [116.0, 210.0], [49.0, 204.0]],\n",
       "    ['|Xt,f|', 0.9335365295410156]],\n",
       "   [[[220.0, 162.0], [241.0, 162.0], [241.0, 201.0], [220.0, 201.0]],\n",
       "    ['LN', 0.9956167936325073]],\n",
       "   [[[360.0, 162.0], [375.0, 162.0], [375.0, 203.0], [360.0, 203.0]],\n",
       "    ['LN', 0.9867944717407227]],\n",
       "   [[[382.0, 158.0], [404.0, 158.0], [404.0, 215.0], [382.0, 215.0]],\n",
       "    ['PRelu', 0.9836107492446899]],\n",
       "   [[[1000.0, 168.0], [1056.0, 168.0], [1056.0, 189.0], [1000.0, 189.0]],\n",
       "    ['Mask', 0.9991042017936707]],\n",
       "   [[[1670.0, 170.0], [1702.0, 170.0], [1702.0, 193.0], [1670.0, 193.0]],\n",
       "    ['LN', 0.9984289407730103]],\n",
       "   [[[989.0, 195.0], [1065.0, 195.0], [1065.0, 217.0], [989.0, 217.0]],\n",
       "    ['Decoder', 0.9976956248283386]],\n",
       "   [[[1634.0, 197.0], [1737.0, 202.0], [1736.0, 229.0], [1633.0, 224.0]],\n",
       "    ['2D-Conv', 0.9963349103927612]],\n",
       "   [[[1606.0, 252.0], [1616.0, 252.0], [1616.0, 266.0], [1606.0, 266.0]],\n",
       "    ['+', 0.7393463850021362]],\n",
       "   [[[40.0, 280.0], [134.0, 280.0], [134.0, 308.0], [40.0, 308.0]],\n",
       "    ['Decouple', 0.9997506737709045]],\n",
       "   [[[1561.0, 274.0], [1660.0, 274.0], [1660.0, 302.0], [1561.0, 302.0]],\n",
       "    ['2D-Conv', 0.9987176656723022]],\n",
       "   [[[1717.0, 274.0], [1815.0, 274.0], [1815.0, 302.0], [1717.0, 302.0]],\n",
       "    ['2D-Conv', 0.9989362955093384]],\n",
       "   [[[230.0, 314.0], [365.0, 314.0], [365.0, 335.0], [230.0, 335.0]],\n",
       "    ['Dense-encoder', 0.9987815618515015]],\n",
       "   [[[1585.0, 310.0], [1635.0, 310.0], [1635.0, 333.0], [1585.0, 333.0]],\n",
       "    ['Tanh', 0.9992495775222778]],\n",
       "   [[[1721.0, 308.0], [1811.0, 308.0], [1811.0, 335.0], [1721.0, 335.0]],\n",
       "    [' Sigmoid', 0.9495617747306824]],\n",
       "   [[[1256.0, 328.0], [1337.0, 319.0], [1340.0, 346.0], [1259.0, 356.0]],\n",
       "    ['Smmb', 0.9830923080444336]],\n",
       "   [[[1342.0, 324.0], [1428.0, 319.0], [1429.0, 347.0], [1344.0, 351.0]],\n",
       "    ['Smmb', 0.9833022356033325]],\n",
       "   [[[1008.0, 335.0], [1056.0, 335.0], [1056.0, 359.0], [1008.0, 359.0]],\n",
       "    ['Real', 0.9989185929298401]],\n",
       "   [[[579.0, 349.0], [606.0, 349.0], [606.0, 475.0], [579.0, 475.0]],\n",
       "    [' Transformer', 0.9748959541320801]],\n",
       "   [[[739.0, 347.0], [765.0, 348.0], [761.0, 476.0], [734.0, 475.0]],\n",
       "    ['Transformer', 0.9997032284736633]],\n",
       "   [[[878.0, 349.0], [905.0, 349.0], [905.0, 499.0], [878.0, 499.0]],\n",
       "    ['AHA Module', 0.9989243745803833]],\n",
       "   [[[193.0, 369.0], [214.0, 369.0], [214.0, 456.0], [193.0, 456.0]],\n",
       "    ['2D-Conv', 0.9989638924598694]],\n",
       "   [[[298.0, 363.0], [325.0, 363.0], [325.0, 457.0], [298.0, 457.0]],\n",
       "    ['DenseNet', 0.9997551441192627]],\n",
       "   [[[331.0, 365.0], [352.0, 365.0], [352.0, 456.0], [331.0, 456.0]],\n",
       "    ['2D-Conv', 0.9986613988876343]],\n",
       "   [[[452.0, 370.0], [473.0, 372.0], [468.0, 454.0], [447.0, 453.0]],\n",
       "    ['2D-Conv', 0.9969728589057922]],\n",
       "   [[[478.0, 373.0], [499.0, 373.0], [499.0, 446.0], [478.0, 446.0]],\n",
       "    ['PReIu', 0.8969310522079468]],\n",
       "   [[[549.0, 371.0], [576.0, 371.0], [576.0, 456.0], [549.0, 456.0]],\n",
       "    [' ATFA-', 0.9252157211303711]],\n",
       "   [[[708.0, 371.0], [734.0, 371.0], [734.0, 456.0], [708.0, 456.0]],\n",
       "    [' ATFA-', 0.925558865070343]],\n",
       "   [[[993.0, 365.0], [1073.0, 365.0], [1073.0, 386.0], [993.0, 386.0]],\n",
       "    ['Decoder', 0.9989275932312012]],\n",
       "   [[[245.0, 376.0], [272.0, 378.0], [267.0, 448.0], [240.0, 446.0]],\n",
       "    ['PRelu', 0.9593448638916016]],\n",
       "   [[[275.0, 375.0], [296.0, 375.0], [296.0, 450.0], [275.0, 450.0]],\n",
       "    ['Dilated', 0.9937801361083984]],\n",
       "   [[[220.0, 388.0], [241.0, 388.0], [241.0, 430.0], [220.0, 430.0]],\n",
       "    ['LN', 0.9907022714614868]],\n",
       "   [[[360.0, 390.0], [375.0, 390.0], [375.0, 428.0], [360.0, 428.0]],\n",
       "    ['LN', 0.9946143627166748]],\n",
       "   [[[384.0, 381.0], [405.0, 381.0], [405.0, 442.0], [384.0, 442.0]],\n",
       "    ['PRelu', 0.9744795560836792]],\n",
       "   [[[17.0, 398.0], [120.0, 398.0], [120.0, 432.0], [17.0, 432.0]],\n",
       "    ['(Xr,Xi)', 0.9725239872932434]],\n",
       "   [[[1276.0, 455.0], [1340.0, 444.0], [1344.0, 472.0], [1281.0, 483.0]],\n",
       "    ['Scrb', 0.9806489944458008]],\n",
       "   [[[1008.0, 473.0], [1058.0, 473.0], [1058.0, 497.0], [1008.0, 497.0]],\n",
       "    ['Imag', 0.99456387758255]],\n",
       "   [[[1637.0, 475.0], [1733.0, 475.0], [1733.0, 503.0], [1637.0, 503.0]],\n",
       "    ['2D-Conv', 0.9985793828964233]],\n",
       "   [[[993.0, 501.0], [1071.0, 501.0], [1071.0, 523.0], [993.0, 523.0]],\n",
       "    ['Decoder', 0.9985194802284241]],\n",
       "   [[[1641.0, 509.0], [1731.0, 509.0], [1731.0, 536.0], [1641.0, 536.0]],\n",
       "    ['Sigmoid', 0.9994775056838989]],\n",
       "   [[[419.0, 558.0], [780.0, 558.0], [780.0, 586.0], [419.0, 586.0]],\n",
       "    ['Complex Refining Branch', 0.9996910095214844]],\n",
       "   [[[667.0, 596.0], [912.0, 596.0], [912.0, 629.0], [667.0, 629.0]],\n",
       "    ['(a) Overall diagram', 0.9997323751449585]],\n",
       "   [[[1570.0, 597.0], [1792.0, 597.0], [1792.0, 625.0], [1570.0, 625.0]],\n",
       "    ['(b) Mask Decoder', 0.9915393590927124]]],\n",
       "  'ocr': [[[220.0, 388.0], [241.0, 388.0], [241.0, 430.0], [220.0, 430.0]],\n",
       "   ['LN', 0.9907022714614868]]},\n",
       " '2102.13177v3-Figure3-1.png': {'caption': 'Fig. 3: Overview of our algorithm at a timestep. Our method takes in an observation, transforms it into a graph with a 5-dimensional feature per node, and passes it to the GNN policy, which selects an object and goal to input to PickAndPlace .',\n",
       "  'imageText': [],\n",
       "  'image_file': '2102.13177v3-Figure3-1.png',\n",
       "  'sections': [{'heading': 'A. Problem formulation: Graphical representation of state',\n",
       "    'text': 'We encode the environment scene as a graph, whose nodes consist of the task-relevant entities, such as objects and their Fig. 2: An overview of our approach. We train a high-level GNN policy that takes a graph representation of state as input and selects the next object to pick, and the next goal to place it in. A lowlevel PickAndPlace primitive then picks the chosen object and places it in the desired goal. Summary in Algo 1. target positions (goals). Let there be K objects, and L goals in the scene. We create a graph G = (V, E), where the vertices\\nV = {v o k } K k=1 ‚à™ {v g l } L\\nl=1 represent the objects and goals in the scene, giving us a total of K + L nodes. We create a dense, fully-connected graph, where all nodes are connected to all other nodes; E = {e i,j\\n} for i = 1 . . . K + L, j = 1, . . . K + L.\\nEach node v ‚àà V in the graph has a feature vector œÜ(v), which contains node-specific information. The input features of each node are 5-dimensional: a categorical feature {0, 1, 2, 3} denoting if a node is a cover, goal for a cover, block or goal for a block, the 3-dimensional position of the object or goal in the frame of the robot, and a binary feature which is 1 if a goal is filled or an object is in a goal, and 0 for empty goals or objects. The current state graph is input to the GNN policy, which outputs a categorical distribution over objects and goals. The selected object and goal positions are sent as inputs to the PickAndPlace primitive. This is illustrated in Figure 3 for a K = L = 3 block stacking trajectory. Our approach generalizes to situations where number of goals and objects are different. For example, in box rearrangment, the policy learns to move the box cover out of the way by placing the cover on the table before moving blocks, and finally closing the box.\\nIn this work, we deal with problems with a shared underlying task structure -for example, pick the highest block from a stack, and place it in the lowest free goal. We use expert demonstrations to train a GNN policy which learns this underlying structure, in contrast to traditional TAMP, where such constraints are pre-defined. Once this structure is learned, the policy automatically generalizes to new unseen problems, as long as the underlying task structure holds. If the test task has a different structure, we need to collect demonstrations in the new domain.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2},\n",
       "   {'heading': 'B. Training the GNN from demonstrations',\n",
       "    'text': 'We pose a long-horizon manipulation problem as a classification problem at each high-level step where a decision is made over which object to move to where using what action. The output of the GNN policy is K + L dimensional corresponding to the object and goal nodes of the original graph. This is reshaped as two K and L dimensional outputs\\nV out g = {v g l } L l=1 and V out o = {v o k } K k=1 . V out o\\nis then passed through a softmax function to generate a K-dimensional categorical distribution\\nP o pred = {p o 1 , p o 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ p o K }\\ndepicting the picking probabilities of objects. The object with the highest predicted probability is the output of the GNN.\\no * = arg max j p(o j ) where p(o j ) = exp(v o j ) K k=1 exp(v o k )(1)\\nThe same transformation is applied to the goals, resulting in a probability distribution\\nP g pred = {p g 1 , p g 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ p g L }\\nover the goals, and the goal with the highest probability is chosen as the next desired goal. Given target distributions P o tgt for the objects and P g tgt for goals from expert data, the GNN policy parameters Œ∏ are trained to minimize the cross-entropy loss:\\narg min Œ∏ ‚àí K k=1 [P o tgt ] k log(p o k ) ‚àí L l=1 [P g tgt ] l log(p g l )(2)\\nThe expert demonstrations used for training the GNN policy are also cast as a graph with target output distributions coming from the expert action. We collect N demonstrations of the expert solving the task. At each step t, we extract input-output pairs {(s } is converted into two K and Ldimensional target distributions P o tgt and P g tgt for goal and object prediction, respectively.\\nt = (o k=1,‚Ä¢‚Ä¢‚Ä¢ ,K , g l=1,‚Ä¢‚Ä¢‚Ä¢ ,L ), a t )},\\nP o tgt = 1[o k = o exp b ]\\nis a onehot vector: 1 for the object chosen by the expert, and 0 for all others. Similarly,\\nP g tgt = 1[g l = g exp b ]\\nis a one-hot vector: 1 for the goal chosen by the expert, and 0 for all others. Parameters Œ∏ of the GNN are learned to minimize the crossentropy loss (Eq. 2) between prediction of the GNN policy given G b as input, and target distributions P o tgt and P g tgt . We note that this high-level policy could be learned in many ways, and one does not need to use a GNN. For example, we could learn a feed-forward multilayer perceptron (MLP) that takes as input the features of the blocks and goals, and predicts the next block and goal. However, if the MLP policy is trained on K = 3 objects, it does not generalize to K = 4, since the number of inputs, and architecture of the policy are different for different K. On the other hand, GNNs Fig. 3: Overview of our algorithm at a timestep. Our method takes in an observation, transforms it into a graph with a 5-dimensional feature per node, and passes it to the GNN policy, which selects an object and goal to input to PickAndPlace . generalize to different number of nodes in the graph, and hence can be used on variable number of objects. Our GNN policy trained on K = 3, 4 shows zero-shot generalization on K = 2, 3, ‚Ä¢ ‚Ä¢ ‚Ä¢ , 9 (Section V-A).',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Efficient and Interpretable Robot Manipulation with Graph Neural Networks',\n",
       "  'abstract': 'Manipulation tasks like loading a dishwasher can be seen as a sequence of spatial constraints and relationships between different objects. For example, a plate can be placed in a tray only if the tray is open. We aim to discover such task-specific rules from demonstrations. We pose manipulation as a classification problem over a graph, whose nodes represent task relevant entities like objects and goals, transform the environment scene into a graph and learn a graph neural network (GNN) policy using imitation learning. In our experiments, a single learned GNN policy, trained using 20 expert demonstrations, can solve multiple blockstacking and rearrangement tasks in both simulation and on hardware, without any task description. The policy successfully generalizes over the number of objects in the environment, their positions, and goal configurations (trained on single stacks, generalizes to pyramids and multiple stacks). We also apply our approach to a complex simulated dishwasher environment, where a robot learns to load a dishwasher from only 5 high-level human demonstrations. These experiments show that imitation learning on a graphical state and policy is a simple, yet powerful tool for solving complex long-horizon manipulation problems, without requiring detailed task descriptions. Videos can be found at: https://youtu.be/x9hcKBh6K0A.',\n",
       "  'paddleOCR': [[[[418.0, 32.0], [557.0, 40.0], [554.0, 85.0], [415.0, 77.0]],\n",
       "    ['[1,xj,1]', 0.9960362911224365]],\n",
       "   [[[755.0, 43.0], [794.0, 43.0], [794.0, 82.0], [755.0, 82.0]],\n",
       "    ['83', 0.991852343082428]],\n",
       "   [[[915.0, 41.0], [952.0, 41.0], [952.0, 71.0], [915.0, 71.0]],\n",
       "    [' 0]', 0.819028913974762]],\n",
       "   [[[989.0, 43.0], [1118.0, 43.0], [1118.0, 74.0], [989.0, 74.0]],\n",
       "    ['M Message-', 0.9950332641601562]],\n",
       "   [[[1137.0, 40.0], [1281.0, 32.0], [1283.0, 69.0], [1139.0, 76.0]],\n",
       "    ['pi = 0.05', 0.9914147853851318]],\n",
       "   [[[1490.0, 43.0], [1523.0, 43.0], [1523.0, 74.0], [1490.0, 74.0]],\n",
       "    ['83', 0.9802253246307373]],\n",
       "   [[[1555.0, 36.0], [1697.0, 22.0], [1702.0, 68.0], [1559.0, 81.0]],\n",
       "    ['pi = 0.05', 0.9683501124382019]],\n",
       "   [[[589.0, 50.0], [608.0, 50.0], [608.0, 71.0], [589.0, 71.0]],\n",
       "    ['0', 0.9660208225250244]],\n",
       "   [[[1736.0, 61.0], [1830.0, 61.0], [1830.0, 84.0], [1736.0, 84.0]],\n",
       "    ['PickAnd', 0.9998756647109985]],\n",
       "   [[[1012.0, 71.0], [1097.0, 76.0], [1095.0, 107.0], [1010.0, 101.0]],\n",
       "    [' passing', 0.9245694875717163]],\n",
       "   [[[247.0, 95.0], [387.0, 95.0], [387.0, 126.0], [247.0, 126.0]],\n",
       "    ['Update graph', 0.961862325668335]],\n",
       "   [[[1746.0, 87.0], [1814.0, 87.0], [1814.0, 117.0], [1746.0, 117.0]],\n",
       "    ['Place', 0.999647319316864]],\n",
       "   [[[1005.0, 108.0], [1105.0, 108.0], [1105.0, 132.0], [1005.0, 132.0]],\n",
       "    ['iterations', 0.9995918273925781]],\n",
       "   [[[1139.0, 119.0], [1263.0, 112.0], [1266.0, 151.0], [1142.0, 159.0]],\n",
       "    ['p2 = 0.9', 0.9212265014648438]],\n",
       "   [[[1486.0, 117.0], [1525.0, 117.0], [1525.0, 158.0], [1486.0, 158.0]],\n",
       "    ['82', 0.9927881956100464]],\n",
       "   [[[1557.0, 111.0], [1682.0, 100.0], [1686.0, 146.0], [1561.0, 157.0]],\n",
       "    ['p2 = 0.9', 0.8990143537521362]],\n",
       "   [[[516.0, 123.0], [547.0, 123.0], [547.0, 154.0], [516.0, 154.0]],\n",
       "    ['0]', 0.9581615924835205]],\n",
       "   [[[829.0, 121.0], [858.0, 121.0], [858.0, 151.0], [829.0, 151.0]],\n",
       "    ['[2', 0.8474496603012085]],\n",
       "   [[[924.0, 121.0], [952.0, 121.0], [952.0, 151.0], [924.0, 151.0]],\n",
       "    ['0]', 0.826559841632843]],\n",
       "   [[[236.0, 138.0], [407.0, 138.0], [407.0, 175.0], [236.0, 175.0]],\n",
       "    ['xj ~ xj,x2,x3', 0.848971426486969]],\n",
       "   [[[891.0, 149.0], [902.0, 149.0], [902.0, 162.0], [891.0, 162.0]],\n",
       "    [')', 0.6068834066390991]],\n",
       "   [[[1555.0, 182.0], [1699.0, 177.0], [1701.0, 222.0], [1557.0, 228.0]],\n",
       "    ['p3 = 0.05', 0.9092522859573364]],\n",
       "   [[[415.0, 200.0], [552.0, 194.0], [554.0, 242.0], [417.0, 247.0]],\n",
       "    ['[1, x3, 0]', 0.8976905941963196]],\n",
       "   [[[748.0, 203.0], [783.0, 195.0], [794.0, 238.0], [759.0, 247.0]],\n",
       "    ['18', 0.915200412273407]],\n",
       "   [[[832.0, 206.0], [851.0, 206.0], [851.0, 232.0], [832.0, 232.0]],\n",
       "    ['2', 0.99940025806427]],\n",
       "   [[[1142.0, 199.0], [1281.0, 192.0], [1283.0, 231.0], [1144.0, 239.0]],\n",
       "    ['p3 = 0.05', 0.9705100655555725]],\n",
       "   [[[1488.0, 201.0], [1519.0, 201.0], [1519.0, 236.0], [1488.0, 236.0]],\n",
       "    ['81', 0.9781394004821777]]],\n",
       "  'ocr': [[[755.0, 43.0], [794.0, 43.0], [794.0, 82.0], [755.0, 82.0]],\n",
       "   ['83', 0.991852343082428]]},\n",
       " '2101.07890v1-Figure1-1.png': {'caption': 'FIG. 1. (a) The experimental setup for microwave detection of magnon dynamics. Various parts of the setup are color-coded: orange denotes the pumping circuit, blue highlights the receiving circuit, and red marks the test circuit. (b) Schematic representation of the bulk BEC mode and one of the edge magnon modes in a cuboid YIG sample. The monotonic blue line shows the profile of the static magnetic field H within the YIG sample. Color points denote three field values: A ‚Äì deeply inside the sample, B ‚Äì at the point near the sample edge, where the bulk BEC mode becomes evanescent with purely imaginary wavenumber, C ‚Äì at the sample edge. (c) Schematic representation of magnon dispersion curves in the middle of the sample (at point A) and near the edge (at point B). The green, blue, and red signal intensity lines represent the microwave power spectra from the YIG sample registered during the one-microsecond interval before the end of the pumping action, and 2¬µs and 4¬µs after the pump pulse is turned off, respectively.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2101.07890v1-Figure1-1.png',\n",
       "  'sections': [{'heading': '',\n",
       "    'text': 'The Bose-Einstein condensate (BEC) is a state of matter encompassing a macroscopically large number of bosons that occupy the lowest quantum state, demonstrating coherence at macroscopic scales [1][2][3][4][5]. This phenomenon was observed and investigated in atomic systems such as 4 He, 3 He (where the role of bosons is played by Cooper pairs of fermionic 3 He atoms), and in ultra-cold trapped atoms [6,7]. BECs were also found in systems of bosonic quasiparticles such as polaritons [8] and excitons [9] in semiconductors, photons in micro-cavities [10], as well as magnons in superfluid 3 He [11] and magnetic crystals [12][13][14].\\nThe presence of macroscopic coherence is of fundamental importance for understanding the physical properties of BECs, including such exciting phenomena as superconductivity and superfluidity. Furthermore, there is a range of novel effects and applications that exploit the coherence of macroscopic BEC wave functions [15][16][17][18][19][20], especially in the rapidly developing field of quantum computing [17][18][19][20]. Unlike already demonstrated superconductor-based quantum computers, which operate at temperatures around 20 ¬µK [21], BEC-based qubits can be implemented at significantly higher temperatures. For instance, a magnon BEC in ferrimagnetic yttrium iron garnet (Y 3 Fe 5 O 12 , YIG) [22] crystals is formed even at room temperature [23].\\nThe magnon condensate is usually created in YIG by parametric pumping of magnons in an external microwave electromagnetic field. In this process [24,25], external microwave photons of frequency œâ p and wavenumber q p 0 split into two magnons with the frequency œâ m = œâ p /2 and wavevectors ¬±q m . They populate a gaseous magnon distribution with internal interactions provided by the four-magnon scattering processes 2 ‚áî 2. Eventually the magnon gas thermalizes to the bottom of the frequency spectrum [26] and forms a Bose-Einstein condensate there [12]. In in-plane magnetized YIG films, magnons condense at two equivalent frequency minima œâ min (q) with q = ¬±q BEC .\\nThe magnon BEC is conveniently studied by means of Brillouin light scattering (BLS) spectroscopy [12,13] delivering information about the magnon spectral density distribution. Unfortunately, due to the limited frequency resolution of the optical Fabry-P√©rot interferometers used in BLS facilities, the coherence of a magnon BEC cannot be proven directly. Due to the phase insensitivity of the Brillouin light scattering process, studies of the BEC relaxation dynamics employing time-resolved BLS spectroscopy fail to account for BEC dephasing. The insufficient frequency resolution makes it impossible to separate the relaxation dynamics of condensed and thermal magnons. Moreover, the possible outflow of the condensate from a spatially localized probing light spot complicates the interpretation of the obtained experimental results (see [27] and the corresponding discussion in [28]).\\nAlternatively, magnon BEC coherence can be tested indirectly by observation of phenomena such as quantized vorticity [29], supercurrents [30], Bogoliubov waves [31], or Josephson oscillations [32], which are canonical features of both atomic and quasiparticle quantum condensates. Our studies of some of these phenomena [30][31][32][33][34] have shown that they occur only in a freely evolving magnon gas after switching off the microwave pumping. This takes place probably because the intense pumping process prevents condensation by heating the magnon gas [13] and mixing the magnon frequencies near the bottom of their spectra [28]. The observation of these effects indicates the presence of a time-dependent BEC coherence, but leaves open the question about the degree of coherence.\\nAttempts to qualitatively characterize BEC coherence were made using a novel high-resolution magneto-optical Schematic representation of the bulk BEC mode and one of the edge magnon modes in a cuboid YIG sample. The monotonic blue line shows the profile of the static magnetic field H within the YIG sample. Color points denote three field values: A -deeply inside the sample, B -at the point near the sample edge, where the bulk BEC mode becomes evanescent with purely imaginary wavenumber, C -at the sample edge. (c) Schematic representation of magnon dispersion curves in the middle of the sample (at point A) and near the edge (at point B). The green, blue, and red signal intensity lines represent the microwave power spectra from the YIG sample registered during the one-microsecond interval before the end of the pumping action, and 2 ¬µs and 4 ¬µs after the pump pulse is turned off, respectively.\\nKerr-effect spectroscopy [35,36], microwave spectroscopy of electromagnetic signals emitted at the ferromagnetic resonance frequency due to the confluence of bottom magnons with opposite wavevectors [37,38], and by BLS observations of the interference of ¬±q BEC magnon condensates [29]. They demonstrate a very low modulation depth of the interference pattern [29], a rather broad frequency spectral BEC line [37], and increase in the BEC line width when the pumping power exceeds the threshold of BEC formation [36]. These results themselves are certainly interesting and important. However, without additional data on the temporal evolution of coherence, their interpretation is difficult and remains questionable.\\nThe main goal of this work is to understand the time evolution of the magnon gas toward a coherent BEC state. By direct measurement of microwave radiation from a bulk YIG sample, we show that the frequency-broadband emission spectrum, detected during the pumping action, transforms after the end of pumping into a sharp spectral peak at the lowest fre-quency of the magnon spectrum. This peak is earlier formed and gets pronounced with increasing pumping power and, consequently, with the density of parametrically pumped magnons. At high pumping powers, the peak has a Lorentz shape and its width is consistent with the magnetic relaxation frequency into the YIG crystal lattice. The appearance of this peak is associated with the formation of the magnon BEC, whose coherence is, therefore, limited only by the natural magnon decay.\\nIn YIG films used in all previous BEC studies, the condensed magnons have a wavelength of about a few micrometers and are thus weakly coupled to the electromagnetic field, making them difficult to detect via directly emitted radiation. The main idea of our experiment is to use a YIG cuboid bulk sample to enhance this coupling. The experimental setup is shown in Fig. 1(a). The YIG sample sized 0.3 √ó 0.3 √ó 0.6 mm 3 is magnetized along its long side, which is oriented along the x coordinate axis. Due to the demagnetization effect, the static magnetic field H(x) inside such a sample [blue line in Fig. 1(b)] is smaller at its edges than in the middle. For a slowly spatially-varying magnetic field H(x), the magnon frequency may be considered as an adiabatic invariant: œâ q(x), H(x) = const, while the wavevector becomes position-dependent q ‚Üí q(x) [39,40].\\nFor the BEC magnons, this frequency is equal to the frequency of the spectrum minimum œâ min in the central part of the sample [marked by point A in Fig. 1\\n(b)]: œâ q(x), H(x) = œâ min = œâ(q min , H A ) ,(1)\\nproviding a relation between q(x) and H(x). The bulk frequency spectrum œâ(q, H A ) is schematically shown by the red line in the upper part of Fig. 1(c). As one moves from point A to some point B near the sample edge, the magnetic field decreases and the spectrum branch œâ q(x), H(x) is continuously shifted down. The spectrum œâ(q, H B ) for the lower magnetic field at point B is schematically shown by the orange line in the lower part of Fig. 1(c). Therefore, according to Eq. (1), the wavenumber q(x) of the BEC magnons with œâ q(x), H(x) = œâ min decreases towards the edges of the sample, reaching zero value for x = x B as is indicated by the black dashed arrow in Fig. 1(c). For x < x B , the bulk mode becomes evanescent with a purely imaginary wavenumber. In the near-edge region, between points B and C, only localized edge modes exist. A small value of q(x) near point B, and, correspondingly, a large wavelength of magnons, enhances the coupling of the magnon BEC with the electromagnetic field.\\nThe large volume of the sample and its cuboidal shape make it possible to achieve the desired detection sensitivity using a simple inductive loop antenna placed around the sample and connected to the receiving circuit marked in blue in Fig. 1(a). The fast microwave switch is used to measure power-frequency radiation spectra J rad (œâ, t) in 1 ¬µs-long time windows shifted by 0.5 ¬µs steps. The low-pass filter protects the spectrum analyzer from a strong pumping signal. Magnons are pumped by 6 ¬µs-long pulses of the electromagnetic field of frequency œâ p = 2œÄ ‚Ä¢ 7.68 GHz, whose amplitude is enhanced by a dielectric resonator (see Fig. 1(a), where the orange circuitry illustrates the pumping circuit).\\nConsider first the structure of the eigenmodes of the cuboid sample. Their absorption spectrum J abs (œâ, t), measured by a vector network analyzer and colored in red in Fig. 1(a), is shown by the red line in Fig. 2(a). In the same figure, the green line denotes the radiation spectrum J rad (œâ, t) of the sample measured during the last microsecond of pumping. Above œâ/(2œÄ) > 3.41 GHz, one can see a set of discrete peaks, whose frequencies coincide [41] in both spectra [see thin vertical dashed lines in Fig. 1(a)]. They originate from the bulk magnon modes, schematically shown on the magnon dispersion branch A in Fig. 1(c). In an infinite sample, the spectrum of such modes is continuous. However, in the finite sample, only a discrete set of wavenumbers q n is allowed. In a simple case of a longitudinally magnetized bar of length L, the periodic boundary conditions dictate q n = 2œÄn/L. They are illustrated in Fig. 1(c) by gray dotted vertical lines. The corresponding \"allowed\" values of œâ = œâ n = œâ(q n ) are shown by empty dots and horizontal gray lines. Larger values of œâ(q n ) correspond to smaller q n , which are better coupled with the inductive loop. This explains why the peaks at higher frequencies are more pronounced in Fig. 2(a). Furthermore, the peak positions become closer as œâ approaches œâ min from above. This behavior is well reproduced by the spectra in Fig. 2(a), where œâ min /(2œÄ) = 3.41 GHz.\\nThe part of the spectra at œâ < œâ min originates from the modes localized near the sample edges. Indeed, the decreasing intrinsic magnetic field [blue line in Fig. 1(b)] between B and the edge of the sample serves as a potential well. In this well, there exists a discrete set of magnon states having a relatively large characteristic scale. These edge modes are well coupled with the electromagnetic field around the sample and therefore are affected by additional radiation damping. Since the additional damping results in a low quality factor of these modes, their discrete structure is hardly visible in the radiation spectrum. For the same reason, these modes practically do not contribute to the absorption spectrum. Note also that the actual positions of the peaks in Fig. 2(a) are not so regular as expected from the one-dimensional model. In a finite sample of a general shape, the role of \"allowed\" œâ n is played by the frequencies of so-called Walker modes in a cuboid, which may be not equidistant [42].\\nConsider now the evolution of the radiation spectrum J rad (œâ, t). During the pumping, it extends from 2.8 GHz to 4 GHz as is indicated by the green line in Fig. 2(a). The main radiation power is located in the 100 MHz band around œâ min . Such a large width is caused by intensive shaking of the entire magnon frequency spectra by a powerful microwave pumping field. For instance, for P p = 26.5 dB, the amplitude of the microwave pumping field h p applied parallel to the bias magnetic field H is estimated to be about 25 Oe. As a result, the magnon frequency spectrum moves up and down in the range of ¬±70 MHz, which is close to the radiation spectrum width.\\nAfter switching off the pumping power, the shaking of the magnon frequencies ceases and the spectrum width quickly decreases as seen in Fig. 2(a-c). The edge modes with œâ < œâ min uniformly decay within the first 2 ¬µs, likely due to effective radiation damping. The evolution of the bulk modes with œâ œâ min is more complicated. The most intense peaks in the initial spectrum are strongly decreased already within a time interval of 0.5 ¬µs, especially at frequencies, for which the radiation damping is most efficient. Another reason for the spectrum narrowing is the redistribution of magnons towards modes with œâ œâ min during the BEC formation.\\nIn Fig. 2(b) and (c), we show details of the further evolution of J rad (œâ, t). Here we plot the spectra for more narrow frequency intervals, colored in Fig. 2 To quantify the radiation spectra, we investigate their bandwidth Œ¥œâ. For single-peak spectra, we chose Œ¥œâ as the peak width at the half-maximum magnitude. This definition corresponds to the width of the Lorentz peak, describing a uniformly broadened spectral line. For the spectra with complex many-peak structure, such as the spectra in Fig. 2(a), we generalize this definition as follows:\\nŒ¥œâ = 2 ‚Ñ¶ 2 f (‚Ñ¶)d‚Ñ¶ f (‚Ñ¶)d‚Ñ¶ ,(2)\\nwhere ‚Ñ¶ = œâ ‚àí œâ min and f (‚Ñ¶) is the truncated spectrum f (‚Ñ¶), with the spectrum part below 5.5% of its maximum magnitude removed. The time evolution of the bandwidth Œ¥œâ(t) for different P p from 22.5 dB to 26.5 dB above the threshold of the parametric instability, is shown in Fig. 2(d). The bandwidth during the pump pulse (t < 0) is larger for larger P p . After the pumping is turned off, Œ¥œâ decreases monotonically due to the Bose-Einstein condensation process. This process is dominated by four-magnon scattering processes with a rate proportional to N 2 [25,26], where N is the number of bottom magnons. Increasing N at larger P p leads to more efficient magnon gathering toward œâ min and a faster decrease in Œ¥œâ. This narrowing has a threshold character and occurs when the pumping power increases from 22.5 dB to 24 dB. We consider this as additional evidence of magnon condensate formation at P p 24 dB. The insert in Fig. 2(d) presents J rad (œâ, t) spectra measured near the detection limit of the experimental setup for two low pumping powers of P p = 23.0 dB and 23.5 dB, and for the highest value of P p = 26.5 dB. Being rather weak, they correspond to the final stages of the evolution of the magnon system at the bottom of their spectrum, when no non-linear scattering is expected and both condensed and gaseous magnons linearly decay to the thermal phonon bath. However, the structure of these residual spectra is determined by the previous processes of nonlinear four-magnon scattering and BEC formation. For weaker pumping, the spectral line at œâ min is surrounded by a distribution of relatively strongly populated magnon modes, which demonstrate a clear comb-like structure at frequencies above œâ min . Increasing pumping power leads to the de-population of all these modes due to magnon gathering toward the dense BEC. As a result, only the spectral line related to the magnon condensate remains in the spectrum.\\nAt high P p , the residual spectra are best fitted with the Lorentz function\\nJ rad (œâ, t) = I rad (t) Œ¥œâ (œâ ‚àí œâ min ) 2 + Œ¥œâ 2 /4 ,(3)\\nin which Œ¥œâ is the bandwidth of the frequency spectra and I rad (t) is the time-dependent total power of the signal. The fit is shown in Fig. 2(c). Another possible (Gaussian) shape is indicated in Fig. 2(c) by the blue dotted line for comparison. Probably the most important evidence for coherency, as shown in Fig. 2, is that at later times (say, after the time delay t d > 2.5 ¬µs) the exponentially decaying residual spectra for P p ‚â• 24 dB have a near-Lorentzian shape (3) with the bandwidth Œ¥œâ approaching the value of about Œ¥œâ fin /(2œÄ) 0.85 MHz, almost independent of P p .\\nTo summarize, the magnon system evolving toward BEC reaches full coherence, with the width of the magnon radiation spectrum decreasing by more than two orders of magnitude. The residual bandwidth is mainly determined by the lifetime of magnons, as expected for a fully coherent BEC consisting of a single magnon state. Moreover, we show that a coupling of the magnon BEC with dynamic stray fields outside the sample is enabled by a proper choice of the sample shape giving direct spectroscopic access to the BEC. Such an approach can function as a convenient tool for integrating magnetic quantum systems into electrical environments.\\nWe believe that this direct demonstration of the magnon BEC coherence brings closer the implementation of room temperature BEC-based computing.\\nThis research was funded by the European Research Council within the Advanced Grant No. 694709 \"SuperMagnonics\" and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) within the Transregional Collaborative Research Center -TRR 173 -268565370 \"Spin+X\" (project B01). The authors are grateful to G. A. Melkov and H. Yu. Musiienko-Shmarova for fruitful discussions.',\n",
       "    'n_publication_ref': 64,\n",
       "    'n_figure_ref': 27}],\n",
       "  'title': 'Evolution of room-temperature magnon gas toward coherent Bose-Einstein condensate',\n",
       "  'abstract': 'The appearance of spontaneous coherence is a fundamental feature of a Bose-Einstein condensate and an essential requirement for possible applications of the condensates for data processing and quantum computing. In the case of a magnon condensate in a magnetic crystal, such computing can be performed even at room temperature. So far, the process of coherence formation in a magnon condensate was inaccessible. We study the evolution of magnon radiation spectra by direct detection of microwave radiation emitted by magnons in a parametrically driven yttrium iron garnet crystal. By using specially shaped bulk samples, we show that the parametrically overpopulated magnon gas evolves to a state, whose coherence is only limited by the natural magnon relaxation into the crystal lattice.',\n",
       "  'paddleOCR': [[[[0.0, 1.0], [41.0, 1.0], [41.0, 38.0], [0.0, 38.0]],\n",
       "    ['(a)', 0.996244490146637]],\n",
       "   [[[184.0, 6.0], [334.0, 11.0], [333.0, 37.0], [183.0, 32.0]],\n",
       "    ['Microwave', 0.9992824196815491]],\n",
       "   [[[540.0, 1.0], [764.0, 4.0], [763.0, 38.0], [540.0, 34.0]],\n",
       "    ['(b) Edge modes', 0.9992161393165588]],\n",
       "   [[[349.0, 14.0], [442.0, 17.0], [440.0, 65.0], [347.0, 62.0]],\n",
       "    ['M', 0.9300374984741211]],\n",
       "   [[[788.0, 11.0], [936.0, 11.0], [936.0, 32.0], [788.0, 32.0]],\n",
       "    ['Bulk modes', 0.9955767393112183]],\n",
       "   [[[198.0, 47.0], [331.0, 41.0], [332.0, 68.0], [199.0, 74.0]],\n",
       "    ['generator', 0.9991577863693237]],\n",
       "   [[[618.0, 41.0], [755.0, 49.0], [753.0, 78.0], [617.0, 70.0]],\n",
       "    ['u!Wo > M', 0.7845714092254639]],\n",
       "   [[[29.0, 51.0], [119.0, 51.0], [119.0, 78.0], [29.0, 78.0]],\n",
       "    ['Vector', 0.9998364448547363]],\n",
       "   [[[793.0, 42.0], [928.0, 48.0], [927.0, 77.0], [792.0, 71.0]],\n",
       "    ['u!Wm< M', 0.757079005241394]],\n",
       "   [[[19.0, 86.0], [127.0, 86.0], [127.0, 114.0], [19.0, 114.0]],\n",
       "    ['network', 0.9997512102127075]],\n",
       "   [[[583.0, 101.0], [620.0, 101.0], [621.0, 349.0], [584.0, 349.0]],\n",
       "    ['Magnetic field H', 0.9995923638343811]],\n",
       "   [[[15.0, 123.0], [131.0, 123.0], [131.0, 150.0], [15.0, 150.0]],\n",
       "    ['analyzer', 0.9990600347518921]],\n",
       "   [[[214.0, 117.0], [336.0, 117.0], [336.0, 144.0], [214.0, 144.0]],\n",
       "    ['Amplifier', 0.9995267391204834]],\n",
       "   [[[215.0, 196.0], [357.0, 196.0], [357.0, 222.0], [215.0, 222.0]],\n",
       "    ['Attenuator', 0.9989954829216003]],\n",
       "   [[[907.0, 213.0], [982.0, 213.0], [982.0, 244.0], [907.0, 244.0]],\n",
       "    ['BEC', 0.9995368123054504]],\n",
       "   [[[795.0, 278.0], [861.0, 278.0], [861.0, 311.0], [795.0, 311.0]],\n",
       "    ['YIG', 0.969713032245636]],\n",
       "   [[[151.0, 302.0], [277.0, 302.0], [277.0, 328.0], [151.0, 328.0]],\n",
       "    ['Dielectric', 0.9998008012771606]],\n",
       "   [[[987.0, 303.0], [1011.0, 303.0], [1011.0, 331.0], [987.0, 331.0]],\n",
       "    ['x', 0.9836868643760681]],\n",
       "   [[[351.0, 313.0], [506.0, 284.0], [512.0, 313.0], [356.0, 342.0]],\n",
       "    ['Waveguide', 0.9997961521148682]],\n",
       "   [[[146.0, 341.0], [277.0, 337.0], [278.0, 360.0], [147.0, 363.0]],\n",
       "    ['resonator', 0.9977933168411255]],\n",
       "   [[[419.0, 336.0], [451.0, 336.0], [451.0, 367.0], [419.0, 367.0]],\n",
       "    ['H', 0.9771241545677185]],\n",
       "   [[[540.0, 381.0], [582.0, 381.0], [582.0, 418.0], [540.0, 418.0]],\n",
       "    ['(c)', 0.9943724274635315]],\n",
       "   [[[673.0, 381.0], [897.0, 390.0], [896.0, 423.0], [672.0, 415.0]],\n",
       "    [' Frequency ', 0.9607445001602173]],\n",
       "   [[[7.0, 441.0], [137.0, 443.0], [136.0, 470.0], [6.0, 468.0]],\n",
       "    ['Amplifier', 0.9997198581695557]],\n",
       "   [[[961.0, 439.0], [990.0, 439.0], [986.0, 601.0], [957.0, 601.0]],\n",
       "    ['Bulk modes', 0.9995183944702148]],\n",
       "   [[[881.0, 456.0], [939.0, 461.0], [937.0, 486.0], [878.0, 481.0]],\n",
       "    ['qmin', 0.9337803721427917]],\n",
       "   [[[198.0, 522.0], [260.0, 525.0], [259.0, 553.0], [197.0, 550.0]],\n",
       "    ['Low', 0.9944171905517578]],\n",
       "   [[[840.0, 531.0], [916.0, 531.0], [916.0, 563.0], [840.0, 563.0]],\n",
       "    ['BEC', 0.9995290637016296]],\n",
       "   [[[195.0, 564.0], [263.0, 564.0], [263.0, 588.0], [195.0, 588.0]],\n",
       "    ['pass', 0.9995719194412231]],\n",
       "   [[[848.0, 563.0], [914.0, 574.0], [910.0, 601.0], [844.0, 590.0]],\n",
       "    ['Wmin', 0.9957903623580933]],\n",
       "   [[[197.0, 591.0], [260.0, 594.0], [259.0, 622.0], [196.0, 619.0]],\n",
       "    ['filter', 0.9989274144172668]],\n",
       "   [[[994.0, 588.0], [1007.0, 585.0], [1011.0, 606.0], [998.0, 609.0]],\n",
       "    ['A', 0.5459803342819214]],\n",
       "   [[[77.0, 618.0], [173.0, 622.0], [172.0, 653.0], [76.0, 649.0]],\n",
       "    ['Switch', 0.9997280240058899]],\n",
       "   [[[321.0, 634.0], [396.0, 640.0], [394.0, 670.0], [319.0, 664.0]],\n",
       "    ['Loop', 0.9997509717941284]],\n",
       "   [[[961.0, 629.0], [994.0, 630.0], [990.0, 800.0], [957.0, 800.0]],\n",
       "    ['Edge modes', 0.999935507774353]],\n",
       "   [[[301.0, 673.0], [415.0, 673.0], [415.0, 698.0], [301.0, 698.0]],\n",
       "    ['antenna', 0.9991750121116638]],\n",
       "   [[[689.0, 769.0], [713.0, 769.0], [713.0, 799.0], [689.0, 799.0]],\n",
       "    ['C', 0.8867790699005127]],\n",
       "   [[[10.0, 796.0], [140.0, 796.0], [140.0, 823.0], [10.0, 823.0]],\n",
       "    ['Spectrum', 0.9997580647468567]],\n",
       "   [[[552.0, 787.0], [646.0, 787.0], [646.0, 817.0], [552.0, 817.0]],\n",
       "    ['Signal', 0.999843418598175]],\n",
       "   [[[15.0, 831.0], [133.0, 831.0], [133.0, 859.0], [15.0, 859.0]],\n",
       "    ['analyzer', 0.9990599155426025]],\n",
       "   [[[546.0, 826.0], [667.0, 830.0], [666.0, 860.0], [545.0, 856.0]],\n",
       "    ['intensity', 0.9996581673622131]],\n",
       "   [[[727.0, 826.0], [948.0, 831.0], [948.0, 860.0], [726.0, 855.0]],\n",
       "    ['Wavenumber q', 0.9945843815803528]]],\n",
       "  'ocr': [[[321.0, 634.0], [396.0, 640.0], [394.0, 670.0], [319.0, 664.0]],\n",
       "   ['Loop', 0.9997509717941284]]},\n",
       " '2205.09510v2-Figure6.5-1.png': {'caption': 'Figure 6.5: An illustration of the ‚ÄúQQ‚Äù setting of quantum machine learning, in which data are quantum and processing is quantum.',\n",
       "  'imageText': ['loss',\n",
       "   'quantum',\n",
       "   'classical',\n",
       "   'dataquantum',\n",
       "   'data',\n",
       "   'average',\n",
       "   'classical',\n",
       "   'optimizer'],\n",
       "  'image_file': '2205.09510v2-Figure6.5-1.png',\n",
       "  'sections': [{'heading': 'A Taxonomy of Quantum Machine Learning',\n",
       "    'text': 'Unlike classical machine learning, in which data and processing are classical, in quantum machine learning data and/or processing are quantum. The four possible scenarios are summarized in Table 6.1, and are reviewed in this section.\\nThe classical data and classical processing (CC) case prescribes the optimization of a standard machine learning model, e.g., of a neural network, via a classical optimizer. As illustrated in Fig. 6.2, a classical machine learning model implements a parametrized function h(x|Œ∏) mapping classical input data x to an output. The classical optimizer minimizes some cost function over vector Œ∏ based on classical data. For example, in supervised learning, classical data consist of pairs of inputs and corresponding desired outputs. Moreover, the cost function is given by the training loss, which measures how well the predictions of the outputs produced by function f (x|Œ∏) match the desired outputs in the available data.\\nCurrently, the most common quantum machine learning setting is characterized by classical data and quantum processing (CQ). In this case, as illustrated in Fig. 6.3, a quantum model is typically defined by a PQC that implements a unitary transformation U (x, Œ∏). The unitary U (x, Œ∏) depends on both classical input x and model parameter vector Œ∏. The input to the PQC is a set of qubits in the ground state |0 , and the output state produced by the PQC is given by the so-called quantum embedding (of the classical data The classical optimizer minimizes some cost function over the model parameter vector Œ∏. The cost function depends on classical data, as well as on an estimate of the expectation of an observable for the quantum embedding |œà(x, Œ∏) . Estimating the expectation of the given cost-defining observable requires running the PCQ multiple times in order to evaluating an empirical average.\\nAs illustrated in Fig. 6.4, with quantum data and classical processing (QC), quantum data are first measured, and then the classical measurement outputs are processed by a classical machine learning model. QC machine learning may be applied to quantum tomography, which is the problem of inferring properties of a quantum state based on measurement results.\\nIn the quantum data and quantum processing (QQ) case, the learner has access to quantum data defined by a collection of quantum systems, each in some density state. Quantum data may be obtained, for instance, via quantum sensors. As illustrated in Fig. 6.5, such data can determine the input to the PQC, and/ or serve as a target to be compared to the quantum state output by the PQC through a quantum loss metric (e.g., the fidelity). An instance of this case is given by quantum generative adversarial networks, in which the quantum loss is computed via a detector that attempts to distinguish real quantum data from quantum states generated by the PQC. Another example is given by quantum variational autoencoders, which aim at compressing a quantum state into a smaller set of qubits. We refer to the recommended resources (Sec. 6.12) for pointers to the literature. In this chapter, we will focus exclusively on the CQ case, which appears to be better studied and more suitable for engineering applications. That said, the discussion in the next section regarding PQCs is Quantum Machine Learning also relevant for the QQ solutions.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 4}],\n",
       "  'title': 'An Introduction to Quantum Machine Learning for Engineers',\n",
       "  'abstract': 'In the current noisy intermediate-scale quantum (NISQ) era, quantum machine learning is emerging as a dominant paradigm to program gate-based quantum computers. In quantum machine learning, the gates of a quantum circuit are parametrized, and the parameters are tuned via classical optimization based on data and on measurements of the outputs of the circuit. Parametrized quantum circuits (PQCs) can efficiently address combinatorial optimization problems, implement probabilistic generative models, and carry out inference (classification and regression). This monograph provides a self-contained introduction to quantum machine learning for an audience of engineers with a background in probability and linear algebra. It first describes the necessary background, concepts, and tools necessary to describe quantum operations and measurements. Then, it covers parametrized quantum circuits, the variational quantum eigensolver, as well as unsupervised and supervised quantum machine learning formulations.',\n",
       "  'paddleOCR': [[[[112.0, 140.0],\n",
       "     [236.0, 140.0],\n",
       "     [236.0, 204.0],\n",
       "     [112.0, 204.0]],\n",
       "    ['U(e)', 0.9245802760124207]],\n",
       "   [[[324.0, 147.0], [427.0, 143.0], [428.0, 171.0], [324.0, 174.0]],\n",
       "    ['quantum', 0.9997400045394897]],\n",
       "   [[[715.0, 141.0], [822.0, 141.0], [822.0, 169.0], [715.0, 169.0]],\n",
       "    ['classical', 0.9998738169670105]],\n",
       "   [[[351.0, 178.0], [401.0, 181.0], [400.0, 208.0], [349.0, 205.0]],\n",
       "    ['loss', 0.9492791295051575]],\n",
       "   [[[708.0, 178.0], [828.0, 178.0], [828.0, 208.0], [708.0, 208.0]],\n",
       "    ['optimizer', 0.9997224807739258]],\n",
       "   [[[550.0, 247.0], [650.0, 251.0], [649.0, 278.0], [549.0, 275.0]],\n",
       "    ['average', 0.999472975730896]],\n",
       "   [[[258.0, 338.0], [418.0, 334.0], [419.0, 360.0], [259.0, 364.0]],\n",
       "    ['quantum data', 0.9955901503562927]]],\n",
       "  'ocr': [[[715.0, 141.0], [822.0, 141.0], [822.0, 169.0], [715.0, 169.0]],\n",
       "   ['classical', 0.9998738169670105]]},\n",
       " '2207.05078v1-Figure1-1.png': {'caption': 'Fig. 1. Process flowchart',\n",
       "  'imageText': [],\n",
       "  'image_file': '2207.05078v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Process Workflow',\n",
       "    'text': \"The process workflow to determining the needed number of samples as well as the distance threshold is divided into three stages as shown in Fig. 1.\\n-Acquisition: In this stage, two datasets are involved, a training dataset and a testing dataset. In our empirical experiments (see sec. 3.2), these datasets are generated from the simulation, but in general they should be derived during development. At this point, power analysis is used to find the number of samples to determine the difference between the operational and training set. In general, this factor can be calibrated for the application at hand, as it determines an additional number of samples beyond the minimum needed to achieve the determined test power. The effect size for the power analysis is established between the training and testing set, using Cohen's d coefficient [4].  the Acquisition stage is used to aggregate operational data points into an operational set. SafeML measures evaluate the statistical distance between this operational set and the TSS. If the value falls within the defined threshold, the model continues its operation normally, otherwise, a signal is sent to run a user-defined action.\",\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Keep your Distance: Determining Sampling and Distance Thresholds in Machine Learning Monitoring',\n",
       "  'abstract': \"Machine Learning (ML) has provided promising results in recent years across different applications and domains. However, in many cases, qualities such as reliability or even safety need to be ensured. To this end, one important aspect is to determine whether or not ML components are deployed in situations that are appropriate for their application scope. For components whose environments are open and variable, for instance those found in autonomous vehicles, it is therefore important to monitor their operational situation to determine its distance from the ML components' trained scope. If that distance is deemed too great, the application may choose to consider the ML component outcome unreliable and switch to alternatives, e.g. using human operator input instead. SafeML is a model-agnostic approach for performing such monitoring, using distance measures based on statistical testing of the training and operational datasets. Limitations in setting SafeML up properly include the lack of a systematic approach for determining, for a given application, how many operational samples are needed to yield reliable distance information as well as to determine an appropriate distance threshold. In this work, we address these limitations by providing a practical approach and demonstrate its use in a well known traffic sign recognition problem, and on an example using the CARLA open-source automotive simulator.\",\n",
       "  'paddleOCR': [[[[629.0, 100.0],\n",
       "     [744.0, 100.0],\n",
       "     [744.0, 121.0],\n",
       "     [629.0, 121.0]],\n",
       "    ['Sample Set', 0.9586920738220215]],\n",
       "   [[[614.0, 124.0], [753.0, 126.0], [752.0, 147.0], [614.0, 146.0]],\n",
       "    ['form Runtime', 0.9901720881462097]],\n",
       "   [[[11.0, 140.0], [36.0, 140.0], [36.0, 265.0], [11.0, 265.0]],\n",
       "    ['Acquisition', 0.9987865686416626]],\n",
       "   [[[136.0, 180.0], [158.0, 180.0], [158.0, 190.0], [136.0, 190.0]],\n",
       "    ['tar', 0.9173030257225037]],\n",
       "   [[[283.0, 199.0], [349.0, 203.0], [348.0, 224.0], [282.0, 221.0]],\n",
       "    ['Create', 0.999319851398468]],\n",
       "   [[[741.0, 210.0], [875.0, 212.0], [875.0, 233.0], [740.0, 232.0]],\n",
       "    ['Power Analysis', 0.9770775437355042]],\n",
       "   [[[1195.0, 201.0], [1359.0, 201.0], [1359.0, 222.0], [1195.0, 222.0]],\n",
       "    ['leeded Samples for', 0.9936283826828003]],\n",
       "   [[[277.0, 224.0], [356.0, 227.0], [355.0, 249.0], [276.0, 245.0]],\n",
       "    ['Dataset', 0.9984182715415955]],\n",
       "   [[[1192.0, 225.0], [1350.0, 227.0], [1350.0, 244.0], [1192.0, 242.0]],\n",
       "    ['confident measure', 0.9864019751548767]],\n",
       "   [[[733.0, 308.0], [882.0, 308.0], [882.0, 328.0], [733.0, 328.0]],\n",
       "    ['Sample randomly', 0.9988210201263428]],\n",
       "   [[[1097.0, 308.0], [1258.0, 308.0], [1258.0, 328.0], [1097.0, 328.0]],\n",
       "    ['Training Scope set', 0.9759793877601624]],\n",
       "   [[[139.0, 428.0], [189.0, 428.0], [189.0, 450.0], [139.0, 450.0]],\n",
       "    ['Data', 0.9992207884788513]],\n",
       "   [[[96.0, 451.0], [233.0, 456.0], [232.0, 477.0], [95.0, 472.0]],\n",
       "    ['Preprocessing', 0.9989103078842163]],\n",
       "   [[[398.0, 469.0], [501.0, 469.0], [501.0, 491.0], [398.0, 491.0]],\n",
       "    ['Training set', 0.9916273951530457]],\n",
       "   [[[640.0, 469.0], [750.0, 469.0], [750.0, 486.0], [640.0, 486.0]],\n",
       "    ['Build Model', 0.9517632126808167]],\n",
       "   [[[879.0, 514.0], [1042.0, 516.0], [1042.0, 537.0], [879.0, 535.0]],\n",
       "    ['Correctly classified', 0.9949175119400024]],\n",
       "   [[[118.0, 537.0], [211.0, 537.0], [211.0, 554.0], [118.0, 554.0]],\n",
       "    ['Split Data', 0.9697430729866028]],\n",
       "   [[[382.0, 532.0], [479.0, 537.0], [477.0, 559.0], [381.0, 553.0]],\n",
       "    ['Testing set', 0.9915786385536194]],\n",
       "   [[[1148.0, 545.0], [1302.0, 545.0], [1302.0, 565.0], [1148.0, 565.0]],\n",
       "    ['Measure distance', 0.9974327087402344]],\n",
       "   [[[664.0, 568.0], [729.0, 568.0], [729.0, 589.0], [664.0, 589.0]],\n",
       "    ['Predict', 0.9984923601150513]],\n",
       "   [[[857.0, 583.0], [1024.0, 583.0], [1024.0, 603.0], [857.0, 603.0]],\n",
       "    ['Incorrectly classified', 0.9870245456695557]],\n",
       "   [[[1104.0, 667.0], [1270.0, 667.0], [1270.0, 687.0], [1104.0, 687.0]],\n",
       "    ['Distance Threshold', 0.9900144934654236]],\n",
       "   [[[11.0, 827.0], [36.0, 827.0], [36.0, 940.0], [11.0, 940.0]],\n",
       "    ['Operation', 0.9980325698852539]],\n",
       "   [[[576.0, 867.0], [606.0, 867.0], [606.0, 885.0], [576.0, 885.0]],\n",
       "    ['No', 0.9949157238006592]],\n",
       "   [[[693.0, 877.0], [753.0, 881.0], [752.0, 903.0], [692.0, 899.0]],\n",
       "    ['Within', 0.999107301235199]],\n",
       "   [[[1175.0, 879.0], [1368.0, 878.0], [1368.0, 899.0], [1175.0, 901.0]],\n",
       "    ['Aggregate Operational', 0.9893810749053955]],\n",
       "   [[[299.0, 890.0], [469.0, 890.0], [469.0, 910.0], [299.0, 910.0]],\n",
       "    ['User Defined Action', 0.9858841300010681]],\n",
       "   [[[891.0, 891.0], [1048.0, 891.0], [1048.0, 911.0], [891.0, 911.0]],\n",
       "    ['Distance Measure', 0.9993546009063721]],\n",
       "   [[[682.0, 902.0], [771.0, 902.0], [771.0, 919.0], [682.0, 919.0]],\n",
       "    ['Threshold', 0.9964885711669922]],\n",
       "   [[[1235.0, 905.0], [1309.0, 905.0], [1309.0, 922.0], [1235.0, 922.0]],\n",
       "    ['samples', 0.9983869194984436]],\n",
       "   [[[762.0, 982.0], [799.0, 982.0], [799.0, 999.0], [762.0, 999.0]],\n",
       "    ['Yes', 0.9987561702728271]]],\n",
       "  'ocr': [[[1192.0, 225.0], [1350.0, 227.0], [1350.0, 244.0], [1192.0, 242.0]],\n",
       "   ['confident measure', 0.9864019751548767]]},\n",
       " '2107.09896v2-Figure1-1.png': {'caption': 'Fig. 1: System model of secure untrusted mobile UAV-relaying via THz communications with cooperative jamming.',\n",
       "  'imageText': ['BS',\n",
       "   'Untrusted',\n",
       "   'UAV-',\n",
       "   'Relay',\n",
       "   '(UUR)',\n",
       "   'UEk+1',\n",
       "   'UEk-1',\n",
       "   'UEK',\n",
       "   'UE2',\n",
       "   'UE3',\n",
       "   'UEk',\n",
       "   'UE1',\n",
       "   'z',\n",
       "   'x'],\n",
       "  'image_file': '2107.09896v2-Figure1-1.png',\n",
       "  'sections': [{'heading': 'II SYSTEM MODEL AND PROBLEM FORMULATION',\n",
       "    'text': \"We consider a UAV-enabled wireless communication system for data collection from a set of K ground UEs towards a BS via a UAV-assisted mobile amplify-and-forward (AF) relay, as shown in Fig. 1. Here we assume that there are no reliable direct links from UEs to BS (see [24], [34] and references therein), and all nodes are equipped with a single antenna 1 , operating in half-duplex mode. Therefore, a UAVrelay is employed to assist end-to-end communications [18]; nonetheless, the UAV-relay may not be fully authorized to access collected confidential information and may conduct malicious eavesdropping, i.e., an UUR [23]. Thus secure data transmission is in demand.\\nWithout loss of generality, we consider a 3D Cartesian coordinate system, where the BS's horizontal coordinate is located at the origin q b = [0, 0] ‚àà R 1√ó2 , and the ground UEs with horizontal coordinates q k = [x k , y k ] ‚àà R 1√ó2 for ‚àÄk ‚àà K, where K = {1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , K}, are randomly distributed in a circular annulus region with the inner radius R 1 and outer radius R 2 and the coordinates are assumed to be known in prior. Here, R 1 is considered to be the largest distance at which a reliable uplink transmission can be obtained, while beyond R 1 in our case implies no direct link between UE and BS. Further, R 2 indicates the boundary of the permitted flying region for the UAV to provide communication service.\\nWe also consider that UAV flies from and back to the same specific point over the region of interest for a duration of T seconds in order to provide relaying services to all UEs with fairness. This specific point may refer to the checkup point wherein the UAV gets recharged and physically examined to maintain its service. Assuming that UAV flies at a fixed altitude 2 H meters whose instantaneous horizontal coordinate and velocity is represented by q(t) = [x(t), y(t)] and v(t)\\n‚àÜ = dq(t)\\ndt , respectively, where 0 ‚â§ t ‚â§ T . For the ease of analysis, we adopt the time-slotted system such that the flight duration T is equally discretized into N sufficiently small time slots of duration Œ¥ t ‚àÜ = T N . Hence, the UAV's horizontal location at time slot n\\n‚àà N = {1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N } can be approximated by q[n] = [x[n], y[n]]. This assumption is valid when d max t ‚àÜ = Œ¥ t v max u ‚â™ H, wherein d max t\\ndenotes the maximum UAV's displacement per time slot.\",\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Terahertz-supported Untrusted UAV-Relaying: Secrecy Energy Efficiency Maximization via Trajectory and Communication Co-design',\n",
       "  'abstract': \"Unmanned aerial vehicles (UAVs) and Terahertz (THz) technology are envisioned to play paramount roles in nextgeneration wireless communications. Hence, this paper presents a novel secure UAV-assisted mobile relaying system operating at THz bands for data acquisition from multiple ground user equipments towards a destination. We assume that the UAV-mounted relay may act, besides providing relaying services, as a potential adversary called the untrusted UAV relay. To safeguard end-toend communications, we present a secure two-phase transmission strategy with cooperative jamming. Then, we formulate an optimization problem in terms of a new measure ‚àí secrecy energy efficiency (SEE), defined as the ratio of achievable average secrecy rate to average system power consumption, which enables us to obtain the best possible security level while taking UAV's inherent flight power limitation into account. This optimization problem leads to a joint design of key system parameters, including UAV's trajectory and velocity, communication scheduling, and power allocations. Since the formulated problem is a mixed-integer nonconvex optimization and computationally intractable, we propose alternative algorithms to solve it efficiently via greedy/sequential block coordinated descent, successive convex approximation, and non-linear fractional programming techniques. Numerical results demonstrate significant SEE performance improvement of our designs when compared to other known benchmarks.\",\n",
       "  'paddleOCR': [[[[496.0, 7.0], [517.0, 7.0], [517.0, 28.0], [496.0, 28.0]],\n",
       "    ['Z', 0.5164191722869873]],\n",
       "   [[[549.0, 168.0], [813.0, 169.0], [812.0, 190.0], [549.0, 188.0]],\n",
       "    ['Untrusted UAV- Relay (UUR)', 0.9679720401763916]],\n",
       "   [[[450.0, 259.0], [501.0, 263.0], [499.0, 283.0], [449.0, 279.0]],\n",
       "    ['UEk-1', 0.877345860004425]],\n",
       "   [[[617.0, 258.0], [667.0, 258.0], [667.0, 323.0], [617.0, 323.0]],\n",
       "    ['8', 0.9973395466804504]],\n",
       "   [[[249.0, 280.0], [294.0, 280.0], [294.0, 340.0], [249.0, 340.0]],\n",
       "    ['8', 0.9876828789710999]],\n",
       "   [[[625.0, 327.0], [664.0, 330.0], [662.0, 351.0], [623.0, 348.0]],\n",
       "    ['UE3', 0.9936020970344543]],\n",
       "   [[[246.0, 348.0], [298.0, 348.0], [298.0, 368.0], [246.0, 368.0]],\n",
       "    ['UEk+1', 0.9985812306404114]],\n",
       "   [[[827.0, 355.0], [866.0, 358.0], [865.0, 379.0], [825.0, 376.0]],\n",
       "    ['UE1', 0.8904860615730286]],\n",
       "   [[[46.0, 386.0], [89.0, 386.0], [89.0, 406.0], [46.0, 406.0]],\n",
       "    ['UEK', 0.9542855620384216]],\n",
       "   [[[996.0, 387.0], [1010.0, 387.0], [1010.0, 401.0], [996.0, 401.0]],\n",
       "    ['X', 0.5926086902618408]],\n",
       "   [[[246.0, 417.0], [273.0, 417.0], [273.0, 442.0], [246.0, 442.0]],\n",
       "    ['R2', 0.9491726756095886]],\n",
       "   [[[600.0, 419.0], [619.0, 419.0], [619.0, 440.0], [600.0, 440.0]],\n",
       "    ['R', 0.9976783394813538]],\n",
       "   [[[495.0, 433.0], [522.0, 433.0], [522.0, 452.0], [495.0, 452.0]],\n",
       "    ['BS', 0.9963233470916748]],\n",
       "   [[[787.0, 462.0], [833.0, 462.0], [833.0, 526.0], [787.0, 526.0]],\n",
       "    ['8', 0.995590329170227]],\n",
       "   [[[433.0, 498.0], [482.0, 498.0], [482.0, 565.0], [433.0, 565.0]],\n",
       "    ['8', 0.9920547008514404]],\n",
       "   [[[792.0, 534.0], [830.0, 534.0], [830.0, 554.0], [792.0, 554.0]],\n",
       "    ['UE2', 0.9816393256187439]],\n",
       "   [[[437.0, 569.0], [478.0, 572.0], [477.0, 592.0], [436.0, 589.0]],\n",
       "    ['UEk', 0.9164616465568542]]],\n",
       "  'ocr': [[[246.0, 417.0], [273.0, 417.0], [273.0, 442.0], [246.0, 442.0]],\n",
       "   ['R2', 0.9491726756095886]]},\n",
       " '2010.10382v1-Figure23-1.png': {'caption': 'Figure 23: A cross-sectional diagram of a monolayer graphene FET in the dual-gated configuration [9].',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.10382v1-Figure23-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Graphene Field Effect Transistors',\n",
       "  'abstract': \"The past decade has seen rapid growth in the research area of graphene and its application to novel electronics. With Moore's law beginning to plateau, the need for post-silicon technology in industry is becoming more apparent. Moreover, existing technology is insufficient for implementing terahertz detectors and receivers, which are required for a number of applications including medical imaging and security scanning. Graphene is considered to be a key potential candidate for replacing silicon in existing CMOS technology as well as realizing field effect transistors for terahertz detection, due to its remarkable electronic properties, with observed electronic mobilities reaching up to 2 √ó 10 5 cm 2 V ‚àí1 s ‚àí1 in suspended graphene samples. This report reviews the physics and electronic properties of graphene in the context of graphene transistor implementations. Common techniques used to synthesize graphene, such as mechanical exfoliation, chemical vapor deposition, and epitaxial growth are reviewed and compared. One of the challenges associated with realizing graphene transistors is that graphene is semimetallic, with a zero bandgap, which is troublesome in the context of digital electronics applications. Thus, the report also reviews different ways of opening a bandgap in graphene by using bilayer graphene and graphene nanoribbons. The basic operation of a conventional field effect transistor is explained and key figures of merit used in the literature are extracted. Finally, a review of some examples of state-of-the-art graphene field effect transistors is presented, with particular focus on monolayer graphene, bilayer graphene, and graphene nanoribbons.\",\n",
       "  'paddleOCR': [[[[305.0, 27.0], [455.0, 30.0], [454.0, 67.0], [304.0, 65.0]],\n",
       "    ['Top-Gate', 0.9981590509414673]],\n",
       "   [[[50.0, 43.0], [160.0, 46.0], [159.0, 78.0], [49.0, 74.0]],\n",
       "    ['Source', 0.9988226890563965]],\n",
       "   [[[613.0, 41.0], [700.0, 44.0], [699.0, 79.0], [612.0, 76.0]],\n",
       "    ['Drain', 0.9982495307922363]],\n",
       "   [[[509.0, 125.0], [641.0, 127.0], [640.0, 157.0], [509.0, 155.0]],\n",
       "    ['Monolayer', 0.9983125329017639]],\n",
       "   [[[513.0, 161.0], [638.0, 163.0], [638.0, 192.0], [513.0, 189.0]],\n",
       "    ['Graphene', 0.9974233508110046]],\n",
       "   [[[316.0, 198.0], [447.0, 198.0], [447.0, 230.0], [316.0, 230.0]],\n",
       "    ['Top-Gate', 0.9988657236099243]],\n",
       "   [[[317.0, 236.0], [447.0, 239.0], [446.0, 269.0], [316.0, 266.0]],\n",
       "    ['Dielectric', 0.9985825419425964]],\n",
       "   [[[244.0, 327.0], [557.0, 329.0], [557.0, 359.0], [244.0, 357.0]],\n",
       "    ['Back-Gate Dielectric', 0.9810927510261536]],\n",
       "   [[[244.0, 421.0], [270.0, 421.0], [270.0, 454.0], [244.0, 454.0]],\n",
       "    ['0', 0.853668212890625]],\n",
       "   [[[446.0, 433.0], [474.0, 433.0], [474.0, 468.0], [446.0, 468.0]],\n",
       "    ['L', 0.9882302284240723]],\n",
       "   [[[570.0, 424.0], [591.0, 424.0], [591.0, 448.0], [570.0, 448.0]],\n",
       "    ['X', 0.8186005353927612]],\n",
       "   [[[279.0, 481.0], [483.0, 484.0], [482.0, 523.0], [278.0, 519.0]],\n",
       "    ['Substrate', 0.9986212849617004]],\n",
       "   [[[297.0, 669.0], [459.0, 669.0], [459.0, 699.0], [297.0, 699.0]],\n",
       "    ['Back-Gate', 0.9962778091430664]]],\n",
       "  'ocr': [[[509.0, 125.0], [641.0, 127.0], [640.0, 157.0], [509.0, 155.0]],\n",
       "   ['Monolayer', 0.9983125329017639]]},\n",
       " '2101.11681v1-Figure1-1.png': {'caption': 'Fig. 1. Network structure of UAV-NOMA-MEC',\n",
       "  'imageText': ['NOMA',\n",
       "   'uplink',\n",
       "   'Computational',\n",
       "   'tasks',\n",
       "   'Successive',\n",
       "   'interference',\n",
       "   'cancellation',\n",
       "   '(SIC)',\n",
       "   'NOMA',\n",
       "   'downlink',\n",
       "   'Task',\n",
       "   'computational',\n",
       "   'results',\n",
       "   'Superposition',\n",
       "   'coding',\n",
       "   '(SC)',\n",
       "   'learning',\n",
       "   'Deep',\n",
       "   'learning',\n",
       "   'Federated',\n",
       "   'learning',\n",
       "   'reinforcement',\n",
       "   'Deep',\n",
       "   'data',\n",
       "   'Social',\n",
       "   'data',\n",
       "   'Cloud',\n",
       "   'data',\n",
       "   'Wireless',\n",
       "   'MEC',\n",
       "   'structure',\n",
       "   'UAV-NOMA-',\n",
       "   'AI',\n",
       "   'solutions',\n",
       "   'Data',\n",
       "   'collection',\n",
       "   'Power',\n",
       "   'f',\n",
       "   'UAV',\n",
       "   'platform',\n",
       "   '2',\n",
       "   'UAV',\n",
       "   'platform',\n",
       "   '1',\n",
       "   'Computing',\n",
       "   'Computing',\n",
       "   'link',\n",
       "   'Backhaul',\n",
       "   'Computing',\n",
       "   'Computing',\n",
       "   'Computing',\n",
       "   'Cloud',\n",
       "   'networks',\n",
       "   'Central',\n",
       "   'network',\n",
       "   'AP',\n",
       "   'link',\n",
       "   'AP',\n",
       "   'Backhaul',\n",
       "   'UAV-NOMA-MEC'],\n",
       "  'image_file': '2101.11681v1-Figure1-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Artificial Intelligence Driven UAV-NOMA-MEC in Next Generation Wireless Networks',\n",
       "  'abstract': 'Driven by the unprecedented high throughput and low latency requirements in next generation wireless networks, this paper introduces an artificial intelligence (AI) enabled framework in which unmanned aerial vehicles (UAVs) use non-orthogonal multiple access (NOMA) and mobile edge computing (MEC) techniques to service terrestrial mobile users (MUs). The proposed framework enables the terrestrial MUs to offload their computational tasks simultaneously, intelligently, and flexibly, thus enhancing their connectivity as well as reducing their transmission latency and their energy consumption. To this end, the fundamentals of this framework are first introduced. Then, a number of communication and AI techniques are proposed to improve the quality of experiences of terrestrial MUs. To this end, federated learning and reinforcement learning are introduced for intelligent task offloading and computing resources allocation. For each learning technique, motivations, challenges, and representative results are introduced. Finally, several key technical challenges and open research issues of the proposed framework are summarized.',\n",
       "  'paddleOCR': [[[[298.0, 29.0], [433.0, 29.0], [433.0, 58.0], [298.0, 58.0]],\n",
       "    ['Wireless', 0.9995099306106567]],\n",
       "   [[[663.0, 51.0], [833.0, 51.0], [833.0, 81.0], [663.0, 81.0]],\n",
       "    ['Social data.', 0.9479636549949646]],\n",
       "   [[[1058.0, 53.0], [1226.0, 53.0], [1226.0, 82.0], [1058.0, 82.0]],\n",
       "    ['Cloud data', 0.9990056157112122]],\n",
       "   [[[327.0, 72.0], [400.0, 75.0], [399.0, 107.0], [325.0, 104.0]],\n",
       "    ['data', 0.9996942281723022]],\n",
       "   [[[10.0, 92.0], [169.0, 92.0], [169.0, 111.0], [10.0, 111.0]],\n",
       "    ['Data collection.', 0.9459309577941895]],\n",
       "   [[[306.0, 188.0], [471.0, 188.0], [471.0, 211.0], [306.0, 211.0]],\n",
       "    ['UAV platform ', 0.9962643980979919]],\n",
       "   [[[688.0, 257.0], [818.0, 257.0], [818.0, 281.0], [688.0, 281.0]],\n",
       "    ['Computing', 0.9998871684074402]],\n",
       "   [[[1169.0, 260.0], [1237.0, 260.0], [1237.0, 281.0], [1169.0, 281.0]],\n",
       "    ['Cloud', 0.9996079206466675]],\n",
       "   [[[986.0, 282.0], [1067.0, 282.0], [1067.0, 301.0], [986.0, 301.0]],\n",
       "    ['Backhaul', 0.9995312690734863]],\n",
       "   [[[288.0, 292.0], [461.0, 292.0], [461.0, 315.0], [288.0, 315.0]],\n",
       "    ['UAV platform 2', 0.9902178645133972]],\n",
       "   [[[944.0, 294.0], [967.0, 294.0], [967.0, 306.0], [944.0, 306.0]],\n",
       "    ['AP', 0.9945194125175476]],\n",
       "   [[[1167.0, 290.0], [1271.0, 290.0], [1271.0, 314.0], [1167.0, 314.0]],\n",
       "    ['networks', 0.9998534917831421]],\n",
       "   [[[1008.0, 306.0], [1046.0, 306.0], [1046.0, 326.0], [1008.0, 326.0]],\n",
       "    ['link', 0.9987127780914307]],\n",
       "   [[[932.0, 366.0], [1028.0, 371.0], [1027.0, 392.0], [931.0, 387.0]],\n",
       "    ['Computing', 0.9986280798912048]],\n",
       "   [[[598.0, 400.0], [784.0, 400.0], [784.0, 419.0], [598.0, 419.0]],\n",
       "    ['Computational tasks', 0.9996421337127686]],\n",
       "   [[[639.0, 432.0], [774.0, 432.0], [774.0, 456.0], [639.0, 456.0]],\n",
       "    ['NOMA uplink', 0.9998989105224609]],\n",
       "   [[[1169.0, 436.0], [1253.0, 436.0], [1253.0, 461.0], [1169.0, 461.0]],\n",
       "    ['Central', 0.9988552927970886]],\n",
       "   [[[11.0, 456.0], [157.0, 456.0], [157.0, 475.0], [11.0, 475.0]],\n",
       "    ['UAV-NOMA-', 0.9941632747650146]],\n",
       "   [[[325.0, 454.0], [452.0, 457.0], [451.0, 486.0], [324.0, 483.0]],\n",
       "    ['Computing', 0.9998939037322998]],\n",
       "   [[[611.0, 461.0], [809.0, 461.0], [809.0, 481.0], [611.0, 481.0]],\n",
       "    ['Successive interference', 0.9945328831672668]],\n",
       "   [[[1159.0, 472.0], [1260.0, 469.0], [1261.0, 494.0], [1159.0, 498.0]],\n",
       "    ['network', 0.9996888041496277]],\n",
       "   [[[2.0, 482.0], [169.0, 488.0], [169.0, 511.0], [1.0, 505.0]],\n",
       "    ['MEC structure', 0.9998252391815186]],\n",
       "   [[[608.0, 486.0], [769.0, 486.0], [769.0, 506.0], [608.0, 506.0]],\n",
       "    ['cancellation (SIC)', 0.997290313243866]],\n",
       "   [[[924.0, 531.0], [949.0, 531.0], [949.0, 547.0], [924.0, 547.0]],\n",
       "    ['AP', 0.9971478581428528]],\n",
       "   [[[998.0, 532.0], [1088.0, 532.0], [1088.0, 551.0], [998.0, 551.0]],\n",
       "    ['Backhaul', 0.9990684390068054]],\n",
       "   [[[628.0, 544.0], [702.0, 548.0], [701.0, 567.0], [627.0, 564.0]],\n",
       "    ['Power', 0.992260754108429]],\n",
       "   [[[369.0, 557.0], [626.0, 557.0], [626.0, 581.0], [369.0, 581.0]],\n",
       "    ['Task computational results', 0.9970541596412659]],\n",
       "   [[[1020.0, 556.0], [1057.0, 556.0], [1057.0, 576.0], [1020.0, 576.0]],\n",
       "    ['link', 0.999327540397644]],\n",
       "   [[[414.0, 592.0], [572.0, 592.0], [572.0, 611.0], [414.0, 611.0]],\n",
       "    ['NOMA downlink', 0.9991596937179565]],\n",
       "   [[[912.0, 610.0], [1008.0, 613.0], [1007.0, 632.0], [911.0, 629.0]],\n",
       "    ['Computing', 0.9820530414581299]],\n",
       "   [[[382.0, 621.0], [604.0, 621.0], [604.0, 640.0], [382.0, 640.0]],\n",
       "    ['Superposition coding (SC)', 0.9847087860107422]],\n",
       "   [[[1150.0, 625.0], [1274.0, 628.0], [1273.0, 653.0], [1149.0, 650.0]],\n",
       "    ['Computing', 0.997546911239624]],\n",
       "   [[[516.0, 688.0], [719.0, 688.0], [719.0, 707.0], [516.0, 707.0]],\n",
       "    ['UAV-NOMA-MEC', 0.9979515671730042]],\n",
       "   [[[337.0, 772.0], [398.0, 777.0], [395.0, 802.0], [335.0, 797.0]],\n",
       "    ['Deep', 0.9989379644393921]],\n",
       "   [[[22.0, 797.0], [154.0, 797.0], [154.0, 821.0], [22.0, 821.0]],\n",
       "    ['AI solutions', 0.972282886505127]],\n",
       "   [[[1092.0, 790.0], [1201.0, 790.0], [1201.0, 814.0], [1092.0, 814.0]],\n",
       "    ['Federated', 0.9996294379234314]],\n",
       "   [[[290.0, 804.0], [445.0, 806.0], [444.0, 829.0], [289.0, 828.0]],\n",
       "    ['reinforcement', 0.9994297623634338]],\n",
       "   [[[677.0, 801.0], [826.0, 804.0], [826.0, 829.0], [676.0, 826.0]],\n",
       "    ['Deep learning', 0.9991951584815979]],\n",
       "   [[[1102.0, 819.0], [1191.0, 824.0], [1190.0, 848.0], [1101.0, 843.0]],\n",
       "    ['learning', 0.9899517297744751]],\n",
       "   [[[322.0, 835.0], [413.0, 838.0], [412.0, 862.0], [321.0, 858.0]],\n",
       "    ['learning', 0.9847844839096069]]],\n",
       "  'ocr': [[[414.0, 592.0], [572.0, 592.0], [572.0, 611.0], [414.0, 611.0]],\n",
       "   ['NOMA downlink', 0.9991596937179565]]},\n",
       " '2107.04236v1-Figure1-1.png': {'caption': 'Figure 1. Mixed-signal neuromorphic circuits. (a) Major computations involved in every layer of a mixed-signal implementation of a modern neuromorphic classifier. Highlighted in gray (white) are those typically implemented in the digital (analog) domain (DAC: digital to analog converter, VMM: vector-by-matrix multiplier, ADC: analog-to-digital converter, BN: batch normalization, Act: activation function, Pool: an optional pooling layer, FC: fullyconnected classifier). VMM implementation using (b) memristive crossbars and (c) gatecouple eFlash memory arrays. Network weights are encoded into two the conductance of memristors (synapses) or the ratio of the state currents of two eFlash devices to a peripheral eFlash memory (see the method section for the mapping functions). The input/outputs are often encoded as voltages (ùëâùëñ) in memristive circuits and currents (ùêºùëñ) in eFlash VMMs. (d) Various',\n",
       "  'imageText': [],\n",
       "  'image_file': '2107.04236v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Experimental Measurements',\n",
       "    'text': 'Fig. 2a shows the scanning electron microscope image of the fabricated crossbar that includes 4096 TiO2 memristors-see the Method section for a discussion on the fabrication process and relevant details on electroforming, tuning, and operation procedures. Fig. 2b shows the measured IV characteristics of 350 randomly selected devices in the non-disturbing low-voltage regime. Upon the application of a voltage in this regime (<0.5 V), the conductance (state) of crosspoint devices remains unchanged at a fixed voltage. However, due to the tunneling or thermionic emission charge transport mechanism, the devices become more conductive in higher voltages and hence nonlinear. Fig. 2c shows the average relative static nonlinearity error versus applied voltage for various conductance states. Fig. 2d shows the measurement results for the relative changes of conductance in 350 devices concerning variations in the die temperature (25-100 ¬∞C). The device conductance has proportional to absolute temperature and complementary to absolute temperature dependency in low conductive and high conductive states, respectively, due to the insulator-metal phase transition. In the case of our memristive devices, such transition occurs at ~70 ¬µS, on average (Fig. 2d). We observe large errors, particularly in low conductive states, which could severely degrade the computational accuracy of mixed-signal models at elevated temperatures.\\nThe switching characteristics of memristors determine how precisely we can adjust their conductances. Individually, we can tune a device with high accuracy, e.g., <1% relative error regardless of its initial conductance. The experimental results in Fig. 2e corroborate this observation on 50 randomly selected devices tuned to 1.7 ¬µs, 50 ¬µs, and 10 ¬µs conductances consecutively. For each device, the accuracy is achieved in less than 100 pulses using a na√Øve write-verify algorithm. However, tuning dynamics are more complicated at the crossbar level since the half-select problem imposes disturbance on already tuned 0T1R memristors. Using additional gate lines in active crossbars (with dedicated in-cell selectors) solves this problem at the cost of at least two orders of magnitude increase in the cell size. Fig. 2f shows an example of the ultimate relative tuning error distribution after the entire 64√ó64 crossbar is programmed to the states that correspond to the grayscale quantized Einstein image [37]. The final tuning error distribution depends on the switching threshold distributions and the tuning algorithm.\\nTo investigate the impact of long-term retention loss, we perform accelerated retention tests and use the Arrhenius equation for room temperature projection of the results. Fig. 2g shows the extremely stable analog-grade operation of 30 devices tuned in various states, subjected to 100 ¬∞C baking for >25 hours -translating into >14 years of room temperature operation assuming 1.1 eV activation energy [40]. Fig. 2h shows the distribution of relative retention loss error for 400 memristors after 14 years of projected room temperature operation. More details of the statistical analysis of data for different states are provided in Supplementary Figure 1. Interestingly, unlike binary memristors [47], the distribution of retention loss error is relatively symmetrical in midrange analog states, i.e., the devices could move toward higher or lower conductive states. Note that we also observe unidirectional retention loss in very high (shifting toward low conductive states) or low (shifting toward high conductive states) conductance states, but we generally avoid switching the devices to extreme values. Finally, Fig. 2i shows the corresponding standard deviation of the relative conductance change versus time binned to different states for these devices. The measured data show that the relative shift in conductance for most devices is expected to be <2% after several years of operation, which is adequately high for the practical implementation of ex-situ trained DNNs.\\nFig. 3a shows the scanning electron microscope image of the fabricated redesigned eFlash memory array-see the Method section for a discussion on tuning and operation procedure. First, we measure the average static input/output characteristics of 200 synapses in the gate-coupled structure (peripheral devices are tuned to the maximum state current, max =30 nA) and find the relative static nonlinearity error, which originates from the voltage-dependent capacitive coupling. Fig. 3b-c shows the static nonlinearity measurement results for multiple synaptic weights. The temperature dependency of state current is also measured and demonstrated in Fig. 3d for 100 eFlash cells tuned to various states. The corresponding relative weight error in the gate-couple structure is also provided in Fig. 3e, indicating significant errors in high temperatures, which could significantly impact the accuracy of neural circuits. The retention characteristics of 100 eFlash memories are measured at 100¬∞C. The measurements are performed by tuning the devices to different states within the relevant dynamic range. Fig. 3f shows the stable operation of 25 devices at 100¬∞C for >6 hours. Regardless of the initial state, we confirm that the relative state change for most devices is comparable with the noise floor of the measurement setup. This superior performance partially stems from much effort spent on optimizing the technology for industrial-grade applications. Finally, in Fig. 3h, the high precision tuning capability of eFlash memories is shown for 50 devices by tuning them with 1% targeted accuracy to 100 nA, 50 nA, 30 nA, and 15 nA, consecutively, each using less than 50 pulses.\\nThe initial assessment of the experimental data indicates that the analog retention is promising in both devices; however, they are prone to variations in temperature that result in significant shifts in synaptic weights. Noise and static nonlinearity are fundamental bottlenecks in most analog systems, and neural circuits are no exception. In both eFlash-and memristor-based neuromorphic systems, we need to optimize the circuit with respect to noise and static nonlinearity. For redesigned eFlash cells, high precision tuning is obtained due to the redesigned memory cell [12], and excellent yield [39] is deemed due to the maturity of the technology. However, for passive memristors, the halfselect disturbance bounds the weight tuning accuracy in neuromorphic circuits built with practically viable kernel sizes, and limited percent-scale yield is a major hindrance. These identified imperfections are then modeled to study their deleterious effect in massive neuromorphic networks simulated in the PyTorch environment.',\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 17},\n",
       "   {'heading': 'Supplementary Information',\n",
       "    'text': \"Supplementary Figure 1. Extended measurement results of accelerated retention test in memristive devices. Panels (a-f) show the cumulative normalized frequency of relative retention loss error among 400 devices tuned to various states. Accelerated retention tests are performed at 100¬∞C at 0.1 V for more than >25 hours. The results are then projected to room temperature using the Arrhenius equation and 1.1 eV activation energy. The insets show the histogram of the error for the case of 14 years. Our results indicate that the retention loss is a bidirectional process for most devices and analog intermediate states, particularly midrange conductances. Note that moving towards high conductive states (e.g., panel (f)), we observe a trend that corroborates devices' tendency to move toward midrange conductances. In fact, we expect and observe a unilateral retention loss behavior upon hard switching devices to the extreme regimes (<5 ¬µS and >150 ¬µS). Nevertheless, the bilateral trend of retention loss of analog states is a positive feature since the tiny retention-induced errors average becomes even smaller when they average out in large matrix multiplier kernels. Supplementary Figure 5. Dynamic model and tuning precision study. The tuning precision in analog memories with selectors, e.g., eFlash, is typically determined by how precisely we can adjust the state of individual devices, which is a function of their switching characteristics, retention, tuning algorithm, etc. However, in passive memories, the half-select problem changes the dynamics of tuning and create a large tail of devices with tuning errors higher than the target error, in particular when the variations are high, or the crossbar size is large. To properly simulate this issue, we develop a dynamic model that predicts our memristors' dynamic behavior and emulates the tuning and exsitu weight transfer processes. The model predicts the change in a device's conductance as a function of its initial state and the pulse amplitude (the duration is fixed at 2 ms to simplify the model). We use a trust-region algorithm for nonlinear least-squares to fit experimental data points from 500 devices to\\n, where is the initial conductance, and Œ≥ i are fitting parameters, and Œ± is a device-unique multiplicative factor that models the variations in the switching thresholds. Panels (a) and (b) show the modeling results for the average set and reset operations, respectively. The model parameters closely reproduce the measurement results. The inset tables show the corresponding goodness of the fit and model parameters. Panels (c-d) show the set/reset characteristics for 100 devices with 10% normalized variations. The inset shows the corresponding distribution of Œ±. In order to emulate the tuning process of a crossbar (ex-situ mapping of weights to the conductance of memristors, while considering the tuning error), we randomly initialize the conductances of devices using a Gaussian distribution with an average of 36.25 ŒºS (midrange conductance) and a standard deviation of 9 ŒºS. This is to assume prior to beginning the tuning process; the devices are  \",\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Mitigating Imperfections in Mixed-Signal Neuromorphic Circuits',\n",
       "  'abstract': 'The progress in neuromorphic computing is fueled by the development of novel nonvolatile memories capable of storing analog information and implementing neural computation efficiently. However, like most other analog circuits, these devices and circuits are prone to imperfections, such as temperature dependency, noise, tuning error, etc., often leading to considerable performance degradation in neural network implementations. Indeed, imperfections are major obstacles in the path of further progress and ultimate commercialization of these technologies. Hence, a practically viable approach should be developed to deal with these nonidealities and unleash the full potential of nonvolatile memories in neuromorphic systems. Here, for the first time, we report a comprehensive characterization of critical imperfections in two analog-grade memories, namely passively-integrated memristors and redesigned eFlash memories, which both feature long-term retention, high endurance, analog storage, low-power operation, and compact nano-scale footprint. Then, we propose a holistic approach that includes modifications in the training, tuning algorithm, memory state optimization, and circuit design to mitigate these imperfections. Our proposed methodology is corroborated on a hybrid software/experimental framework using two benchmarks: a moderate-size convolutional neural network and ResNet-18 trained on CIFAR-10 and ImageNet datasets, respectively. Our proposed approaches allow 2.5√ó to 9√ó improvements in the energy consumption of memory arrays during inference and sub-percent accuracy drop across 25-100 ¬∞C temperature range. The defect tolerance is improved by >100√ó, and a sub-percent accuracy drop is demonstrated in deep neural networks built with 64√ó64 passive memristive crossbars featuring 25% normalized switching threshold variations. We believe that our results substantially improve the inference accuracy and efficiency of neuromorphic systems and paves the way towards building reliable large-scale integrated neuromorphic circuits.',\n",
       "  'paddleOCR': [[[[59.0, 33.0], [146.0, 33.0], [146.0, 101.0], [59.0, 101.0]],\n",
       "    ['(a)', 0.9984073638916016]],\n",
       "   [[[530.0, 54.0], [664.0, 54.0], [664.0, 104.0], [530.0, 104.0]],\n",
       "    ['VMM', 0.9948434233665466]],\n",
       "   [[[1105.0, 52.0], [1180.0, 52.0], [1180.0, 97.0], [1105.0, 97.0]],\n",
       "    ['BN', 0.9975104331970215]],\n",
       "   [[[1345.0, 50.0], [1433.0, 50.0], [1433.0, 92.0], [1345.0, 92.0]],\n",
       "    ['Act', 0.9968065619468689]],\n",
       "   [[[1621.0, 54.0], [1722.0, 54.0], [1722.0, 99.0], [1621.0, 99.0]],\n",
       "    ['Pool', 0.9973361492156982]],\n",
       "   [[[259.0, 101.0], [368.0, 101.0], [368.0, 146.0], [259.0, 146.0]],\n",
       "    ['DAC', 0.9985373020172119]],\n",
       "   [[[818.0, 106.0], [919.0, 106.0], [919.0, 142.0], [818.0, 142.0]],\n",
       "    ['ADC', 0.9973110556602478]],\n",
       "   [[[867.0, 403.0], [980.0, 403.0], [980.0, 439.0], [867.0, 439.0]],\n",
       "    ['Active', 0.9998996257781982]],\n",
       "   [[[865.0, 446.0], [990.0, 446.0], [990.0, 486.0], [865.0, 486.0]],\n",
       "    ['\"1T1R\"', 0.9909140467643738]],\n",
       "   [[[1656.0, 665.0], [1795.0, 665.0], [1795.0, 691.0], [1656.0, 691.0]],\n",
       "    ['amplitude', 0.9995282292366028]],\n",
       "   [[[1663.0, 701.0], [1788.0, 696.0], [1789.0, 726.0], [1664.0, 732.0]],\n",
       "    ['encoded', 0.9990419745445251]],\n",
       "   [[[1166.0, 734.0], [1258.0, 734.0], [1258.0, 757.0], [1166.0, 757.0]],\n",
       "    ['curren', 0.9964421391487122]],\n",
       "   [[[1628.0, 738.0], [1824.0, 738.0], [1824.0, 769.0], [1628.0, 769.0]],\n",
       "    ['analog output', 0.994678795337677]],\n",
       "   [[[754.0, 748.0], [928.0, 748.0], [928.0, 788.0], [754.0, 788.0]],\n",
       "    ['amplitude', 0.9935669302940369]],\n",
       "   [[[1147.0, 769.0], [1289.0, 769.0], [1289.0, 795.0], [1147.0, 795.0]],\n",
       "    ['amplitude', 0.9997034072875977]],\n",
       "   [[[763.0, 796.0], [916.0, 790.0], [917.0, 830.0], [764.0, 836.0]],\n",
       "    ['encoded', 0.9997369647026062]],\n",
       "   [[[1162.0, 805.0], [1277.0, 805.0], [1277.0, 830.0], [1162.0, 830.0]],\n",
       "    ['encoded', 0.9992796182632446]],\n",
       "   [[[112.0, 830.0], [238.0, 836.0], [237.0, 869.0], [110.0, 863.0]],\n",
       "    ['voltage', 0.999676525592804]],\n",
       "   [[[719.0, 837.0], [964.0, 843.0], [963.0, 883.0], [718.0, 877.0]],\n",
       "    ['analog output', 0.999943733215332]],\n",
       "   [[[1133.0, 842.0], [1303.0, 842.0], [1303.0, 868.0], [1133.0, 868.0]],\n",
       "    ['analog input', 0.9996775984764099]],\n",
       "   [[[84.0, 883.0], [259.0, 877.0], [260.0, 910.0], [85.0, 916.0]],\n",
       "    ['amplitude', 0.9998418092727661]],\n",
       "   [[[1279.0, 878.0], [1324.0, 878.0], [1324.0, 908.0], [1279.0, 908.0]],\n",
       "    ['Iin', 0.8834084868431091]],\n",
       "   [[[1725.0, 885.0], [1774.0, 885.0], [1774.0, 901.0], [1725.0, 901.0]],\n",
       "    ['W', 0.9060150980949402]],\n",
       "   [[[99.0, 928.0], [242.0, 922.0], [243.0, 955.0], [100.0, 961.0]],\n",
       "    ['encoded', 0.9997521638870239]],\n",
       "   [[[1682.0, 930.0], [1711.0, 930.0], [1711.0, 948.0], [1682.0, 948.0]],\n",
       "    ['i=1', 0.9511497616767883]],\n",
       "   [[[353.0, 946.0], [462.0, 946.0], [462.0, 989.0], [353.0, 989.0]],\n",
       "    ['s ~0', 0.6635028719902039]],\n",
       "   [[[61.0, 973.0], [282.0, 965.0], [283.0, 1005.0], [62.0, 1013.0]],\n",
       "    ['analog input', 0.9997435212135315]],\n",
       "   [[[813.0, 984.0], [881.0, 984.0], [881.0, 1022.0], [813.0, 1022.0]],\n",
       "    ['G,V', 0.8692821860313416]],\n",
       "   [[[1236.0, 976.0], [1324.0, 966.0], [1328.0, 999.0], [1239.0, 1008.0]],\n",
       "    ['Vth periph', 0.9427603483200073]],\n",
       "   [[[104.0, 1142.0], [862.0, 1144.0], [862.0, 1184.0], [104.0, 1182.0]],\n",
       "    ['Ex-situ Training with Redundant Networks', 0.9999091029167175]],\n",
       "   [[[1079.0, 1149.0], [1357.0, 1151.0], [1357.0, 1192.0], [1079.0, 1189.0]],\n",
       "    ['In-situ Training', 0.9998856782913208]],\n",
       "   [[[74.0, 1219.0], [194.0, 1225.0], [192.0, 1258.0], [73.0, 1252.0]],\n",
       "    ['Training', 0.9996946454048157]],\n",
       "   [[[972.0, 1231.0], [1091.0, 1237.0], [1090.0, 1270.0], [970.0, 1264.0]],\n",
       "    ['Training', 0.9997106790542603]],\n",
       "   [[[832.0, 1281.0], [905.0, 1281.0], [905.0, 1307.0], [832.0, 1307.0]],\n",
       "    ['Bird', 0.9927852749824524]],\n",
       "   [[[1468.0, 1335.0], [1861.0, 1335.0], [1861.0, 1368.0], [1468.0, 1368.0]],\n",
       "    ['A GPU cluster Mixed-signal', 0.998130738735199]],\n",
       "   [[[422.0, 1354.0], [537.0, 1354.0], [537.0, 1380.0], [422.0, 1380.0]],\n",
       "    ['Transfer', 0.9996116757392883]],\n",
       "   [[[1688.0, 1366.0], [1862.0, 1371.0], [1861.0, 1404.0], [1687.0, 1399.0]],\n",
       "    ['edge device', 0.9999369978904724]],\n",
       "   [[[418.0, 1382.0], [540.0, 1388.0], [538.0, 1423.0], [417.0, 1418.0]],\n",
       "    ['Weights', 0.9996241927146912]],\n",
       "   [[[1138.0, 1444.0], [1678.0, 1449.0], [1677.0, 1491.0], [1138.0, 1486.0]],\n",
       "    ['Hybrid In-situ/Ex-situ Training', 0.9997778534889221]],\n",
       "   [[[240.0, 1460.0], [700.0, 1463.0], [700.0, 1496.0], [240.0, 1493.0]],\n",
       "    ['Chip-in-the-Loop Training', 0.9994544982910156]],\n",
       "   [[[309.0, 1517.0], [686.0, 1520.0], [686.0, 1553.0], [309.0, 1550.0]],\n",
       "    ['Transfer chip specification', 0.9984090328216553]],\n",
       "   [[[1197.0, 1524.0], [1664.0, 1524.0], [1664.0, 1555.0], [1197.0, 1555.0]],\n",
       "    ['Partial Training/in-situ calibration', 0.9998233318328857]],\n",
       "   [[[75.0, 1560.0], [196.0, 1560.0], [196.0, 1593.0], [75.0, 1593.0]],\n",
       "    ['Training', 0.9997414946556091]],\n",
       "   [[[946.0, 1557.0], [1063.0, 1562.0], [1062.0, 1595.0], [944.0, 1590.0]],\n",
       "    ['Training', 0.9980233907699585]],\n",
       "   [[[832.0, 1614.0], [900.0, 1614.0], [900.0, 1640.0], [832.0, 1640.0]],\n",
       "    ['Bird', 0.9957980513572693]],\n",
       "   [[[368.0, 1684.0], [608.0, 1687.0], [608.0, 1720.0], [367.0, 1718.0]],\n",
       "    ['Transfer Weights', 0.9998123049736023]],\n",
       "   [[[1180.0, 1675.0], [1421.0, 1675.0], [1421.0, 1706.0], [1180.0, 1706.0]],\n",
       "    ['Transfer Weights', 0.9946330785751343]],\n",
       "   [[[1793.0, 1678.0], [1854.0, 1678.0], [1854.0, 1703.0], [1793.0, 1703.0]],\n",
       "    ['Bird', 0.9970962405204773]],\n",
       "   [[[341.0, 1766.0], [481.0, 1775.0], [479.0, 1817.0], [338.0, 1809.0]],\n",
       "    ['IR drop', 0.9313154220581055]],\n",
       "   [[[672.0, 1762.0], [1258.0, 1765.0], [1258.0, 1800.0], [671.0, 1798.0]],\n",
       "    ['Hardware-Aware Ex-situ Training.', 0.992322564125061]],\n",
       "   [[[123.0, 1819.0], [262.0, 1824.0], [261.0, 1857.0], [122.0, 1852.0]],\n",
       "    ['Modeling', 0.9939806461334229]],\n",
       "   [[[573.0, 1819.0], [749.0, 1819.0], [749.0, 1859.0], [573.0, 1859.0]],\n",
       "    ['Retention', 0.9998444318771362]],\n",
       "   [[[94.0, 1859.0], [295.0, 1859.0], [295.0, 1890.0], [94.0, 1890.0]],\n",
       "    ['Imperfections', 0.9997621774673462]],\n",
       "   [[[181.0, 1890.0], [276.0, 1890.0], [276.0, 1923.0], [181.0, 1923.0]],\n",
       "    ['Yield', 0.9997245669364929]],\n",
       "   [[[912.0, 1911.0], [1148.0, 1914.0], [1147.0, 1947.0], [912.0, 1944.0]],\n",
       "    ['Hardware-aware', 0.9998213648796082]],\n",
       "   [[[575.0, 1939.0], [841.0, 1939.0], [841.0, 1972.0], [575.0, 1972.0]],\n",
       "    ['D2D Variations', 0.9986987113952637]],\n",
       "   [[[970.0, 1944.0], [1091.0, 1949.0], [1090.0, 1985.0], [968.0, 1979.0]],\n",
       "    ['Training', 0.9998476505279541]],\n",
       "   [[[1774.0, 2015.0], [1845.0, 2015.0], [1845.0, 2041.0], [1774.0, 2041.0]],\n",
       "    ['Bird', 0.9992132782936096]],\n",
       "   [[[153.0, 2031.0], [309.0, 2031.0], [309.0, 2064.0], [153.0, 2064.0]],\n",
       "    ['Linearity', 0.9998096823692322]],\n",
       "   [[[549.0, 2067.0], [773.0, 2067.0], [773.0, 2100.0], [549.0, 2100.0]],\n",
       "    ['Temperature', 0.9999426007270813]],\n",
       "   [[[1282.0, 2112.0], [1520.0, 2112.0], [1520.0, 2142.0], [1282.0, 2142.0]],\n",
       "    ['Transfer Weights', 0.9900277256965637]],\n",
       "   [[[796.0, 2152.0], [1119.0, 2152.0], [1119.0, 2182.0], [796.0, 2182.0]],\n",
       "    ['Embedding the models', 0.9971759915351868]],\n",
       "   [[[822.0, 2187.0], [1098.0, 2187.0], [1098.0, 2218.0], [822.0, 2218.0]],\n",
       "    ['in the forward pass', 0.998211681842804]],\n",
       "   [[[384.0, 2197.0], [488.0, 2197.0], [488.0, 2230.0], [384.0, 2230.0]],\n",
       "    ['Noise', 0.9998357892036438]]],\n",
       "  'ocr': [[[832.0, 1614.0], [900.0, 1614.0], [900.0, 1640.0], [832.0, 1640.0]],\n",
       "   ['Bird', 0.9957980513572693]]},\n",
       " '2102.11165v1-Figure2-1.png': {'caption': 'Figure 2: (Left) The model architecture of Graph Deviation Networks (GDN) for network anomaly detection with limited labeled data. (Right) The illustration of the overall framework Meta-GDN. Meta-GDN is trained across multiple auxiliary networks and can be well adapted to the target network with few-shot labeled data. Figure best viewed in color.',\n",
       "  'imageText': ['GDN',\n",
       "   '\"\\'&\"(&',\n",
       "   '\"',\n",
       "   '‚Ñí%!',\n",
       "   '‚Ñí%\"‚Ñí%#',\n",
       "   '\"!',\n",
       "   '&',\n",
       "   'Cross-network',\n",
       "   'Meta-learning',\n",
       "   'Unlabeled',\n",
       "   'Anomaly',\n",
       "   'Normal',\n",
       "   'Node',\n",
       "   'Labeled',\n",
       "   'Anomaly',\n",
       "   'Target',\n",
       "   'Network',\n",
       "   'Auxiliary',\n",
       "   'Networks',\n",
       "   'Train',\n",
       "   'Train',\n",
       "   'Train',\n",
       "   '!$',\n",
       "   'Fine-tune',\n",
       "   '!#\"',\n",
       "   '!!\"',\n",
       "   '‚Ä¶',\n",
       "   '`',\n",
       "   '`',\n",
       "   'ùíõ\"',\n",
       "   '`',\n",
       "   'Node',\n",
       "   'Representation',\n",
       "   'ùëü#,',\n",
       "   'ùëü$,',\n",
       "   '‚Ä¶',\n",
       "   ',',\n",
       "   'ùëü%',\n",
       "   '~',\n",
       "   'ùêπ',\n",
       "   'ùë†\"',\n",
       "   'Anomaly',\n",
       "   'Score',\n",
       "   'ùúá!,',\n",
       "   'ùúé!',\n",
       "   'Reference',\n",
       "   'Scores',\n",
       "   'Deviation',\n",
       "   'Loss',\n",
       "   'Prior',\n",
       "   'Distribution',\n",
       "   'ùëìùúΩ!',\n",
       "   'ùëìùúΩ\"',\n",
       "   'Network',\n",
       "   'Encoder',\n",
       "   'Abnormality',\n",
       "   'Valuator',\n",
       "   'Input',\n",
       "   'Network'],\n",
       "  'image_file': '2102.11165v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'PROPOSED APPROACH',\n",
       "    'text': 'In this section, we introduce the details of the proposed framework -Meta-GDN for few-shot network anomaly detection. Specifically, Meta-GDN addresses the discussed challenges with the following two key contributions: (1) Graph Deviation Networks (GDN), a new family of graph neural networks that enable anomaly detection on an arbitrary individual network with limited labeled data; and (2) a cross-network meta-learning algorithm, which empowers GDN to transfer meta-knowledge across multiple auxiliary networks to enable few-shot anomaly detection on the target network. An overview of the proposed Meta-GDN is provided in Figure 2.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Few-shot Network Anomaly Detection via Cross-network Meta-learning',\n",
       "  'abstract': 'Network anomaly detection aims to find network elements (e.g., nodes, edges, subgraphs) with significantly different behaviors from the vast majority. It has a profound impact in a variety of applications ranging from finance, healthcare to social network analysis. Due to the unbearable labeling cost, existing methods are predominately developed in an unsupervised manner. Nonetheless, the anomalies they identify may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies of interest. Hence, it is critical to investigate and develop few-shot learning for network anomaly detection. In real-world scenarios, few labeled anomalies are also easy to be accessed on similar networks from the same domain as of the target network, while most of the existing works omit to leverage them and merely focus on a single network. Taking advantage of this potential, in this work, we tackle the problem of few-shot network anomaly detection by ( 1) proposing a new family of graph neural networks -Graph Deviation Networks (GDN) that can leverage a small number of labeled anomalies for enforcing statistically significant deviations between abnormal and normal nodes on a network; (2) equipping the proposed GDN with a new cross-network meta-learning algorithm to realize few-shot network anomaly detection by transferring metaknowledge from multiple auxiliary networks. Extensive evaluations demonstrate the efficacy of the proposed approach on few-shot or even one-shot network anomaly detection.',\n",
       "  'paddleOCR': [[[[190.0, 23.0], [382.0, 23.0], [382.0, 54.0], [190.0, 54.0]],\n",
       "    ['Network Encoder', 0.9999398589134216]],\n",
       "   [[[489.0, 26.0], [721.0, 26.0], [721.0, 49.0], [489.0, 49.0]],\n",
       "    ['Abnormality Valuator', 0.9999324083328247]],\n",
       "   [[[1584.0, 42.0], [1647.0, 42.0], [1647.0, 68.0], [1584.0, 68.0]],\n",
       "    ['Train', 0.9999548196792603]],\n",
       "   [[[2005.0, 33.0], [2055.0, 33.0], [2055.0, 51.0], [2005.0, 51.0]],\n",
       "    ['GDN', 0.9977172017097473]],\n",
       "   [[[428.0, 56.0], [450.0, 56.0], [450.0, 77.0], [428.0, 77.0]],\n",
       "    ['Z i', 0.7559733390808105]],\n",
       "   [[[727.0, 124.0], [765.0, 124.0], [765.0, 152.0], [727.0, 152.0]],\n",
       "    ['Si', 0.923677921295166]],\n",
       "   [[[1581.0, 142.0], [1647.0, 142.0], [1647.0, 168.0], [1581.0, 168.0]],\n",
       "    ['Train', 0.999920666217804]],\n",
       "   [[[1025.0, 156.0], [1132.0, 161.0], [1131.0, 187.0], [1024.0, 182.0]],\n",
       "    ['Auxiliary', 0.9998064041137695]],\n",
       "   [[[666.0, 184.0], [826.0, 184.0], [826.0, 208.0], [666.0, 208.0]],\n",
       "    ['Anomaly Score', 0.9998696446418762]],\n",
       "   [[[1024.0, 189.0], [1132.0, 189.0], [1132.0, 212.0], [1024.0, 212.0]],\n",
       "    ['Networks', 0.9998476505279541]],\n",
       "   [[[328.0, 224.0], [546.0, 224.0], [546.0, 247.0], [328.0, 247.0]],\n",
       "    ['Node Representation', 0.9999690651893616]],\n",
       "   [[[1868.0, 219.0], [1887.0, 219.0], [1887.0, 238.0], [1868.0, 238.0]],\n",
       "    ['9', 0.8826148509979248]],\n",
       "   [[[2024.0, 220.0], [2050.0, 227.0], [2046.0, 247.0], [2019.0, 240.0]],\n",
       "    [\"A'\", 0.7224562764167786]],\n",
       "   [[[1584.0, 252.0], [1647.0, 252.0], [1647.0, 278.0], [1584.0, 278.0]],\n",
       "    ['Train', 0.99560546875]],\n",
       "   [[[808.0, 269.0], [965.0, 269.0], [965.0, 294.0], [808.0, 294.0]],\n",
       "    ['Deviation Loss', 0.9998685121536255]],\n",
       "   [[[1715.0, 273.0], [1874.0, 273.0], [1874.0, 299.0], [1715.0, 299.0]],\n",
       "    ['Cross-network', 0.9999174475669861]],\n",
       "   [[[1715.0, 299.0], [1872.0, 301.0], [1872.0, 327.0], [1715.0, 324.0]],\n",
       "    ['Meta-learning', 0.9996764063835144]],\n",
       "   [[[501.0, 331.0], [680.0, 336.0], [679.0, 362.0], [500.0, 357.0]],\n",
       "    ['Reference Scores', 0.9764282703399658]],\n",
       "   [[[496.0, 383.0], [679.0, 380.0], [679.0, 413.0], [496.0, 416.0]],\n",
       "    ['r1,r2,...,r~ F', 0.9258953332901001]],\n",
       "   [[[1900.0, 409.0], [2055.0, 409.0], [2055.0, 425.0], [1900.0, 425.0]],\n",
       "    ['Labeled Anomaly', 0.9910602569580078]],\n",
       "   [[[1042.0, 423.0], [1118.0, 423.0], [1118.0, 448.0], [1042.0, 448.0]],\n",
       "    ['Target', 0.9999497532844543]],\n",
       "   [[[1640.0, 420.0], [1745.0, 420.0], [1745.0, 446.0], [1640.0, 446.0]],\n",
       "    ['Fine-tune', 0.9998576641082764]],\n",
       "   [[[784.0, 430.0], [861.0, 430.0], [861.0, 455.0], [784.0, 455.0]],\n",
       "    ['Hr, Or', 0.883083164691925]],\n",
       "   [[[37.0, 452.0], [99.0, 458.0], [97.0, 486.0], [34.0, 480.0]],\n",
       "    ['Input', 0.9998620748519897]],\n",
       "   [[[247.0, 453.0], [435.0, 453.0], [435.0, 476.0], [247.0, 476.0]],\n",
       "    ['Prior Distribution', 0.9999461770057678]],\n",
       "   [[[1033.0, 451.0], [1127.0, 451.0], [1127.0, 476.0], [1033.0, 476.0]],\n",
       "    ['Network', 0.9999404549598694]],\n",
       "   [[[1158.0, 450.0], [1184.0, 440.0], [1191.0, 459.0], [1165.0, 469.0]],\n",
       "    ['Gt', 0.5518133640289307]],\n",
       "   [[[1900.0, 444.0], [2075.0, 444.0], [2075.0, 467.0], [1900.0, 467.0]],\n",
       "    [' Unlabeled Anomaly', 0.9861401319503784]],\n",
       "   [[[22.0, 486.0], [116.0, 486.0], [116.0, 511.0], [22.0, 511.0]],\n",
       "    ['Network', 0.9999375939369202]],\n",
       "   [[[1903.0, 479.0], [2025.0, 479.0], [2025.0, 502.0], [1903.0, 502.0]],\n",
       "    ['Normal Node', 0.9999403357505798]]],\n",
       "  'ocr': [[[501.0, 331.0], [680.0, 336.0], [679.0, 362.0], [500.0, 357.0]],\n",
       "   ['Reference Scores', 0.9764282703399658]]},\n",
       " '2010.10382v1-Figure24-1.png': {'caption': 'Figure 24: A cross-sectional diagram of a bilayer graphene FET in the dual-gated configuration [48].',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.10382v1-Figure24-1.png',\n",
       "  'sections': [{'heading': 'Bilayer Graphene FETs',\n",
       "    'text': 'Another way to implement a graphene FET is to use a bilayer graphene channel. A crosssectional schematic of a bilayer graphene FET is shown in Fig. 24.\\nFigure 24: A cross-sectional diagram of a bilayer graphene FET in the dual-gated configuration [48].\\nAlthough bilayer graphene typically exhibits a lower mobility than monolayer graphene, the use of a bilayer graphene channel in FETs offers some advantages over monolayer graphene. In particular, bilayer graphene FETs have been shown to possess a larger intrinsic voltage gain than monolayer graphene FETs [6]. Moreover, the bandgap induced in bilayer graphene by applying a perpendicular electric field has been shown to improve current saturation and the maximum frequency of oscillation, f osc [21,56]. This is because the existence of a nonzero bandgap in bilayer graphene (upon the application of a perpendicular electric field) suppresses interband tunneling. Furthermore, bilayer graphene FETs show a leakage current that is orders of magnitude lower than that of a typical monolayer graphene FET at low temperatures [47]. Although the gap in leakage currents between the two FET devices decreases at higher temperatures, a lower leakage current is desirable in both analog and digital applications.\\nThe zero bandgap of monolayer graphene implies a small value of Œª (‚âà 5 for top-gated FETs) which is unsuitable for digital applications [6]. As stated in section 2, bandgaps as large as 130 meV have been demonstrated in bilayer graphene. For bilayer graphene FETs, this corresponds to a value of Œª ‚âà 10 2 [52]. While this is an improvement over the values of Œª observed in monolayer graphene FETs, it is not sufficient for modern applications in digital electronics, which require a minimum value of Œª on the order of 10 3 to 10 4 .',\n",
       "    'n_publication_ref': 7,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Graphene Field Effect Transistors',\n",
       "  'abstract': \"The past decade has seen rapid growth in the research area of graphene and its application to novel electronics. With Moore's law beginning to plateau, the need for post-silicon technology in industry is becoming more apparent. Moreover, existing technology is insufficient for implementing terahertz detectors and receivers, which are required for a number of applications including medical imaging and security scanning. Graphene is considered to be a key potential candidate for replacing silicon in existing CMOS technology as well as realizing field effect transistors for terahertz detection, due to its remarkable electronic properties, with observed electronic mobilities reaching up to 2 √ó 10 5 cm 2 V ‚àí1 s ‚àí1 in suspended graphene samples. This report reviews the physics and electronic properties of graphene in the context of graphene transistor implementations. Common techniques used to synthesize graphene, such as mechanical exfoliation, chemical vapor deposition, and epitaxial growth are reviewed and compared. One of the challenges associated with realizing graphene transistors is that graphene is semimetallic, with a zero bandgap, which is troublesome in the context of digital electronics applications. Thus, the report also reviews different ways of opening a bandgap in graphene by using bilayer graphene and graphene nanoribbons. The basic operation of a conventional field effect transistor is explained and key figures of merit used in the literature are extracted. Finally, a review of some examples of state-of-the-art graphene field effect transistors is presented, with particular focus on monolayer graphene, bilayer graphene, and graphene nanoribbons.\",\n",
       "  'paddleOCR': [[[[399.0, 33.0], [642.0, 33.0], [642.0, 67.0], [399.0, 67.0]],\n",
       "    ['TOPGATE', 0.9972725510597229]],\n",
       "   [[[339.0, 97.0], [372.0, 97.0], [372.0, 133.0], [339.0, 133.0]],\n",
       "    ['y', 0.9823833703994751]],\n",
       "   [[[61.0, 110.0], [259.0, 110.0], [259.0, 144.0], [61.0, 144.0]],\n",
       "    ['SOURCE', 0.9976939558982849]],\n",
       "   [[[700.0, 103.0], [859.0, 103.0], [859.0, 142.0], [700.0, 142.0]],\n",
       "    ['DRAIN', 0.9984620809555054]],\n",
       "   [[[377.0, 256.0], [672.0, 256.0], [672.0, 302.0], [377.0, 302.0]],\n",
       "    ['Top-gate oxide', 0.9845404028892517]],\n",
       "   [[[952.0, 283.0], [1163.0, 286.0], [1163.0, 321.0], [951.0, 318.0]],\n",
       "    ['BILAYER', 0.9976959824562073]],\n",
       "   [[[936.0, 339.0], [1187.0, 341.0], [1187.0, 377.0], [935.0, 374.0]],\n",
       "    ['GRAPHENE', 0.998104453086853]],\n",
       "   [[[717.0, 385.0], [755.0, 385.0], [755.0, 411.0], [717.0, 411.0]],\n",
       "    ['X', 0.8453938961029053]],\n",
       "   [[[341.0, 413.0], [373.0, 413.0], [373.0, 453.0], [341.0, 453.0]],\n",
       "    ['0', 0.8430883884429932]],\n",
       "   [[[670.0, 418.0], [705.0, 418.0], [705.0, 455.0], [670.0, 455.0]],\n",
       "    ['L', 0.987171471118927]],\n",
       "   [[[363.0, 500.0], [679.0, 500.0], [679.0, 539.0], [363.0, 539.0]],\n",
       "    ['Back-gate oxide', 0.9822341203689575]],\n",
       "   [[[394.0, 763.0], [676.0, 763.0], [676.0, 793.0], [394.0, 793.0]],\n",
       "    ['BACK GATE', 0.9503797888755798]]],\n",
       "  'ocr': [[[700.0, 103.0], [859.0, 103.0], [859.0, 142.0], [700.0, 142.0]],\n",
       "   ['DRAIN', 0.9984620809555054]]},\n",
       " '2102.10283v1-Figure6-1.png': {'caption': 'Fig. 6. Overview of proposed NN using CNN for normalization',\n",
       "  'imageText': ['(stride',\n",
       "   '1,',\n",
       "   '3x5x3),',\n",
       "   'relu',\n",
       "   '#$',\n",
       "   '\"',\n",
       "   '%',\n",
       "   'BN,',\n",
       "   'conv',\n",
       "   'slave',\n",
       "   '!\"',\n",
       "   '(9)',\n",
       "   'concat',\n",
       "   '(50)',\n",
       "   '(1dim)'],\n",
       "  'image_file': '2102.10283v1-Figure6-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Imitation Learning for Variable Speed Object Manipulation',\n",
       "  'abstract': 'To operate in a real-world environment, robots have several requirements including environmental adaptability. Moreover, the desired success rate for the completion of tasks must be achieved. In this regard, end-to-end learning for autonomous operation is currently being investigated. However, the issue of operating speed has not been investigated in detail. Therefore, in this paper, we propose a method for generating variable operating speeds while adapting to perturbations in the environment. When the work speed changes, there is a nonlinear relationship between the operating speed and force (e.g., inertial and frictional forces). However, the proposed method can be adapted to nonlinearities by utilizing minimal motion data. We experimentally evaluated the proposed method for erasing a line using an eraser fixed to the tip of a robot. Furthermore, the proposed method enables a robot to perform a task faster than a human operator.',\n",
       "  'paddleOCR': [[[[798.0, 0.0], [881.0, 0.0], [881.0, 30.0], [798.0, 30.0]],\n",
       "    ['CNN', 0.9922678470611572]],\n",
       "   [[[429.0, 15.0], [522.0, 15.0], [522.0, 54.0], [429.0, 54.0]],\n",
       "    ['slave', 0.9996470212936401]],\n",
       "   [[[1276.0, 9.0], [1386.0, 9.0], [1386.0, 47.0], [1276.0, 47.0]],\n",
       "    ['LSTM', 0.9985513687133789]],\n",
       "   [[[193.0, 24.0], [249.0, 24.0], [249.0, 47.0], [193.0, 47.0]],\n",
       "    ['res', 0.9999224543571472]],\n",
       "   [[[292.0, 21.0], [340.0, 21.0], [340.0, 45.0], [292.0, 45.0]],\n",
       "    ['res', 0.9998371601104736]],\n",
       "   [[[1645.0, 20.0], [1760.0, 15.0], [1762.0, 47.0], [1646.0, 52.0]],\n",
       "    ['master', 0.999898374080658]],\n",
       "   [[[747.0, 42.0], [865.0, 50.0], [863.0, 78.0], [745.0, 70.0]],\n",
       "    ['BN, conv', 0.999226450920105]],\n",
       "   [[[941.0, 56.0], [1111.0, 56.0], [1111.0, 83.0], [941.0, 83.0]],\n",
       "    ['BN, conv, relu', 0.9995624423027039]],\n",
       "   [[[684.0, 81.0], [934.0, 81.0], [934.0, 109.0], [684.0, 109.0]],\n",
       "    ['(stride 1, 3x5x3), relu', 0.9955179691314697]],\n",
       "   [[[1260.0, 75.0], [1394.0, 75.0], [1394.0, 98.0], [1260.0, 98.0]],\n",
       "    ['LSTM 2lvs', 0.9750540256500244]],\n",
       "   [[[549.0, 98.0], [644.0, 98.0], [644.0, 128.0], [549.0, 128.0]],\n",
       "    ['reshape', 0.9997186660766602]],\n",
       "   [[[1086.0, 107.0], [1162.0, 107.0], [1162.0, 128.0], [1086.0, 128.0]],\n",
       "    ['concat', 0.9971001148223877]],\n",
       "   [[[1494.0, 109.0], [1593.0, 109.0], [1593.0, 133.0], [1494.0, 133.0]],\n",
       "    ['Max-Min', 0.9996935129165649]],\n",
       "   [[[1679.0, 112.0], [1759.0, 104.0], [1762.0, 136.0], [1682.0, 144.0]],\n",
       "    ['Ares', 0.9372758269309998]],\n",
       "   [[[1858.0, 111.0], [1948.0, 111.0], [1948.0, 135.0], [1858.0, 135.0]],\n",
       "    ['sres', 0.7850862741470337]],\n",
       "   [[[1457.0, 141.0], [1631.0, 141.0], [1631.0, 163.0], [1457.0, 163.0]],\n",
       "    ['Denormalization', 0.9999228715896606]],\n",
       "   [[[1703.0, 139.0], [1734.0, 139.0], [1734.0, 156.0], [1703.0, 156.0]],\n",
       "    ['m', 0.9625560641288757]],\n",
       "   [[[1512.0, 171.0], [1575.0, 171.0], [1575.0, 195.0], [1512.0, 195.0]],\n",
       "    ['(0~1)', 0.999335765838623]],\n",
       "   [[[464.0, 208.0], [557.0, 208.0], [557.0, 238.0], [464.0, 238.0]],\n",
       "    ['(9dims)', 0.9999054074287415]],\n",
       "   [[[671.0, 208.0], [750.0, 208.0], [750.0, 240.0], [671.0, 240.0]],\n",
       "    ['(3,1,3)', 0.9996286630630493]],\n",
       "   [[[829.0, 208.0], [924.0, 208.0], [924.0, 238.0], [829.0, 238.0]],\n",
       "    ['(16,1,1)', 0.9731872081756592]],\n",
       "   [[[1013.0, 210.0], [1048.0, 210.0], [1048.0, 238.0], [1013.0, 238.0]],\n",
       "    ['(9)', 0.8397253155708313]],\n",
       "   [[[320.0, 293.0], [446.0, 298.0], [445.0, 328.0], [319.0, 323.0]],\n",
       "    ['Frequency', 0.99983811378479]],\n",
       "   [[[698.0, 306.0], [802.0, 306.0], [802.0, 336.0], [698.0, 336.0]],\n",
       "    ['Max-Min', 0.9996535181999207]],\n",
       "   [[[321.0, 330.0], [441.0, 330.0], [441.0, 353.0], [321.0, 353.0]],\n",
       "    ['Command', 0.9998748898506165]],\n",
       "   [[[468.0, 323.0], [555.0, 323.0], [555.0, 353.0], [468.0, 353.0]],\n",
       "    ['(1dim)', 0.9976597428321838]],\n",
       "   [[[1013.0, 328.0], [1053.0, 328.0], [1053.0, 353.0], [1013.0, 353.0]],\n",
       "    ['(1)', 0.9297105669975281]],\n",
       "   [[[1191.0, 325.0], [1392.0, 325.0], [1392.0, 353.0], [1191.0, 353.0]],\n",
       "    ['(10) (50) (50)', 0.9763988256454468]],\n",
       "   [[[1422.0, 328.0], [1462.0, 322.0], [1466.0, 350.0], [1427.0, 356.0]],\n",
       "    ['(9)', 0.9831711649894714]],\n",
       "   [[[675.0, 342.0], [827.0, 342.0], [827.0, 366.0], [675.0, 366.0]],\n",
       "    ['Normalization', 0.9999414086341858]],\n",
       "   [[[719.0, 368.0], [785.0, 368.0], [785.0, 400.0], [719.0, 400.0]],\n",
       "    ['(0~1)', 0.9172517657279968]]],\n",
       "  'ocr': [[[321.0, 330.0], [441.0, 330.0], [441.0, 353.0], [321.0, 353.0]],\n",
       "   ['Command', 0.9998748898506165]]},\n",
       " '2101.00124v2-Figure3-1.png': {'caption': 'Figure 3: The illustration of Hybrid Matching (HM) for the instance ‚ÄúDisease-A causes Syndrome-B. DrugC treats Syndrome-B and Syndrome-D‚Äù with target entities ‚ÄúDisease-A‚Äù, ‚ÄúDrug-C‚Äù and ‚ÄúSyndrome-D‚Äù. HM consists of two steps: SEM and NHEM. After performing HM, distances from ‚ÄúDrug-C‚Äù to ‚ÄúDisease-A‚Äù, and from ‚ÄúDrug-C‚Äù to ‚ÄúSyndrome-D‚Äù decrease by 1.',\n",
       "  'imageText': ['ùüè',\n",
       "   'ùüè√ó',\n",
       "   'ùüë',\n",
       "   'ùëö(',\n",
       "   'ùëö!',\n",
       "   \"ùëö'\",\n",
       "   'Syndrome-B',\n",
       "   'Disease-A',\n",
       "   \"ùëö'\",\n",
       "   'Syndrome-D',\n",
       "   'ùëõ&',\n",
       "   'ùëõ%',\n",
       "   'ùëõ$',\n",
       "   'ùëõ\"',\n",
       "   'ùëõ#',\n",
       "   'ùëõ!',\n",
       "   \"ùëõ'\",\n",
       "   'ùëõ(',\n",
       "   'Syndrome-B',\n",
       "   'and',\n",
       "   'Syndrome-B',\n",
       "   'treats',\n",
       "   'causes',\n",
       "   'Drug-C',\n",
       "   'Disease-A',\n",
       "   'Syndrome-D',\n",
       "   'and',\n",
       "   'Syndrome-B',\n",
       "   'treats',\n",
       "   'Drug-C',\n",
       "   'NHEM',\n",
       "   'causes',\n",
       "   'ùüè',\n",
       "   'ùüë√ó',\n",
       "   'ùüê',\n",
       "   'ùüè',\n",
       "   'ùüë√ó',\n",
       "   'ùüê',\n",
       "   'ùüè',\n",
       "   'ùüê√ó',\n",
       "   'ùüê',\n",
       "   'ùüè',\n",
       "   'ùüë√ó',\n",
       "   'ùüë',\n",
       "   'Syndrome-D',\n",
       "   'ùëõ&',\n",
       "   'ùëõ%',\n",
       "   'ùëõ$',\n",
       "   'ùëõ#',\n",
       "   'ùëõ\"',\n",
       "   'ùëõ!',\n",
       "   'Syndrome-B',\n",
       "   'and',\n",
       "   'treats',\n",
       "   'causes',\n",
       "   'Drug-C',\n",
       "   'Disease-A',\n",
       "   'Syndrome-B',\n",
       "   'SEM'],\n",
       "  'image_file': '2101.00124v2-Figure3-1.png',\n",
       "  'sections': [{'heading': 'Graph Pooling',\n",
       "    'text': 'Graph pooling iteratively coarsens graph G into a smaller but structurally similar graph G . It usually first discovers nodes that can be grouped (Ying et al., 2018;Lee et al., 2019). Then, nodes that are matched together will be merged into a supernode. We introduce Hybrid Matching (HM) and our proposed Clause Matching (CM) in this section. Hybrid Matching. Hybrid Matching (Liang et al., 2018) is shown to be effective for learning largescale graph embeddings. It performs node matching based on the connectivity between nodes and consists of structural equivalence matching (SEM) and normalized heavy edge matching (NHEM).\\nSEM merges two nodes that share the exact same neighbor. In the Fig. 3 example, node n 0 and n 2 are considered as structural equivalence, since they share the exact same neighbor node n 1 .\\nNHEM uses the adjacency matrix A to perform matching. Each node will be matched with its neighbor that has the largest normalized edge weight (NEW) and supernodes cannot be merged again. The normalized edge weight, N (u, v), for each edge in the graph is defined as:\\nN (u, v) = Auv D(u) √ó D(v) ,(1)\\nwhere D(u) is the degree of node u. The adjacency matrix A of the original input document graph (i.e, A 0 ) has cell value A 0 uv being either 1 or 0, indicating if a connection from node u to v exists or not. As for adjacency matrix for pooled graph, it can be calculated based on its parent graph (see Eq. 3).\\nIn NHEM, we visit nodes by ascending order according to the node degree following Liang et al. (2018), and we illustrate the process in Fig. 3. After computing NEW of the graph, we first visit node n 3 (degree equals 1), and merge it with its only neighbor n 4 into the supernode m 1 . Then, we visit node n 6 and merge it with n 7 that has the largest NEW with n 6 . Since supernodes cannot be merged again in each round of HM, n 1 and n 5 remain. After HM, distances among targeted entities decrease. Clause Matching. The edge attribute is important to group nodes in the document graphs. For example, \"a red apple\" can be split into three nodes in the document graph, although they are essentially one noun phrase. The dependency tree shows that \"a\" is the determiner (det) of \"apple\", and \"red\" is an adjectival modifier (amod). Such edge information should be considered in the pooling operation, but has been ignored by many general graph pooling method like HM. Therefore, we propose Clause Matching (CM), which merges tokens based on the dependency relation between them.\\nSpecifically, following De Marneffe et al., we first classify dependency relations into two categories -core arguments and others. Core arguments link predicates with their core dependents, Algorithm 1 Clause Matching\\nRequire: G = (V, E), E ‚äÜ {e = (vi, vj)|(vi, vj) ‚àà V 2 }.\\nRequire: Edge type function fT : fT (e)= the edge type of e. Require: Mergeable edge type set T .\\n1: Sort V by the number of neighbors in ascending order. 2: C = ‚àÖ # The set collects nodes that have merged others.\\n3: for vi ‚àà V do 4: if vi / ‚àà C then 5:\\nfor vj ‚àà {vj|vj ‚àà V ; (vj, vi) ‚àà E} do 6:\\nif fT (e = (vj, vi)) ‚àà S then 7:\\nV = V \\\\ {vi} # Merge vi to vj 8: for v k ‚àà {v k |v k ‚àà V ; (v k , vi) ‚àà E} do 9:\\n# Move edges in vi to vj 10:\\nE = E \\\\ {(v k , vi)} 11: E = E ‚à™ {(v k , vj)} 12: C = C ‚à™ {vj} 13: break 14: return Pooled graph G = (V, E)\\nas a clause should at least consist of a predicate with its core dependents 1 . CM merges tokens that are connected by dependency relations in others categories 2 . Since we do not merge nodes linked with core arguments, the basis of a clause will be retained. As a result, CM simplifies the graph and maintains the core components of a clause.\\nThe details of CM is in Alg. 1 3 . CM share similarities with HM: i). we visit nodes by ascending order according to the node degree (line 1); ii). supernodes cannot be merged again in each round of CM (line 4). Being different from HM, we decide whether the visited node can be merged with its dependent head based on the edge type (line 6-7) 4 .\\nIn Fig. 4, we show an example of CM and more visualizations are in the appendix (Fig. 7). We first visit n 0 , n 2 , n 4 , ..., n 5 in order, based on node degree. When visiting n 0 , CM matches n 0 with its dependent head n 1 because the dependent arc between them belongs to others types. Similarly, n 2 is merged with n 3 and n 4 is matched with n 5 , forming m 1 and m 2 , respectively. However, m 2 cannot be further combined with m 1 because m 2 is already a supernode in the current round of CM. Moreover, m 0 cannot be merged with m 1 even if we perform CM again because the dependency arcs \"nsubj:pass\" belongs to core arguments.\\nPooling Operation. Each matching process generates a coarsened hypergraph. Performing match-1 Following the definition in Universal Dependency: www. universaldependencies.org/u/overview/simple-syntax.html.\\n2 E.g., \"det\" and \"amod\" are not core arguments but others. 3 The set T is defined as {all dependency edges} \\\\ {\"nsubj\", \"nsubj:pass\", \"dobj\", \"iobj\", \"csubj\", \"csubj:pass\", \"ccomp\", \"xcomp\"}. 4 When moving edges from children nodes to supernodes, we do not include self-loop causing by merging. given input \"A study was performed in patients with bladder carcinoma\". \"ADJ:NEXT\" indicates the adjacency edges. After executing CM once, the graph size reduces largely yet still maintain the core structure of the original sentence.\\ning L times produces L hypergraphs with increasing coarsening levels, denoted as G 0 , G 1 , ..., G L , where G 0 is the initial graph. We use matching matrix M l‚àí1,l to mathematically represent the merging process from level l ‚àí 1 to level l.\\nM l‚àí1,l ‚àà R n√óm converts G l‚àí1 ‚àà R n√ón to G l ‚àà R m√óm with n ‚â• m. Each cell m ij in M l‚àí1,l is: mij = 1, if node i is matched into supernode j. 0, otherwise.(2)\\nWith M l‚àí1,l constructed, we can compute the adjacency matrix of level l, A l , based on A l‚àí15 :\\nA l = M T l‚àí1,l A l‚àí1 M l‚àí1,l ,(3)\\nand perform representation transformation to get the initial node embeddings for the next level:\\nH in l = M l‚àí1,l H out l‚àí1 (4)\\nwhere H out l‚àí1 and H in l represent the output embedding of G l‚àí1 and the input embedding of G l .',\n",
       "    'n_publication_ref': 7,\n",
       "    'n_figure_ref': 4},\n",
       "   {'heading': 'C Matching Matrices',\n",
       "    'text': 'In this section, we attach the corresponding matching matrices and illustrate how adjacency matrix A 1 can be derived from A 0 in Fig. 6, given the Hybrid Matching (HM) example in Fig. 3.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'MrGCN: Mirror Graph Convolution Network for Relation Extraction with Long-Term Dependencies',\n",
       "  'abstract': 'The ability to capture complex linguistic structures and long-term dependencies among words in the passage is essential for relation extraction (RE) tasks. Graph neural networks (GNNs), one of the means to encode dependency graphs, have been shown to be effective in prior works. However, relatively little attention has been paid to receptive fields of GNNs, which can be crucial for tasks with extremely long text that requires discourse understanding. In this work, we leverage the idea of graph pooling and propose the Mirror Graph Convolution Network, a GNN model with a poolingunpooling structure tailored to RE tasks. The pooling branch reduces the graph size and enables the GNN to obtain larger receptive fields within fewer layers; the unpooling branch restores the pooled graph to its original resolution for token-level RE tasks. Experiments on two discourse-level relation extraction datasets demonstrate the effectiveness of our method, showing significant improvements over prior methods especially when modeling long-term dependencies is necessary. Moreover, we propose Clause Matching (CM), a novel graph pooling method that merges nodes based on dependency relations in graph. CM can largely reduce the graph size while retaining the main semantics of the input text.',\n",
       "  'paddleOCR': [[[[455.0, 9.0], [482.0, 12.0], [480.0, 27.0], [453.0, 24.0]],\n",
       "    ['mo', 0.9815847873687744]],\n",
       "   [[[767.0, 8.0], [794.0, 12.0], [792.0, 28.0], [765.0, 24.0]],\n",
       "    ['mo', 0.9569510221481323]],\n",
       "   [[[89.0, 22.0], [113.0, 27.0], [109.0, 42.0], [85.0, 37.0]],\n",
       "    ['no', 0.9693158864974976]],\n",
       "   [[[252.0, 22.0], [275.0, 27.0], [272.0, 43.0], [249.0, 38.0]],\n",
       "    ['n2', 0.9027085900306702]],\n",
       "   [[[425.0, 23.0], [511.0, 26.0], [511.0, 43.0], [424.0, 41.0]],\n",
       "    ['Disease-A', 0.9962227940559387]],\n",
       "   [[[740.0, 22.0], [826.0, 26.0], [825.0, 46.0], [739.0, 43.0]],\n",
       "    ['Disease-A', 0.999344527721405]],\n",
       "   [[[51.0, 38.0], [138.0, 40.0], [138.0, 58.0], [51.0, 55.0]],\n",
       "    ['Disease-A', 0.9984747767448425]],\n",
       "   [[[203.0, 38.0], [318.0, 38.0], [318.0, 58.0], [203.0, 58.0]],\n",
       "    ['Syndrome-B', 0.999092698097229]],\n",
       "   [[[414.0, 44.0], [527.0, 46.0], [527.0, 66.0], [413.0, 64.0]],\n",
       "    ['Syndrome-B', 0.9985774159431458]],\n",
       "   [[[727.0, 46.0], [839.0, 46.0], [839.0, 65.0], [727.0, 65.0]],\n",
       "    ['Syndrome-B', 0.9947378039360046]],\n",
       "   [[[479.0, 90.0], [500.0, 93.0], [498.0, 109.0], [477.0, 106.0]],\n",
       "    ['n1', 0.8339003920555115]],\n",
       "   [[[134.0, 102.0], [155.0, 105.0], [153.0, 121.0], [131.0, 118.0]],\n",
       "    ['n1', 0.8978471755981445]],\n",
       "   [[[301.0, 111.0], [341.0, 111.0], [341.0, 130.0], [301.0, 130.0]],\n",
       "    ['SEM', 0.9979761242866516]],\n",
       "   [[[454.0, 111.0], [521.0, 111.0], [521.0, 128.0], [454.0, 128.0]],\n",
       "    ['causes', 0.9963414072990417]],\n",
       "   [[[587.0, 109.0], [642.0, 109.0], [642.0, 127.0], [587.0, 127.0]],\n",
       "    ['NHEM', 0.9980984926223755]],\n",
       "   [[[784.0, 107.0], [851.0, 107.0], [851.0, 124.0], [784.0, 124.0]],\n",
       "    ['causes', 0.9964788556098938]],\n",
       "   [[[111.0, 123.0], [177.0, 123.0], [177.0, 140.0], [111.0, 140.0]],\n",
       "    ['causes', 0.9960749745368958]],\n",
       "   [[[383.0, 150.0], [396.0, 150.0], [396.0, 166.0], [383.0, 166.0]],\n",
       "    ['1', 0.9999492168426514]],\n",
       "   [[[579.0, 151.0], [592.0, 151.0], [592.0, 169.0], [579.0, 169.0]],\n",
       "    ['1', 0.9998880624771118]],\n",
       "   [[[723.0, 144.0], [748.0, 147.0], [746.0, 161.0], [722.0, 159.0]],\n",
       "    ['m1', 0.7969071865081787]],\n",
       "   [[[478.0, 168.0], [499.0, 170.0], [497.0, 186.0], [476.0, 183.0]],\n",
       "    ['n4', 0.8944406509399414]],\n",
       "   [[[702.0, 159.0], [772.0, 159.0], [772.0, 180.0], [702.0, 180.0]],\n",
       "    ['Drug-C', 0.9998066425323486]],\n",
       "   [[[134.0, 180.0], [156.0, 180.0], [156.0, 195.0], [134.0, 195.0]],\n",
       "    ['n4', 0.992142379283905]],\n",
       "   [[[359.0, 179.0], [425.0, 179.0], [425.0, 200.0], [359.0, 200.0]],\n",
       "    ['13.', 0.8688976168632507]],\n",
       "   [[[465.0, 186.0], [524.0, 186.0], [524.0, 204.0], [465.0, 204.0]],\n",
       "    ['treats', 0.9955565929412842]],\n",
       "   [[[560.0, 180.0], [618.0, 177.0], [619.0, 199.0], [561.0, 201.0]],\n",
       "    ['3x3', 0.8706264495849609]],\n",
       "   [[[706.0, 183.0], [768.0, 183.0], [768.0, 203.0], [706.0, 203.0]],\n",
       "    ['treats', 0.9972328543663025]],\n",
       "   [[[119.0, 195.0], [182.0, 197.0], [181.0, 217.0], [119.0, 214.0]],\n",
       "    ['treats', 0.9963495135307312]],\n",
       "   [[[253.0, 221.0], [274.0, 223.0], [272.0, 240.0], [250.0, 237.0]],\n",
       "    ['ns', 0.7040598392486572]],\n",
       "   [[[594.0, 213.0], [615.0, 213.0], [615.0, 229.0], [594.0, 229.0]],\n",
       "    ['ns', 0.7235383987426758]],\n",
       "   [[[391.0, 235.0], [415.0, 240.0], [412.0, 255.0], [388.0, 250.0]],\n",
       "    ['n3', 0.9368264675140381]],\n",
       "   [[[554.0, 227.0], [666.0, 229.0], [666.0, 249.0], [553.0, 247.0]],\n",
       "    ['Syndrome-B', 0.9979804754257202]],\n",
       "   [[[779.0, 230.0], [893.0, 230.0], [893.0, 250.0], [779.0, 250.0]],\n",
       "    ['Syndrome-B', 0.999457061290741]],\n",
       "   [[[48.0, 246.0], [71.0, 249.0], [68.0, 265.0], [46.0, 262.0]],\n",
       "    ['n3', 0.9701326489448547]],\n",
       "   [[[211.0, 240.0], [327.0, 240.0], [327.0, 260.0], [211.0, 260.0]],\n",
       "    ['Syndrome-B', 0.9983204007148743]],\n",
       "   [[[365.0, 249.0], [439.0, 251.0], [438.0, 275.0], [364.0, 273.0]],\n",
       "    ['Drug-C', 0.999129593372345]],\n",
       "   [[[24.0, 263.0], [93.0, 263.0], [93.0, 284.0], [24.0, 284.0]],\n",
       "    ['Drug-C', 0.9997527599334717]],\n",
       "   [[[480.0, 257.0], [549.0, 254.0], [550.0, 279.0], [481.0, 281.0]],\n",
       "    ['32', 0.9904068112373352]],\n",
       "   [[[646.0, 256.0], [655.0, 254.0], [658.0, 266.0], [649.0, 268.0]],\n",
       "    ['1', 0.8504841923713684]],\n",
       "   [[[139.0, 277.0], [161.0, 280.0], [159.0, 295.0], [136.0, 292.0]],\n",
       "    ['n6', 0.8926035761833191]],\n",
       "   [[[483.0, 288.0], [506.0, 288.0], [506.0, 304.0], [483.0, 304.0]],\n",
       "    ['n6', 0.9053577780723572]],\n",
       "   [[[620.0, 281.0], [687.0, 281.0], [687.0, 303.0], [620.0, 303.0]],\n",
       "    ['32', 0.9913190603256226]],\n",
       "   [[[762.0, 285.0], [789.0, 290.0], [786.0, 305.0], [760.0, 301.0]],\n",
       "    ['m2', 0.9066960215568542]],\n",
       "   [[[125.0, 299.0], [162.0, 295.0], [164.0, 314.0], [127.0, 318.0]],\n",
       "    ['and', 0.9964869618415833]],\n",
       "   [[[253.0, 304.0], [275.0, 307.0], [273.0, 323.0], [250.0, 320.0]],\n",
       "    ['n7', 0.866639256477356]],\n",
       "   [[[475.0, 306.0], [510.0, 303.0], [511.0, 319.0], [476.0, 321.0]],\n",
       "    ['and', 0.9951514601707458]],\n",
       "   [[[755.0, 306.0], [792.0, 303.0], [794.0, 321.0], [757.0, 323.0]],\n",
       "    ['and', 0.996195375919342]],\n",
       "   [[[213.0, 324.0], [325.0, 324.0], [325.0, 344.0], [213.0, 344.0]],\n",
       "    ['Syndrome-D', 0.9992405772209167]],\n",
       "   [[[607.0, 321.0], [629.0, 321.0], [629.0, 337.0], [607.0, 337.0]],\n",
       "    ['n7', 0.924586296081543]],\n",
       "   [[[723.0, 323.0], [836.0, 323.0], [836.0, 340.0], [723.0, 340.0]],\n",
       "    ['Syndrome-D', 0.9987696409225464]],\n",
       "   [[[567.0, 339.0], [680.0, 339.0], [680.0, 359.0], [567.0, 359.0]],\n",
       "    ['Syndrome-D', 0.9990555047988892]],\n",
       "   [[[486.0, 351.0], [554.0, 348.0], [555.0, 373.0], [487.0, 375.0]],\n",
       "    ['2x2', 0.9757066369056702]]],\n",
       "  'ocr': [[[702.0, 159.0], [772.0, 159.0], [772.0, 180.0], [702.0, 180.0]],\n",
       "   ['Drug-C', 0.9998066425323486]]},\n",
       " '2202.01862v1-Figure3-1.png': {'caption': 'Figure 3. (a, b) We collect a set of sim and real images through teleoperation, and use them first to train a RetinaGAN model. (b) We then use the trained real-to-sim and sim-to-real models to create paired (real, adapted real) images and (paired sim, adapted sim) images, respectively. (c) Policy representations and predictions are encouraged to be invariant between paired images via a novel Task Consistency Loss. The same procedure can be used for the depth images (not shown here but used in the paper). (d) We use simulation in parallel to model training the evaluate all the checkpoints, and use it as a metric to select the right checkpoint for the real world deployment.',\n",
       "  'imageText': ['ResNet-18',\n",
       "   'Encoder',\n",
       "   'Checkpoint',\n",
       "   'Selection',\n",
       "   'Real',\n",
       "   'Eval',\n",
       "   '(a)',\n",
       "   'Data',\n",
       "   'Collection',\n",
       "   '(c)',\n",
       "   'Training',\n",
       "   '(d)',\n",
       "   'Deployment(b)',\n",
       "   'Dataset',\n",
       "   'Generation',\n",
       "   'Sim',\n",
       "   'Eval',\n",
       "   'A',\n",
       "   'N',\n",
       "   'in',\n",
       "   'aG',\n",
       "   'Sim2RealR',\n",
       "   'et',\n",
       "   'Real2Sim',\n",
       "   'Sim2Real',\n",
       "   'Embedding',\n",
       "   'ConsistencyReal2Sim',\n",
       "   'Action',\n",
       "   'Supervision',\n",
       "   'Action',\n",
       "   'Consistency',\n",
       "   'N',\n",
       "   'augmentations',\n",
       "   'of',\n",
       "   'paired',\n",
       "   'images',\n",
       "   'Batches',\n",
       "   '=',\n",
       "   'mix',\n",
       "   'of',\n",
       "   'sim',\n",
       "   'and',\n",
       "   'real',\n",
       "   'datasets',\n",
       "   '‚Ä¶‚Ä¶',\n",
       "   '‚Ä¶',\n",
       "   'Paired',\n",
       "   'Real',\n",
       "   'Data',\n",
       "   'Paired',\n",
       "   'Sim',\n",
       "   'Data',\n",
       "   'Sim',\n",
       "   'Data',\n",
       "   '2.7',\n",
       "   'hours',\n",
       "   'Real',\n",
       "   'Data',\n",
       "   '13.5',\n",
       "   'hours'],\n",
       "  'image_file': '2202.01862v1-Figure3-1.png',\n",
       "  'sections': [],\n",
       "  'title': 'Practical Imitation Learning in the Real World via Task Consistency Loss',\n",
       "  'abstract': 'Recent work in visual end-to-end learning for robotics has shown the promise of imitation learning across a variety of tasks. Such approaches are expensive both because they require large amounts of real world training demonstrations and because identifying the best model to deploy in the real world requires time-consuming real-world evaluations. These challenges can be mitigated by simulation: by supplementing real world data with simulated demonstrations and using simulated evaluations to identify high performing policies. However, this introduces the well-known \"reality gap\" problem, where simulator inaccuracies decorrelate performance in simulation from that of reality. In this paper, we build on top of prior work in GAN-based domain adaptation and introduce the notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages sim and real alignment both at the feature and action-prediction levels. We demonstrate the effectiveness of our approach by teaching a mobile manipulator to autonomously approach a door, turn the handle to open the door, and enter the room. The policy performs control from RGB and depth images and generalizes to doors not encountered in training data. We achieve 80% success across ten seen and unseen scenes using only ‚àº16.2 hours of teleoperated demonstrations in sim and real. To the best of our knowledge, this is the first work to tackle latched door opening from a purely end-toend learning approach, where the task of navigation and manipulation are jointly modeled by a single neural network.',\n",
       "  'paddleOCR': [[[[0.0, 6.0], [192.0, 4.0], [192.0, 33.0], [0.0, 36.0]],\n",
       "    ['(a) Data Collection', 0.9983460307121277]],\n",
       "   [[[403.0, 10.0], [625.0, 10.0], [625.0, 31.0], [403.0, 31.0]],\n",
       "    ['(b) Dataset Generation', 0.9775636196136475]],\n",
       "   [[[1039.0, 4.0], [1155.0, 9.0], [1154.0, 38.0], [1038.0, 33.0]],\n",
       "    ['(c) Training', 0.9844527840614319]],\n",
       "   [[[1833.0, 6.0], [1986.0, 6.0], [1986.0, 29.0], [1833.0, 29.0]],\n",
       "    ['(d) Deployment', 0.9867916703224182]],\n",
       "   [[[1895.0, 38.0], [1989.0, 33.0], [1991.0, 62.0], [1897.0, 68.0]],\n",
       "    [' Sim Eval', 0.9677350521087646]],\n",
       "   [[[928.0, 59.0], [995.0, 59.0], [995.0, 80.0], [928.0, 80.0]],\n",
       "    ['Paired', 0.9991820454597473]],\n",
       "   [[[1336.0, 65.0], [1532.0, 65.0], [1532.0, 92.0], [1336.0, 92.0]],\n",
       "    ['N augmentations of', 0.9997908473014832]],\n",
       "   [[[935.0, 84.0], [987.0, 84.0], [987.0, 107.0], [935.0, 107.0]],\n",
       "    ['Real', 0.9996291399002075]],\n",
       "   [[[1588.0, 82.0], [1702.0, 82.0], [1702.0, 105.0], [1588.0, 105.0]],\n",
       "    ['Embedding', 0.9997002482414246]],\n",
       "   [[[676.0, 98.0], [776.0, 98.0], [776.0, 119.0], [676.0, 119.0]],\n",
       "    ['Real2Sim', 0.9963251352310181]],\n",
       "   [[[1364.0, 94.0], [1504.0, 94.0], [1504.0, 115.0], [1364.0, 115.0]],\n",
       "    ['paired images', 0.9995546340942383]],\n",
       "   [[[935.0, 111.0], [987.0, 111.0], [987.0, 134.0], [935.0, 134.0]],\n",
       "    ['Data', 0.9993139505386353]],\n",
       "   [[[1583.0, 109.0], [1702.0, 109.0], [1702.0, 132.0], [1583.0, 132.0]],\n",
       "    ['Consistency', 0.9995928406715393]],\n",
       "   [[[1361.0, 147.0], [1499.0, 147.0], [1499.0, 176.0], [1361.0, 176.0]],\n",
       "    ['ResNet-18', 0.9773991107940674]],\n",
       "   [[[1379.0, 182.0], [1482.0, 182.0], [1482.0, 205.0], [1379.0, 205.0]],\n",
       "    ['Encoder', 0.9991095662117004]],\n",
       "   [[[261.0, 211.0], [364.0, 211.0], [364.0, 234.0], [261.0, 234.0]],\n",
       "    ['Real Data', 0.9998347759246826]],\n",
       "   [[[532.0, 228.0], [623.0, 228.0], [623.0, 251.0], [532.0, 251.0]],\n",
       "    ['Real2Sim', 0.9925854206085205]],\n",
       "   [[[261.0, 239.0], [366.0, 239.0], [366.0, 260.0], [261.0, 260.0]],\n",
       "    ['13.5 hours', 0.9995641708374023]],\n",
       "   [[[1609.0, 237.0], [1676.0, 237.0], [1676.0, 258.0], [1609.0, 258.0]],\n",
       "    ['Action', 0.9993314743041992]],\n",
       "   [[[1579.0, 262.0], [1702.0, 264.0], [1701.0, 287.0], [1579.0, 285.0]],\n",
       "    ['Consistency', 0.99936842918396]],\n",
       "   [[[1898.0, 262.0], [2010.0, 262.0], [2010.0, 285.0], [1898.0, 285.0]],\n",
       "    ['Checkpoint', 0.9998804926872253]],\n",
       "   [[[450.0, 289.0], [467.0, 289.0], [467.0, 327.0], [450.0, 327.0]],\n",
       "    ['Reti', 0.9981580376625061]],\n",
       "   [[[1904.0, 289.0], [2001.0, 289.0], [2001.0, 310.0], [1904.0, 310.0]],\n",
       "    ['Selection', 0.999749481678009]],\n",
       "   [[[527.0, 300.0], [620.0, 295.0], [621.0, 318.0], [529.0, 323.0]],\n",
       "    [' Sim2Real', 0.9471831917762756]],\n",
       "   [[[1763.0, 364.0], [1796.0, 361.0], [1802.0, 421.0], [1770.0, 424.0]],\n",
       "    ['BC', 0.9968366622924805]],\n",
       "   [[[1623.0, 415.0], [1662.0, 424.0], [1655.0, 452.0], [1616.0, 442.0]],\n",
       "    ['lA', 0.9324555397033691]],\n",
       "   [[[263.0, 431.0], [358.0, 431.0], [358.0, 454.0], [263.0, 454.0]],\n",
       "    ['Sim Data', 0.9878471493721008]],\n",
       "   [[[674.0, 428.0], [774.0, 422.0], [776.0, 452.0], [675.0, 457.0]],\n",
       "    [' Sim2Real', 0.9419921040534973]],\n",
       "   [[[933.0, 427.0], [1000.0, 427.0], [1000.0, 450.0], [933.0, 450.0]],\n",
       "    ['Paired', 0.9996039867401123]],\n",
       "   [[[265.0, 459.0], [364.0, 459.0], [364.0, 479.0], [265.0, 479.0]],\n",
       "    ['2.7 hours', 0.9995172023773193]],\n",
       "   [[[944.0, 454.0], [987.0, 454.0], [987.0, 477.0], [944.0, 477.0]],\n",
       "    ['Sim', 0.999824583530426]],\n",
       "   [[[1047.0, 467.0], [1305.0, 467.0], [1305.0, 488.0], [1047.0, 488.0]],\n",
       "    [' Batches = mix of sim and', 0.9825686812400818]],\n",
       "   [[[1613.0, 459.0], [1678.0, 459.0], [1678.0, 482.0], [1613.0, 482.0]],\n",
       "    ['Action', 0.9998972415924072]],\n",
       "   [[[939.0, 479.0], [991.0, 479.0], [991.0, 502.0], [939.0, 502.0]],\n",
       "    ['Data', 0.9997216463088989]],\n",
       "   [[[1114.0, 494.0], [1245.0, 494.0], [1245.0, 515.0], [1114.0, 515.0]],\n",
       "    ['real datasets', 0.992201566696167]],\n",
       "   [[[1588.0, 486.0], [1704.0, 486.0], [1704.0, 507.0], [1588.0, 507.0]],\n",
       "    ['Supervision', 0.9996876120567322]],\n",
       "   [[[1898.0, 500.0], [1995.0, 500.0], [1995.0, 521.0], [1898.0, 521.0]],\n",
       "    ['Real Eval', 0.9719009399414062]]],\n",
       "  'ocr': [[[928.0, 59.0], [995.0, 59.0], [995.0, 80.0], [928.0, 80.0]],\n",
       "   ['Paired', 0.9991820454597473]]},\n",
       " '2101.11093v2-Figure1-1.png': {'caption': 'Fig. 1. Overview of the proposed distributed planning approach for nonmonotone information gathering (see Sec. IV). Robots generate individual candidate trajectories and jointly build a team plan via distributed local search (DLS), by repeatedly proposing changes to the collective trajectories.',\n",
       "  'imageText': ['Team',\n",
       "   'Solution',\n",
       "   '(DLS)',\n",
       "   'Trajectory',\n",
       "   'Generation',\n",
       "   'Trajectory',\n",
       "   'Generation',\n",
       "   'Proposal',\n",
       "   'Exchange'],\n",
       "  'image_file': '2101.11093v2-Figure1-1.png',\n",
       "  'sections': [{'heading': 'I. INTRODUCTION',\n",
       "    'text': \"Developments in sensing and mobility have enabled effective utilization of robot systems in autonomous mapping [1]- [4], search and rescue [5]- [7], and environmental monitoring [8]- [11]. These tasks require spatiotemporal information collection which can be achieved more efficiently and accurately by larger robot teams, rather than relying on individual robots. Robot teams may take advantage of heterogeneous capabilities, require less storage and computation per robot, and may achieve better environment coverage in shorter time [12]- [15]. Task-level performance is usually quantified by a measure of information gain, where typically the marginal improvements diminish given additional measurements (submodularity), and adding new measurements does not reduce the objective (monotonicity). Although planning optimally Fig. 1. Overview of the proposed distributed planning approach for nonmonotone information gathering (see Sec. IV). Robots generate individual candidate trajectories and jointly build a team plan via distributed local search (DLS), by repeatedly proposing changes to the collective trajectories.\\nfor multi-robot sensing trajectories is generally intractable, these two properties allow for near-optimal approximation algorithms that scale to large robot teams, while providing worst-case guarantees. Additionally, practical implementations often need to consider various measures for energy expenditure, such as control effort or distance travelled. A common approach is to impose fixed budgets, which preserves submodularity and monotonicity of the objective, so that existing algorithms may still be used [16]- [18].\\nIn this paper, we are motivated by scenarios where robots, with potentially different sensing and control capabilities, seek a desired trade-off between sensing and energy cost. Specifically, we formulate an energy-aware active information acquisition problem, where the goal is to plan trajectories for a team of heterogeneous robots to maximize a weighted sum of information gain and energy cost. One key observation is that adding the energy cost breaks the monotonicity of the objective, violating an assumption held by existing approximation algorithms. Thus, we propose a new distributed planning algorithm based on local search [19] (see Fig. 1) that has a worst-case guarantee for the non-monotone objective. We also show how to reduce the method's computation and communication to improve scalability. Related Work. Our work belongs to the category of multirobot informative path planning, where robots plan sensing trajectories to reduce uncertainty about a dynamic process (e.g., [2], [4], [16], [18], [20]- [25]). To alleviate the computational complexity, which is exponential in the number of robots, approximation methods have been developed to pro-duce near-optimal solutions for a submodular and monotone objective (e.g., mutual information). A common technique is coordinate descent, where robots plan successively while incorporating the plans of previous robots. Ref. [16] showed that coordinate descent extends the near-optimality of a single-robot planner to the multi-robot scenario. This result was extend to dynamic targets by [26], achieving at least 50% of the optimal performance regardless of the planning order. Refs. [18], [22] decentralized the greedy method [27] by adding the best single-robot trajectory to the team solution in every round. Ref. [4] proposed distributed sequential greedy algorithm to alleviate the inefficiency in sequential planning.\\nOur problem can be seen as non-monotone submodular maximization subject to a partition matroid constraint (see Sec. III), for which approximation algorithms already exist. The first such algorithm was developed by [19] based on local search, which can handle multiple matroid constraints. Extending [19], ref. [28] proposed a greedy-based approach that can handle multiple independence systems (more general than matroids), but has a worse approximation ratio given a single matroid. Other methods use multilinear relaxation such as [29], [30] for better approximation ratios, but require significant computation. Applying some of these ideas in robotics, ref. [31] used the continuous greedy method by [29] for decentralized multi-robot task assignment. In the same domain, ref. [32] combined sampling, greedy method, and lazy evaluation [33] to achieve fast computation. We decided to build upon [19] for its simplicity and guarantees. We also attempt to incorporate well-known techniques like greedy method and lazy evaluation, but they are specialized in the context of local search, as detailed in Sec. IV-B. Contributions. The main limitation of the prior works is the assumption of monotonicity of the objective function. Problems without monotonicity, such as the energy-aware problem we propose, cannot be solved by the above methods while retaining their near-optimality properties. In contrast, our proposed algorithm provides a theoretical performance guarantee even for non-monotone objectives. In this work:\\n‚Ä¢ We propose a distributed algorithm based on local search where robots collaboratively build a team plan by proposing modifications to the collective trajectories; ‚Ä¢ We reduce its computation and communication requirements by prioritizing search orders of local search and warm starting with greedy solutions, respectively; ‚Ä¢ We show that the proposed algorithm outperforms a state-of-the-art algorithm for multi-robot target tracking in coordinating a team of heterogeneous robots, while trading off sensing performance and energy expenditure.\",\n",
       "    'n_publication_ref': 33,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Non-Monotone Energy-Aware Information Gathering for Heterogeneous Robot Teams',\n",
       "  'abstract': \"This paper considers the problem of planning trajectories for a team of sensor-equipped robots to reduce uncertainty about a dynamical process. Optimizing the tradeoff between information gain and energy cost (e.g., control effort, distance travelled) is desirable but leads to a nonmonotone objective function in the set of robot trajectories. Therefore, common multi-robot planning algorithms based on techniques such as coordinate descent lose their performance guarantees. Methods based on local search provide performance guarantees for optimizing a non-monotone submodular function, but require access to all robots' trajectories, making it not suitable for distributed execution. This work proposes a distributed planning approach based on local search and shows how lazy/greedy methods can be adopted to reduce the computation and communication of the approach. We demonstrate the efficacy of the proposed method by coordinating robot teams composed of both ground and aerial vehicles with different sensing/control profiles and evaluate the algorithm's performance in two target tracking scenarios. Compared to the naive distributed execution of local search, our approach saves up to 60% communication and 80-92% computation on average when coordinating up to 10 robots, while outperforming the coordinate descent based algorithm in achieving a desirable trade-off between sensing and energy cost.\",\n",
       "  'paddleOCR': [[[[72.0, 34.0], [247.0, 39.0], [246.0, 74.0], [71.0, 69.0]],\n",
       "    ['Trajectory', 0.9998390078544617]],\n",
       "   [[[725.0, 33.0], [902.0, 38.0], [901.0, 73.0], [724.0, 68.0]],\n",
       "    ['Trajectory', 0.9799111485481262]],\n",
       "   [[[408.0, 45.0], [567.0, 45.0], [567.0, 80.0], [408.0, 80.0]],\n",
       "    ['Proposal', 0.9997414946556091]],\n",
       "   [[[63.0, 84.0], [257.0, 86.0], [256.0, 114.0], [63.0, 112.0]],\n",
       "    ['Generation', 0.9996930360794067]],\n",
       "   [[[401.0, 93.0], [574.0, 97.0], [573.0, 129.0], [400.0, 124.0]],\n",
       "    ['Exchange', 0.9993011355400085]],\n",
       "   [[[718.0, 84.0], [912.0, 86.0], [912.0, 114.0], [717.0, 112.0]],\n",
       "    ['Generation', 0.9997531771659851]],\n",
       "   [[[440.0, 140.0], [535.0, 140.0], [535.0, 177.0], [440.0, 177.0]],\n",
       "    ['(DLS)', 0.9983588457107544]],\n",
       "   [[[439.0, 247.0], [536.0, 251.0], [534.0, 283.0], [437.0, 280.0]],\n",
       "    ['Team', 0.9998533725738525]],\n",
       "   [[[414.0, 295.0], [562.0, 298.0], [561.0, 330.0], [413.0, 326.0]],\n",
       "    ['Solution', 0.9997168779373169]]],\n",
       "  'ocr': [[[439.0, 247.0], [536.0, 251.0], [534.0, 283.0], [437.0, 280.0]],\n",
       "   ['Team', 0.9998533725738525]]},\n",
       " '2010.01319v3-Figure1-1.png': {'caption': 'Figure 1: The architecture of the LaDBSDE scheme.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.01319v3-Figure1-1.png',\n",
       "  'sections': [{'heading': 'The locally additive deep BSDE scheme',\n",
       "    'text': \"The LDBSDE scheme improves the results of the DBSDE scheme for the approximations in the entire time domain. However, it can also get stuck in poor local minima as the DBSDE scheme especially for a complex solution structure and a long terminal time. Our idea is to consider a formulation based on a global optimization with local loss function, where each loss term includes the terminal condition. This is achieved by using the iterative time discretization (7). We refer to this as the Locally additive Deep BSDE (LaDBSDE) scheme as each local loss term accumulates the information up to the terminal condition. The algorithm is given as follows:\\n‚Ä¢ At each time t i , i = 0, 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N ‚àí1: use one deep neural network œà ,n,L d 0 ,d 1 (x; Œ∏) : R d 0 ‚Üí R d 1 to approximate Y i ‚àà R as Y Œ∏ i ,\\nwhere the input x of the network is the time value t i ‚àà R + and the markovian process\\nX i ‚àà R d , d 0 = d + 1, d 1 = 1,and\\nZ Œ∏ i = ‚àÇœà ,n,L d 0 ,d 1 (x; Œ∏) ‚àÇX X=X i œÉ (t i , X i ) ,\\na formulation based on (4).\\n‚Ä¢ The empirical loss and optimal parameters Œ∏ are given as\\nL(Œ∏) = M m=1 \\uf8eb \\uf8ed N ‚àí1 i=0 |Y m,Œ∏ i ‚àí N ‚àí1 j=i f t j , X m j , Y m,Œ∏ j , Z m,Œ∏ j ‚àÜt ‚àí Z m,Œ∏ j ‚àÜW m j ‚àí g(X m N )| 2 \\uf8f6 \\uf8f8 , Œ∏ * ‚àà argmin Œ∏‚ààR œÅ L(Œ∏),(9)\\nwhen using M samples.\\nWe see that a neural network is used to approximate the solution of the BSDE and its gradient via AD. These approximations are obtained by the global minimization of quadratic local loss functions including the terminal time ( 9). The algorithmic framework (without using minibatches and Adam optimizer) can be formulated as follows.\\nFramework 3.1. Let T, Œ≥ ‚àà (0, ‚àû), d, œÅ, N ‚àà N, X 0 ‚àà R d , ¬µ : [0, T ]√óR d ‚Üí R d , œÉ : [0, T ]√óR d ‚Üí R d√ód , f : [0, T ] √ó R d √ó R √ó R 1√ód ‚Üí R and g : R d ‚Üí R be functions, let (‚Ñ¶, F, P) be a probability space, let W m : [0, T ] √ó ‚Ñ¶ ‚Üí R d , m ‚àà N 0 , be independent d-dimensional standard Brownian motions on (‚Ñ¶, F, P), let t 0 , t 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , t N ‚àà [0, T ] be real numbers with 0 = t 0 < t 1 < ‚Ä¢ ‚Ä¢ ‚Ä¢ < t N = T, for every m ‚àà N 0 let X m : {0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N } √ó ‚Ñ¶ ‚Üí R d be a stochastic process which satisfies for i ‚àà {0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N ‚àí 1}, ‚àÜW m i = W m i+1 ‚àí W m i that X m i+1 = X m i + ¬µ (t i , X m i ) ‚àÜt + œÉ (t i , X m i ) ‚àÜW m i , X m 0 = X 0 , for every Œ∏ ‚àà R œÅ , i ‚àà {0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N ‚àí 1}, d 0 = d + 1, d 1 = 1, : R ‚Üí R, L ‚àà N, n ‚àà N let œà ,n,L d 0 ,d 1 : R d 0 ‚Üí R d 1 (œà ,n,L d 0 ,d 1 ‚àà C 1\\n) be a function (neural network), the output given as Y Œ∏ i and let\\nZ Œ∏ i = ‚àá x œà ,n,L d 0 ,d 1 ((t i , x); Œ∏) x=X m i œÉ(t i , X m i ), for every m ‚àà N 0 , i ‚àà {0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N ‚àí 1} let œÜ m i : R œÅ √ó ‚Ñ¶ ‚Üí R be the function which satisfies for all Œ∏ ‚àà R œÅ , œâ ‚àà ‚Ñ¶ that œÜ m i (Œ∏, œâ) = |Y m,Œ∏ i (œâ)‚àí N ‚àí1 j=i f t j , X m j (œâ), Y m,Œ∏ j (œâ), Z m,Œ∏ j (œâ) ‚àÜt ‚àí Z m,Œ∏ j (œâ)‚àÜW m j (œâ) ‚àíg(X m N (œâ))| 2 , for every m ‚àà N 0 let œÜ m : R œÅ √ó ‚Ñ¶ ‚Üí R be the function which satisfies for all Œ∏ ‚àà R œÅ , œâ ‚àà ‚Ñ¶ that œÜ m (Œ∏, œâ) = N ‚àí1 i=0 œÜ m i (Œ∏, œâ), for every m ‚àà N 0 let Œ¶ m : R œÅ √ó ‚Ñ¶ ‚Üí R œÅ be a function which satisfies for all œâ ‚àà ‚Ñ¶, Œ∏ ‚àà {v ‚àà R œÅ : (R œÅ w ‚Üí œÜ m (w, œâ) ‚àà R is differentiable at v ‚àà R œÅ )} that Œ¶ m (Œ∏, œâ) = (‚àá Œ∏ œÜ m )(Œ∏, œâ),\\nand let Œò : N 0 √ó ‚Ñ¶ ‚Üí R œÅ be a stochastic process which satisfy for all m ‚àà N that\\nŒò m = Œò m‚àí1 ‚àí Œ≥Œ¶ m (Œò m‚àí1 ).\\nFigure 1: The architecture of the LaDBSDE scheme.\\nThe architecture of the LaDBSDE scheme is displayed in Figure 1. The flow of the information is represented by the direction of the arrows. The calculations can be broken down into three steps. In the first step, the samples of the forward SDE are calculated. The information used in this step is represented by the dotted lines. For instance, to calculate\\nX 2 , (t 1 , ‚àÜW 1 , X 1 ) is used, and (t N ‚àí1 , ‚àÜW N ‚àí1 , X N ‚àí1 ) for X N . The second step is to calculate the values (Y Œ∏ i , Z Œ∏ i ) for i = 0, 1, ‚Ä¢ ‚Ä¢ ‚Ä¢ , N ‚àí 1\\nusing a deep neural network (DNN) and the AD. The information needed for such calculations is represented by the solid lines. For example, the DNN uses as input (t 1 , X 1 ) to calculate Y Œ∏ 1 . Using the AD we calculate the gradient in the spatial direction to obtain Z Œ∏ 1 . Finally, the local losses are calculated backwardly with the information presented by the dashed lines.\\nTo calculate L N ‚àí1 , the terminal condition Y N = g(X N ) and (t\\nN ‚àí1 , ‚àÜW N ‚àí1 , X N ‚àí1 , Y Œ∏ N ‚àí1 , Z Œ∏ N ‚àí1 ) are used. For L N ‚àí2 , (t N ‚àí2 , ‚àÜW N ‚àí2 , X N ‚àí2 , Y Œ∏ N ‚àí2 , Z Œ∏ N ‚àí2\\n) and the information form\\nL N ‚àí1 are used, namely Y N and (t N ‚àí1 , ‚àÜW N ‚àí1 , X N ‚àí1 , Y Œ∏ N ‚àí1 )\\n. The same holds for the other loss terms. We use a backward implementation of the local loss functions because it is more efficient than their forward implementation. The forward and backward implementations of (9) for one sample are given in Algorithm 1 and 2, respectively. With Algorithm 1 the computation time of LaDBSDE Algorithm 1 A forward implementation of the loss function (9)\\nData: (t i , ‚àÜW i , X i , Y Œ∏ i , Z Œ∏ i ) 0‚â§i‚â§N ‚àí1 , X N , ‚àÜt Result: L L ‚Üê 0 for i = 0 : N ‚àí 1 d√µ Y Œ∏ i ‚Üê g(X N ) for j = i : N ‚àí 1 d√µ Y Œ∏ i ‚Üê·ª∏ Œ∏ i + f (t j , X j , Y Œ∏ j , Z Œ∏ j )‚àÜt ‚àí Z Œ∏ j ‚àÜW j end for L i ‚Üê |Y Œ∏ i ‚àí·ª∏ Œ∏ i | 2 L ‚Üê L + L i end for\\nis comparable to that of LDBSDE.\\nWe consider a similar network architecture as in [Raissi, 2018]. Based on Framework 3.1 we Algorithm 2 A backward implementation of the loss function (9)\\nData: (t i , ‚àÜW i , X i , Y Œ∏ i , Z Œ∏ i ) 0‚â§i‚â§N ‚àí1 , X N , ‚àÜt Result: L L ‚Üê 0 Y Œ∏ N ‚Üê g(X N ) for i = N ‚àí 1 : 0 d√µ Y Œ∏ i ‚Üê·ª∏ Œ∏ i+1 + f (t i , X i , Y Œ∏ i , Z Œ∏ i )‚àÜt ‚àí Z Œ∏ i ‚àÜW i end for for i = 0 : N ‚àí 1 do L i ‚Üê |Y Œ∏ i ‚àí·ª∏ Œ∏ i | 2 L ‚Üê L + L i end for\\nrequire to optimize over differentiable deep neural networks, and using the classical rectifier function may lead to an explosion while calculating the numerical approximation of the Z process. We consider R\\nx ‚Üí tanh(x) ‚àà [‚àí1, 1]. Moreover, using L = 4 hidden layers and n = 10 + d neurons for the hidden layers is enough, increasing L or n does not improve the accuracy in our tests. The dimension of the parameters is given in Remark 3.1. = 2d 2 + 56d + 361.\\n(10)\\nCompared to the complexity (8) given in [Raissi, 2018], our parametrization of the neural network gives a smaller complexity (10). For instance, considering an example in d = 100, the complexity based on equation ( 10) is decreased with a factor around 9 when compared to (8). In order to further reduce the computation time compared to the learning approach given in [Raissi, 2018], we consider a learning rate decay optimization approach based on the relative magnitude of the loss function [Chan-Wai-Nam et al., 2019]. We start with a learning rate Œ≥ 0 . For each 1000 optimization steps, we evaluate the loss every 100 steps on a validation size of 1024. Then we can take the average of 10 collected loss values. If the relative loss over two consecutive periods is less than 5%, we have reached a loss plateau and reduce the learning rate by half. To avoid using very small learning rates, we set a threshold Œ≥ min . If the loss value doesn't decrease any more, the learning process is terminated. Otherwise, we continue until 60000 optimization steps. The hyperparameter values used for all schemes are reported in Table 1, which give the best approximations in each scheme in our numerical experiments.  \",\n",
       "    'n_publication_ref': 4,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Deep learning algorithms for solving high dimensional nonlinear backward stochastic differential equations',\n",
       "  'abstract': 'In this work, we propose a new deep learning-based scheme for solving high dimensional nonlinear backward stochastic differential equations (BSDEs). The idea is to reformulate the problem as a global optimization, where the local loss functions are included. Essentially, we approximate the unknown solution of a BSDE using a deep neural network and its gradient with automatic differentiation. The approximations are performed by globally minimizing the quadratic local loss function defined at each time step, which always includes the terminal condition. This kind of loss functions are obtained by iterating the Euler discretization of the time integrals with the terminal condition. Our formulation can prompt the stochastic gradient descent algorithm not only to take the accuracy at each time layer into account, but also converge to a good local minima. In order to demonstrate performances of our algorithm, several high-dimensional nonlinear BSDEs including pricing problems in finance are provided.',\n",
       "  'paddleOCR': [[[[197.0, 26.0], [236.0, 26.0], [236.0, 53.0], [197.0, 53.0]],\n",
       "    ['Lo', 0.932504415512085]],\n",
       "   [[[424.0, 26.0], [462.0, 26.0], [462.0, 53.0], [424.0, 53.0]],\n",
       "    ['L1', 0.9885433912277222]],\n",
       "   [[[639.0, 22.0], [677.0, 27.0], [674.0, 54.0], [636.0, 50.0]],\n",
       "    ['L2', 0.9344176650047302]],\n",
       "   [[[1072.0, 21.0], [1145.0, 28.0], [1142.0, 59.0], [1070.0, 53.0]],\n",
       "    ['LN-1', 0.9859937429428101]],\n",
       "   [[[87.0, 133.0], [115.0, 133.0], [115.0, 158.0], [87.0, 158.0]],\n",
       "    ['Zb', 0.6668518781661987]],\n",
       "   [[[76.0, 248.0], [129.0, 248.0], [129.0, 274.0], [76.0, 274.0]],\n",
       "    ['AD', 0.9990790486335754]],\n",
       "   [[[309.0, 248.0], [360.0, 248.0], [360.0, 275.0], [309.0, 275.0]],\n",
       "    ['AD', 0.9985960721969604]],\n",
       "   [[[537.0, 248.0], [586.0, 248.0], [586.0, 275.0], [537.0, 275.0]],\n",
       "    ['AD', 0.9991921782493591]],\n",
       "   [[[965.0, 248.0], [1018.0, 248.0], [1018.0, 274.0], [965.0, 274.0]],\n",
       "    ['AD', 0.9990767240524292]],\n",
       "   [[[1241.0, 369.0], [1284.0, 375.0], [1279.0, 404.0], [1237.0, 398.0]],\n",
       "    ['YN', 0.9547613859176636]],\n",
       "   [[[1255.0, 410.0], [1271.0, 410.0], [1271.0, 432.0], [1255.0, 432.0]],\n",
       "    ['A', 0.9869393110275269]],\n",
       "   [[[62.0, 489.0], [141.0, 489.0], [141.0, 518.0], [62.0, 518.0]],\n",
       "    ['DNN', 0.9981791377067566]],\n",
       "   [[[295.0, 489.0], [374.0, 489.0], [374.0, 518.0], [295.0, 518.0]],\n",
       "    ['DNN', 0.9986762404441833]],\n",
       "   [[[523.0, 492.0], [598.0, 492.0], [598.0, 517.0], [523.0, 517.0]],\n",
       "    ['DNN', 0.9979645609855652]],\n",
       "   [[[951.0, 489.0], [1032.0, 489.0], [1032.0, 518.0], [951.0, 518.0]],\n",
       "    ['DNN', 0.9984519481658936]],\n",
       "   [[[542.0, 608.0], [577.0, 608.0], [577.0, 637.0], [542.0, 637.0]],\n",
       "    ['X', 0.6234018802642822]],\n",
       "   [[[953.0, 603.0], [1024.0, 611.0], [1021.0, 642.0], [950.0, 635.0]],\n",
       "    ['I-Nx', 0.849035382270813]],\n",
       "   [[[1241.0, 610.0], [1281.0, 610.0], [1281.0, 637.0], [1241.0, 637.0]],\n",
       "    ['XN', 0.8460390567779541]],\n",
       "   [[[74.0, 708.0], [135.0, 713.0], [132.0, 745.0], [71.0, 739.0]],\n",
       "    ['AWo', 0.7987833023071289]],\n",
       "   [[[306.0, 709.0], [367.0, 715.0], [364.0, 746.0], [303.0, 741.0]],\n",
       "    ['AW!', 0.9526393413543701]],\n",
       "   [[[531.0, 711.0], [590.0, 715.0], [588.0, 745.0], [529.0, 741.0]],\n",
       "    ['AW2', 0.8818182349205017]],\n",
       "   [[[945.0, 708.0], [1031.0, 715.0], [1028.0, 749.0], [942.0, 742.0]],\n",
       "    ['AWR-1', 0.8204941749572754]],\n",
       "   [[[92.0, 829.0], [112.0, 829.0], [112.0, 847.0], [92.0, 847.0]],\n",
       "    ['to', 0.9854860305786133]],\n",
       "   [[[323.0, 826.0], [342.0, 826.0], [342.0, 847.0], [323.0, 847.0]],\n",
       "    ['t1', 0.9674326181411743]],\n",
       "   [[[550.0, 826.0], [571.0, 826.0], [571.0, 847.0], [550.0, 847.0]],\n",
       "    ['t2', 0.9822913408279419]],\n",
       "   [[[963.0, 824.0], [1013.0, 830.0], [1010.0, 852.0], [960.0, 846.0]],\n",
       "    ['tN-1', 0.961844265460968]]],\n",
       "  'ocr': [[[295.0, 489.0], [374.0, 489.0], [374.0, 518.0], [295.0, 518.0]],\n",
       "   ['DNN', 0.9986762404441833]]},\n",
       " '113902-Figure1-1.png': {'caption': 'Fig. 1. The architecture of the proposed semantic tracker, which contains a shared convolutional network (NetS), a classification network (NetC) and a tracking network (NetT).',\n",
       "  'imageText': [],\n",
       "  'image_file': '113902-Figure1-1.png',\n",
       "  'sections': [{'heading': 'INTRODUCTION',\n",
       "    'text': 'Visual object tracking has actively been researched for several decades. Depending on the prior information about the target category, the tracking algorithms are usually classified as categoryfree methods, like KCF [14], Struck [13], LGT [30], and categorybased methods, like human tracking [32], vehicle tracking [2], hand tracking [26]. The category-free tracking methods are acknowledged for their simple initialisation (a single bounding box) and easy generalisation across different object categories. They have extensively been studied and compared [39], [15]. However, as those methods have no prior information about the target inside the bounding box, the tracking performance heavily depends on the heuristic assumptions of image regions, i.e., appearance consistency [42] and motion consistency [5], which fail when those assumptions are not met. In contrast, the category-based methods benefit from the prior information about the target and can better adjust the target model and predict its dynamics or appearance variations during tracking. Those category-based methods can achieve superior performance on a specific category but usually have difficulties being generalised to other object categories. As many sophisticated machine learning algorithms have recently been adopted for tracking [21], [35], [38], an interesting question is whether we can build a semantic tracker, based on those methods, to bridge the gap between the category-free tracking methods and category-based tracking methods (see Tab. 1). Early attempts to track and recognise the objects simultaneously were done by [19], [9], [43]. However, the aforementioned works were developed using conventional hand-crafted features, which have difficulties of being scaled up. Inspired by the recent success of convolutional networks [16], we propose, in this article, a semantic tracker with a unified convolutional framework which encodes generic features across different object categories while also captures category-based features for model adaptation during tracking. With the help of the category-classification network, the semantic tracker can avoid heuristic assumptions about the tracked objects.\\nThe proposed semantic tracker comprises three stages: off-line training, online tracking, and network adaptation. It consists of a shared convolutional network (NetS), a classification network (NetC) and a tracking network (NetT), see Fig. 1. In the offline training stage, NetS is pre-trained from ImageNet to extract generic features across different object categories. Those features are then fed into NetC for classification and NetT for tracking. Note that NetT has multiple branches to distinguish the tracked TABLE 1 Relationships among category-free, category-based methods and the proposed semantic tracking. Category-based methods and the proposed semantic tracking encompass off-line category-specific training processes whereas the category-free methods do not. During online tracking, only the category-based methods know the target category from the initialisation stage while the proposed semantic tracking algorithm simultaneously recognises and tracks the target on-the-fly. object from the background. Since each branch is trained by the videos of a specific object category, this enables each branch in NetT to learn the category-specific features related to both foreground and background, e.g., when tracking a pedestrian, it is more likely to learn the features of a car in the background than features of a fish. During online tracking, NetC first recognises the object category and activates the corresponding branch in NetT. Then, NetT is automatically fine-tuned for that particular tracking video by exploiting the foreground and the background sample regions in the first frame. When a new image frame arrives, the algorithm samples a set of image regions and each sample is fed through both NetC and NetT. The regions with the right category and the foreground label are used for target estimation (i.e., the location and the size of the target bounding box). Note that the target appearance often changes during the tracking, therefore it is extremely crucial for a tracker to adapt the model accordingly.\\nTo improve the robustness and precision, NetC and NetT intersupervise each other and trigger network adaptation when their outputs are ambiguous (i.e., not consistent) for several image regions, e.g., when an image region is classified as a non-target category from NetC but as foreground from NetT or as a target category from NetC and background from NetT. The samples with consistent labellings are used to update the networks which also results in a reduced number of ambiguous sample regions.\\nWe have evaluated the contribution of each key component to the overall performance on OTB tracking benchmark [39] (100 sequences), and also compared the whole algorithm to the other state-of-the-art single-target tracking algorithms. The experimental results demonstrate the effectiveness of our algorithm as it outperformed other 38 state-of-the-art tracking algorithms not only overall, but also on the sub-datasets annotated with specific attributes. Different from conventional category-free and category-based trackers, the main contributions of our semantic tracker can be summarised as: 1) Our tracker simultaneously tracks a single target and recognises its category using convolutional networks, which alleviates the problems with heuristic assumptions about the targets; 2) A novel unified framework with NetS network, which extracts generic features across different object categories, combined with NetC and NetT networks which encode category-based features; 3) NetC and NetT jointly determine image samples for estimation of the target, and inter-supervise each other by triggering network adaptation to improve robustness and precision.\\nThe rest of the paper is organised as follows. We first review related work in Sec. 2. The details of the proposed method are provided in Sec. 3. Sec. 4 presents and discusses the experimental results on a tracking benchmark [39]. Sec. 5 provides concluding remarks.',\n",
       "    'n_publication_ref': 20,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Recent research has shown the relationship between the human vision system and deep hierarchies in computer vision',\n",
       "    'text': 'Recent research [21] has shown that shallow layers in CNN contain more generic information while deep layers are more related to semantic information. Thus, our tracker consists of shared convolutional layers to extract generic features in the shallow network (NetS), followed by NetC network for classification and NetT network for extracting category-based features for tracking. Note that NetS extracts generic features across different object categories, where those features have some common properties, e.g., robustness to scale and orientation changes, and illumination variations [24], which can be useful for other higher level tasks. Therefore, those extracted generic features are fed into NetC and NetT for more semantic related tasks. NetC is a multiclass classification network to recognise the object category. NetT, which is a binary classification network, aims at distinguishing foreground region (target) from the background. Considering that the images of tracked objects of the same category often contain characteristic features both in terms of the foreground as well as the background, but which are different from other categories, e.g., when tracking a pedestrian it is more likely to have cars in the background than fish, NetT comprises multiple category-based branches, and each branch is particularly trained from the videos that contain the same object category. During on-line tracking, NetC and NetT inter-supervise each other by triggering network adaptation to improve robustness and precision, shown in Fig. 1. The details of the network structure are shown in Tab. 2.',\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Semantic tracking: Single-target tracking with inter-supervised convolutional networks',\n",
       "  'abstract': 'This article presents a semantic tracker which simultaneously tracks a single target and recognises its category. In general, it is hard to design a tracking model suitable for all object categories, e.g., a rigid tracker for a car is not suitable for a deformable gymnast. Category-based trackers usually achieve superior tracking performance for the objects of that specific category, but have difficulties being generalised. Therefore, we propose a novel unified robust tracking framework which explicitly encodes both generic features and category-based features. The tracker consists of a shared convolutional network (NetS), which feeds into two parallel networks, NetC for classification and NetT for tracking. NetS is pre-trained on ImageNet to serve as a generic feature extractor across the different object categories for NetC and NetT. NetC utilises those features within fully connected layers to classify the object category. NetT has multiple branches, corresponding to multiple categories, to distinguish the tracked object from the background. Since each branch in NetT is trained by the videos of a specific category or groups of similar categories, NetT encodes categorybased features for tracking. During online tracking, NetC and NetT jointly determine the target regions with the right category and foreground labels for target estimation. To improve the robustness and precision, NetC and NetT inter-supervise each other and trigger network adaptation when their outputs are ambiguous for the same image regions (i.e., when the category label contradicts the foreground/background classification). We have compared the performance of our tracker to other state-of-the-art trackers on a large-scale tracking benchmark [39] (100 sequences)-the obtained results demonstrate the effectiveness of our proposed tracker as it outperformed other 38 state-of-the-art tracking algorithms.',\n",
       "  'paddleOCR': [[[[699.0, 30.0], [845.0, 30.0], [845.0, 54.0], [699.0, 54.0]],\n",
       "    ['Adaptation', 0.9994298815727234]],\n",
       "   [[[3.0, 41.0], [132.0, 45.0], [131.0, 69.0], [2.0, 66.0]],\n",
       "    ['k-1 frame', 0.9534018039703369]],\n",
       "   [[[472.0, 80.0], [655.0, 80.0], [655.0, 103.0], [472.0, 103.0]],\n",
       "    ['NetC network', 0.9593751430511475]],\n",
       "   [[[743.0, 101.0], [858.0, 152.0], [846.0, 180.0], [731.0, 129.0]],\n",
       "    ['Category', 0.9992290735244751]],\n",
       "   [[[449.0, 114.0], [677.0, 114.0], [677.0, 137.0], [449.0, 137.0]],\n",
       "    ['(Classification network)', 0.9869237542152405]],\n",
       "   [[[928.0, 105.0], [1022.0, 105.0], [1022.0, 129.0], [928.0, 129.0]],\n",
       "    ['Results', 0.9995811581611633]],\n",
       "   [[[29.0, 166.0], [132.0, 166.0], [132.0, 190.0], [29.0, 190.0]],\n",
       "    ['k frame', 0.9965890645980835]],\n",
       "   [[[528.0, 193.0], [749.0, 193.0], [749.0, 217.0], [528.0, 217.0]],\n",
       "    ['Inter-supervision', 0.9995843172073364]],\n",
       "   [[[468.0, 267.0], [651.0, 270.0], [651.0, 294.0], [467.0, 292.0]],\n",
       "    ['NetT network', 0.998752772808075]],\n",
       "   [[[122.0, 277.0], [300.0, 277.0], [300.0, 301.0], [122.0, 301.0]],\n",
       "    ['NetS network', 0.9978907704353333]],\n",
       "   [[[77.0, 314.0], [345.0, 314.0], [345.0, 338.0], [77.0, 338.0]],\n",
       "    ['(Extracting generic features)', 0.9885033369064331]],\n",
       "   [[[468.0, 306.0], [651.0, 306.0], [651.0, 329.0], [468.0, 329.0]],\n",
       "    ['(Tracking network)', 0.9469186067581177]],\n",
       "   [[[681.0, 357.0], [839.0, 354.0], [839.0, 378.0], [682.0, 381.0]],\n",
       "    ['Adaptation', 0.9992402195930481]]],\n",
       "  'ocr': [[[122.0, 277.0], [300.0, 277.0], [300.0, 301.0], [122.0, 301.0]],\n",
       "   ['NetS network', 0.9978907704353333]]},\n",
       " '177535-Figure7-1.png': {'caption': 'Figure 7. Network architecture for our image restoration framework. Our image restoration framework is an encoder-decoder pipeline with the encoder and decoder connected by a channel-wise fully-connected layer. The illustration is for image inpainting task. The same network architecture also holds for the other three tasks: pixel interpolation, image deblurring, and image denoising.',\n",
       "  'imageText': ['(reshape)',\n",
       "   '(reshape)',\n",
       "   'Channel-wise',\n",
       "   'Fully',\n",
       "   'Connected',\n",
       "   '8192',\n",
       "   '8192',\n",
       "   '64',\n",
       "   '64',\n",
       "   '64',\n",
       "   '64',\n",
       "   'Encoder',\n",
       "   'Decoder',\n",
       "   '32',\n",
       "   '64',\n",
       "   '32',\n",
       "   '16',\n",
       "   '128',\n",
       "   '16',\n",
       "   '8',\n",
       "   '4',\n",
       "   '256',\n",
       "   '8',\n",
       "   '4',\n",
       "   '4',\n",
       "   '512',\n",
       "   '4',\n",
       "   '512',\n",
       "   '8',\n",
       "   '256',\n",
       "   '8',\n",
       "   '16',\n",
       "   '128',\n",
       "   '16',\n",
       "   '32',\n",
       "   '64',\n",
       "   '32'],\n",
       "  'image_file': '177535-Figure7-1.png',\n",
       "  'sections': [{'heading': 'Deep Learning Network Architecture',\n",
       "    'text': 'Finally, we present the network architecture used for all tasks to implement our on-demand learning idea. Our image restoration network is a simple encoder-decoder pipeline. See Fig. 7. The encoder takes a corrupted image C of size 64 √ó 64 as input and encodes it in the latent feature space. The decoder takes the feature representation and outputs the quality in image restoration tasks. We found PSNR to be superior to an L2 loss; because it is normalized by the max possible power and expressed in log scale, it is better than L2 at comparing across difficulty levels. restored image f (C, w). Our encoder and decoder are connected through a channel-wise fully-connected layer. The loss function we use during training is L2 loss, which is the mean squared error between the restored image f (C, w) and the real image R. We use a symmetric encoder-decoder pipeline that is efficient for training and effective for learning. It is a unified framework that can be used for all four image restoration tasks. Please see Supp. for the complete network architecture and detailed design choices.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Conclusion',\n",
       "    'text': 'We have addressed a common problem in existing work that leverages deep models to solve image restoration tasks: overfitting. We devise a symmetric encoder-decoder network amenable to all image restoration tasks, and propose a simple but novel on-demand learning algorithm that turns a fixated model into one that performs well on a task across the spectrum of difficulty. Experiments on four tasks on three diverse datasets demonstrate the effectiveness of our method. Our on-demand learning idea is a general concept not restricted to image restoration tasks, and may be applicable in other domains as well, e.g., self-supervised feature learning. As future work, we plan to design continuous subtasks to avoid discrete sub-task bins, and we will explore ways to make an image restoration task more self-paced by allowing the network to design the most desired sub-task on its own. Finally, another promising direction is to explore combinations of different types of distortions. Fig. 7 shows the complete network architecture used for all tasks to implement our on-demand learning idea. Our image restoration network is a symmetric encoder-decoder pipeline. The encoder takes a corrupted image of size 64 √ó 64 as input and encodes it in the latent feature space. The decoder takes the feature representation and outputs the restored image. Our encoder and decoder are connected through a channel-wise fully-connected layer.\\nSpecifically, for our encoder, we use four convolutional layers. Following similar design choices in DCGAN [37], we put a batch normalization layer [17] after each convolutional layer to accelerate training and stabilize learning. The leaky rectified linear unit (LeakyReLU) activation [33,46] is used in all layers in the encoder.\\nThe four convolutional layers in the encoder only connect all the feature maps together, but there are no direct connections among different locations within each specific feature map. Fully-connected layers are usually used to handle this information propagation in present successful network architectures [23,40]. In our network, the latent feature dimension is 4 √ó 4 √ó 512 = 8192 for both encoder and decoder. Fully-connecting our encoder and decoder will increase the number of parameters explosively. To more efficiently train our network and demonstrate our concept, we use a channel-wise fully-connected layer to connect the encoder and decoder, as in [35]. The channel-wise fully-connected layer is designed to only propagate information within activations of each feature map. In our case, each 4√ó4 feature map in the encoder side is fully-connected with each 4√ó4 feature map in the decoder side. This largely reduces the number of parameters in our network and accelerates training significantly.\\nThe decoder consists of four up-convolutional layers [32,9,49], each of which is followed by a rectified linear unit (ReLU) activation except the output layer. We use the Tanh function in the output layer, and the output is of the same size as the input image. The series of up-convolutions and non-linearities conducts a non-linear weighted upsampling of the feature produced by the encoder and generates a higher resolution image of our target size (64 √ó 64).',\n",
       "    'n_publication_ref': 10,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'On-Demand Learning for Deep Image Restoration',\n",
       "  'abstract': 'While machine learning approaches to image restoration offer great promise, current methods risk training models fixated on performing well only for image corruption of a particular level of difficulty-such as a certain level of noise or blur. First, we examine the weakness of conventional \"fixated\" models and demonstrate that training general models to handle arbitrary levels of corruption is indeed non-trivial. Then, we propose an on-demand learning algorithm for training image restoration models with deep convolutional neural networks. The main idea is to exploit a feedback mechanism to self-generate training instances where they are needed most, thereby learning models that can generalize across difficulty levels. On four restoration tasks-image inpainting, pixel interpolation, image deblurring, and image denoising-and three diverse datasets, our approach consistently outperforms both the status quo training procedure and curriculum learning alternatives.',\n",
       "  'paddleOCR': [[[[949.0, 7.0], [1022.0, 7.0], [1022.0, 40.0], [949.0, 40.0]],\n",
       "    ['8192', 0.9999887347221375]],\n",
       "   [[[1079.0, 7.0], [1152.0, 7.0], [1152.0, 40.0], [1079.0, 40.0]],\n",
       "    ['8192', 0.9999921321868896]],\n",
       "   [[[1763.0, 21.0], [1798.0, 21.0], [1798.0, 52.0], [1763.0, 52.0]],\n",
       "    ['64', 0.9999397397041321]],\n",
       "   [[[374.0, 47.0], [409.0, 47.0], [409.0, 75.0], [374.0, 75.0]],\n",
       "    ['64', 0.9999752640724182]],\n",
       "   [[[1698.0, 56.0], [1738.0, 56.0], [1738.0, 85.0], [1698.0, 85.0]],\n",
       "    ['32', 0.9999680519104004]],\n",
       "   [[[312.0, 80.0], [347.0, 80.0], [347.0, 108.0], [312.0, 108.0]],\n",
       "    ['32', 0.9999537467956543]],\n",
       "   [[[1613.0, 72.0], [1665.0, 65.0], [1668.0, 93.0], [1617.0, 100.0]],\n",
       "    ['128', 0.9999670386314392]],\n",
       "   [[[511.0, 82.0], [555.0, 82.0], [555.0, 108.0], [511.0, 108.0]],\n",
       "    ['128', 0.9999474883079529]],\n",
       "   [[[637.0, 97.0], [694.0, 97.0], [694.0, 132.0], [637.0, 132.0]],\n",
       "    ['256', 0.9999196529388428]],\n",
       "   [[[1435.0, 92.0], [1486.0, 92.0], [1486.0, 120.0], [1435.0, 120.0]],\n",
       "    ['256', 0.9999579787254333]],\n",
       "   [[[1550.0, 99.0], [1588.0, 99.0], [1588.0, 125.0], [1550.0, 125.0]],\n",
       "    ['16', 0.999691367149353]],\n",
       "   [[[447.0, 115.0], [471.0, 115.0], [471.0, 132.0], [447.0, 132.0]],\n",
       "    ['16', 0.9973399639129639]],\n",
       "   [[[825.0, 122.0], [876.0, 122.0], [876.0, 148.0], [825.0, 148.0]],\n",
       "    ['512', 0.9999809265136719]],\n",
       "   [[[1241.0, 125.0], [1285.0, 125.0], [1285.0, 144.0], [1241.0, 144.0]],\n",
       "    ['512', 0.999819815158844]],\n",
       "   [[[1387.0, 122.0], [1398.0, 122.0], [1398.0, 137.0], [1387.0, 137.0]],\n",
       "    ['8', 0.6640245914459229]],\n",
       "   [[[7.0, 162.0], [49.0, 162.0], [49.0, 191.0], [7.0, 191.0]],\n",
       "    ['64', 0.9999354481697083]],\n",
       "   [[[2070.0, 170.0], [2110.0, 170.0], [2110.0, 200.0], [2070.0, 200.0]],\n",
       "    ['64', 0.9999486804008484]],\n",
       "   [[[285.0, 224.0], [321.0, 224.0], [321.0, 252.0], [285.0, 252.0]],\n",
       "    ['32', 0.9999683499336243]],\n",
       "   [[[840.0, 233.0], [962.0, 233.0], [962.0, 259.0], [840.0, 259.0]],\n",
       "    ['(reshape', 0.9669074416160583]],\n",
       "   [[[1128.0, 233.0], [1252.0, 233.0], [1252.0, 259.0], [1128.0, 259.0]],\n",
       "    ['(reshape)', 0.99272221326828]],\n",
       "   [[[1681.0, 231.0], [1701.0, 231.0], [1701.0, 245.0], [1681.0, 245.0]],\n",
       "    ['3', 0.6796152591705322]],\n",
       "   [[[135.0, 285.0], [175.0, 285.0], [175.0, 315.0], [135.0, 315.0]],\n",
       "    ['64', 0.9999604225158691]],\n",
       "   [[[1946.0, 285.0], [1984.0, 285.0], [1984.0, 315.0], [1946.0, 315.0]],\n",
       "    ['64', 0.9999800324440002]],\n",
       "   [[[955.0, 355.0], [1139.0, 355.0], [1139.0, 386.0], [955.0, 386.0]],\n",
       "    ['Channel-wise', 0.999610185623169]],\n",
       "   [[[938.0, 393.0], [1157.0, 393.0], [1157.0, 424.0], [938.0, 424.0]],\n",
       "    ['Fully Connected', 0.9998568892478943]],\n",
       "   [[[506.0, 407.0], [661.0, 407.0], [661.0, 447.0], [506.0, 447.0]],\n",
       "    ['Encoder', 0.9998456239700317]],\n",
       "   [[[1471.0, 407.0], [1625.0, 407.0], [1625.0, 447.0], [1471.0, 447.0]],\n",
       "    ['Decoder', 0.9998184442520142]]],\n",
       "  'ocr': [[[1698.0, 56.0], [1738.0, 56.0], [1738.0, 85.0], [1698.0, 85.0]],\n",
       "   ['32', 0.9999680519104004]]},\n",
       " '2202.03158v1-Figure2-1.png': {'caption': 'Fig. 2: The architecture of dual-CLVSA. We add another sequence-to-sequence framework to train historical sentiment data, compared to the original CLVSA. We concatenate the outputs of the two sequence-to-sequence framework before projection layers. We do not apply another set of Kullback-Leibler divergence (KLD) for the channel of sentiment data because of the impulsive characteristic of sentiment data.',\n",
       "  'imageText': ['Encoder',\n",
       "   'Decoder',\n",
       "   'Dual-CLVSA',\n",
       "   'Encoder',\n",
       "   'Decoder',\n",
       "   'Fully',\n",
       "   'Connected',\n",
       "   'Layers',\n",
       "   'Historical',\n",
       "   'Trading',\n",
       "   'Data',\n",
       "   'Historical',\n",
       "   'Sentiment',\n",
       "   'Data',\n",
       "   'Concatenation',\n",
       "   'Inter-attn',\n",
       "   'Conv-LSTM',\n",
       "   'wtih',\n",
       "   'Self-attnInter-attn',\n",
       "   'Conv-LSTM',\n",
       "   'wtih',\n",
       "   'Self-attn',\n",
       "   'Conv-LSTM',\n",
       "   'wtih',\n",
       "   'Self-attn',\n",
       "   'Conv-LSTM',\n",
       "   'wtih',\n",
       "   'Self-attn'],\n",
       "  'image_file': '2202.03158v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'C. dual-CLVSA: the fusion method',\n",
       "    'text': 'The base approach of our fusion method, CLVSA, is a hybrid model consisting of convolutional LSTM units, sequence-to-sequence framework with attention mechanism, and stochastic recurrent networks, schematically shown in Figure 1. The encoder and decoder of the sequence-to-sequence framework take 2-D frames of historical trading data of two consecutive days as input, respectively. The inter-attention module highlights parts of the first one of two consecutive days as the context of the second day. The convolutional LSTM units of the encoder and decoder process 2-D data frames in two steps: i) Convolutional kernels capture local features, ii) Based on the local features, LSTM networks capture temporal features with gated recurrent networks. In each layer of the encoder and decoder, a self-attention module is utilized to highlight parts of the sequence of daily data frames.\\nFrom the above description, we can see that convolutional kernels play a fundamental role in CLVSA. The convolutional kernels operate directly on input data, so the other parts, such as LSTM units and attention layers, work based on local features extracted by convolutional kernels. However, as demonstrated in [11], Cross-Data-Type 1-D Convolution (CDT 1-D Convolution) is applied as convolutional kernels in CLVSA to accommodate the characteristics of historical trading data, which is comprised of five elements: Open, High, Low, Close prices, and Volume. However, there is a prerequisite to use CDT 1-D Convolution, that is, all elements should have strong relevance with each other (e.g. prices and volume under financial markets) so they can share parameters. Our experimental results show that the performance of CLVSA with a direct fusion of TRMI data and historical SPY trading data (CLV SA 2 in Table I) degrades by 18.5%, and 1.01 for average annual return (AAR), Sharpe ratio (SR), respectively, compared to CLVSA with historical SPY trading data only (CLV SA 1 ).\\nTo solve this problem, we propose a dual-CLVSA model to fuse TRMI data and historical trading data. The architecture of dual-CLVSA is illustrated in Figure 2. The basic idea is that, we assign two separate sequence-to-sequence framework to TRMI data and historical trading data, respectively. The two channels are not fused until outputs of decoders from the two channels are concatenated and fed into fully connected layers. On one hand, two separate channels avoid mix-up on convolutions as they have different characteristics; on the other hand, the two channels are combined after individual sequence-to-sequence framework, guaranteeing that the two independent sets of features are processed with the same weight in the fully connected layers. We do not apply another set of Kullback-Leibler divergence (KLD) for the channel of TRMI data because of the sporadic characteristic of sentiment data.',\n",
       "    'n_publication_ref': 1,\n",
       "    'n_figure_ref': 2},\n",
       "   {'heading': 'B. Experimental results of LST M s',\n",
       "    'text': 'The baseline method at the first step aims to verify the informativeness of sentiment data. We use recurrent neural network with LSTM units (LST M s ) to train and test the following four types of datasets, SPY historical trading data, SPY historical trading data with technical indicators, SPY historical trading data with sentiment data, SPY historical trading data with technical indicators and sentiment data. We named the above four experimental sessions as LST M 1 s to LST M 4 s , respectively. Table I shows their experimental results.\\nSince LST M s is designed for temporal feature extraction, which lacks the capability of local feature extraction. Consequently, the experimental results of LST M 1 s shows severe losses, -19.9% average annual return (AAR). LST M 2 s , however, stays positive and achieves an AAR of 34.2%. The significant difference between the above two experiments demonstrates that technical indicators provide informative local features to LST M s .\\nThe experimental results of LST M 3 s show a positive AAR of 32.8% as well. Although it works slightly worse than the experiment with technical indicators, the performance is significantly better than the experiment with historical SPY trading data only (LST M 1 s ). This result verifies that TRMI data is able to provide informative features as technical indicators.\\nThe experiments of LST M 4 s show interesting results. Compared to the aforementioned two experiments, the AAR drops to 28.9%, indicating that technical indicators and TRMI data can not be fused directly although both of them contain informative features. We also observe similar results in the experiments of CLVSA with SPY historical trading data and TRMI data, which is demonstrated in the next section. Fig. 2: The architecture of dual-CLVSA. We add another sequence-to-sequence framework to train historical sentiment data, compared to the original CLVSA. We concatenate the outputs of the two sequence-to-sequence framework before projection layers. We do not apply another set of Kullback-Leibler divergence (KLD) for the channel of sentiment data because of the impulsive characteristic of sentiment data.\\nFig. 3: The cumulative and monthly return of SPY by dual-CLVSA with historical SPY trading data and TRMI data.\\nWhile the experiments show the informativeness of TRMI data, the mediocre performance of LST M s with either TRMI data or the mixed data indicates that LST M s may not be the optimal framework to take advantages of TRMI data.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Dual-CLVSA: a Novel Deep Learning Approach to Predict Financial Markets with Sentiment Measurements',\n",
       "  'abstract': 'It is a challenging task to predict financial markets. The complexity of this task is mainly due to the interaction between financial markets and market participants, who are not able to keep rational all the time, and often affected by emotions such as fear and ecstasy. Based on the state-ofthe-art approach particularly for financial market predictions, a hybrid convolutional LSTM Based variational sequence-tosequence model with attention (CLVSA), we propose a novel deep learning approach, named dual-CLVSA, to predict financial market movement with both trading data and the corresponding social sentiment measurements, each through a separate sequence-to-sequence channel. We evaluate the performance of our approach with backtesting on historical trading data of SPDR SP 500 Trust ETF over eight years. The experiment results show that dual-CLVSA can effectively fuse the two types of data, and verify that sentiment measurements are not only informative for financial market predictions, but they also contain extra profitable features to boost the performance of our predicting system.',\n",
       "  'paddleOCR': [[[[712.0, 0.0], [869.0, 0.0], [869.0, 20.0], [713.0, 22.0]],\n",
       "    ['Dual-CLVSA', 0.9881378412246704]],\n",
       "   [[[4.0, 10.0], [259.0, 10.0], [259.0, 37.0], [4.0, 37.0]],\n",
       "    ['Historical Sentiment', 0.9688959121704102]],\n",
       "   [[[96.0, 45.0], [163.0, 45.0], [163.0, 75.0], [96.0, 75.0]],\n",
       "    ['Data', 0.9998321533203125]],\n",
       "   [[[385.0, 45.0], [498.0, 45.0], [498.0, 73.0], [385.0, 73.0]],\n",
       "    ['Encoder', 0.9998544454574585]],\n",
       "   [[[1084.0, 45.0], [1196.0, 45.0], [1196.0, 73.0], [1084.0, 73.0]],\n",
       "    ['Decoder', 0.9997797608375549]],\n",
       "   [[[361.0, 124.0], [518.0, 124.0], [518.0, 153.0], [361.0, 153.0]],\n",
       "    ['Conv-LSTM', 0.9984481334686279]],\n",
       "   [[[1062.0, 124.0], [1218.0, 124.0], [1218.0, 153.0], [1062.0, 153.0]],\n",
       "    ['Conv-LSTM', 0.9977900385856628]],\n",
       "   [[[357.0, 159.0], [528.0, 159.0], [528.0, 185.0], [357.0, 185.0]],\n",
       "    ['wtih Self-attn', 0.9993248581886292]],\n",
       "   [[[1054.0, 159.0], [1224.0, 159.0], [1224.0, 185.0], [1054.0, 185.0]],\n",
       "    ['wtih Self-attn', 0.9992867708206177]],\n",
       "   [[[1343.0, 254.0], [1481.0, 254.0], [1481.0, 275.0], [1343.0, 275.0]],\n",
       "    ['oncatenation', 0.9994035363197327]],\n",
       "   [[[365.0, 340.0], [522.0, 340.0], [522.0, 368.0], [365.0, 368.0]],\n",
       "    ['Conv-LSTM', 0.9977107644081116]],\n",
       "   [[[1062.0, 340.0], [1218.0, 340.0], [1218.0, 368.0], [1062.0, 368.0]],\n",
       "    ['Conv-LSTM', 0.9964423775672913]],\n",
       "   [[[723.0, 358.0], [841.0, 358.0], [841.0, 387.0], [723.0, 387.0]],\n",
       "    ['Inter-attn', 0.9669221043586731]],\n",
       "   [[[357.0, 374.0], [528.0, 374.0], [528.0, 403.0], [357.0, 403.0]],\n",
       "    ['wtih Self-attn', 0.9776630401611328]],\n",
       "   [[[1054.0, 374.0], [1224.0, 374.0], [1224.0, 403.0], [1054.0, 403.0]],\n",
       "    ['wtih Self-attn', 0.9996967315673828]],\n",
       "   [[[385.0, 454.0], [498.0, 454.0], [498.0, 482.0], [385.0, 482.0]],\n",
       "    ['Encoder', 0.9998466372489929]],\n",
       "   [[[1084.0, 454.0], [1196.0, 454.0], [1196.0, 482.0], [1084.0, 482.0]],\n",
       "    ['Decoder', 0.9997634887695312]],\n",
       "   [[[1520.0, 452.0], [1698.0, 452.0], [1698.0, 478.0], [1520.0, 478.0]],\n",
       "    ['Fully Connected', 0.9999012351036072]],\n",
       "   [[[20.0, 462.0], [243.0, 462.0], [243.0, 490.0], [20.0, 490.0]],\n",
       "    ['Historical Trading.', 0.9656741619110107]],\n",
       "   [[[1570.0, 480.0], [1650.0, 480.0], [1650.0, 509.0], [1570.0, 509.0]],\n",
       "    ['Layers', 0.9996610283851624]],\n",
       "   [[[102.0, 490.0], [165.0, 495.0], [163.0, 520.0], [100.0, 518.0]],\n",
       "    ['Data', 0.9998564720153809]]],\n",
       "  'ocr': [[[1570.0, 480.0], [1650.0, 480.0], [1650.0, 509.0], [1570.0, 509.0]],\n",
       "   ['Layers', 0.9996610283851624]]},\n",
       " '2107.07001v1-Figure1-1.png': {'caption': 'Fig. 1 Illustration of the proposed algorithm‚Äôs relationship to existing literature on handling discrete logic in a continuous-variable optimization framework.',\n",
       "  'imageText': ['Proposed',\n",
       "   'method',\n",
       "   'STCs',\n",
       "   'Slack',\n",
       "   'variable',\n",
       "   '[25‚Äì27]',\n",
       "   'Multiplicative',\n",
       "   'coeÔ¨Écient',\n",
       "   '[28,',\n",
       "   '29]',\n",
       "   'Compound',\n",
       "   'logic',\n",
       "   '[30,',\n",
       "   '31]',\n",
       "   'CSC',\n",
       "   '[23,',\n",
       "   '24]',\n",
       "   'Indirect',\n",
       "   'methods',\n",
       "   'RASHS',\n",
       "   '[22]',\n",
       "   'Direct',\n",
       "   'methods',\n",
       "   'Nonconvex',\n",
       "   'OCP'],\n",
       "  'image_file': '2107.07001v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'A. Contributions',\n",
       "    'text': \"This paper's contribution is a numerical optimization algorithm to solve optimal control problems (OCPs) that involve a general class of discrete logic constraints. The algorithm is based on a novel arrangement of two core methodologies: sequential convex programming and numerical continuation. SCP is a trust region method for solving general nonconvex optimal control problems [9]. However, it is incapable of handling discrete constraints in their pure (integer) form. By using a homotopy map based on the multinomial logit function, we embed smooth approximations of discrete constraints into the SCP framework, a process also known as continuous embedding [21]. The homotopy map is then updated via a numerical continuation scheme, which transforms an initial coarse approximation into an arbitrarily precise representation of the discrete logic. Herein lies our key innovation: we run SCP and numerical continuation in parallel, rather than in the traditional sequenced approach where one homotopy update is followed by a full SCP solve.\\nFor this reason, we call the method embedded numerical continuation. The resulting algorithm is shown to converge quickly and reliably for a representative terminal rendezvous problem inspired by the Apollo Transposition and Docking maneuver. The problem involves the following major constraints: full six degree of freedom (DOF) dynamics, thruster minimum impulse-bit, range-triggered approach cone, and range-triggered plume impingement. The latter constraints are similar to those considered in [14,15], with the advantage that discrete logic allows the approach cone and plume impingement constraints to be switched on/off automatically by the algorithm, without user input.\\nThis paper represents a significant upgrade in terms of both runtime performance and convergence reliability over the same authors' previous publication on SCP-based rendezvous [26]. Figure 1 illustrates how the proposed algorithm relates to existing literature. Closest to our approach are the recently published relaxed autonomous switched hybrid system (RASHS) and composite smooth control (CSC) algorithms [22][23][24]. Both RASHS and CSC belong to the indirect family of methods in the sense that they solve for the optimality conditions obtained from Pontryagin's maximum principle [32][33][34]. Furthermore, both RASHS and CSC handle discrete logic that is representable by a sequence of Boolean and gates. Our method is distinct from RASHS and CSC in two ways. First, it is a direct method in the sense Nonconvex OCP\",\n",
       "    'n_publication_ref': 11,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'Indirect methods',\n",
       "    'text': 'RASHS [22] CSC [23,24] STCs Slack variable [25][26][27] Multiplicative coefficient [28,29] Compound logic [30,31] Proposed method Fig. 1 Illustration of the proposed algorithm\\'s relationship to existing literature on handling discrete logic in a continuous-variable optimization framework.\\nthat it uses numerical optimization to solve a discretized version of the optimal control problem. This generally makes it easier to handle constraints, which are nontrivial to include in an indirect approach. Second, the proposed method models discrete logic that is representable by a sequence of Boolean or gates. As a result, our work combined with RASHS and CSC can extend homotopy to general Boolean logic using any combination of logic gates. A more detailed comparison of the methods is given in Section III.B.1.\\nOur algorithm is also closely related to the recently introduced family of state triggered constraints (STCs) for SCP algorithms [25,28]. Unlike our method, STCs directly use linearization instead of homotopy in order to enforce an equivalent continuous-variable formulation of discrete logic constraints. Several versions of STCs have been introduced, and we cover these in more detail in Section III.A. Past work on STCs, however, discovered that they can exhibit unfavorable \"locking\" behavior for thruster minimum impulse-bit constraints that are relevant for spacecraft rendezvous [26]. This phenomenon prevents the algorithm from converging, and we describe it in detail in Section III.A. The algorithm presented in this article handles discrete logic constraints like STCs, and does not exhibit locking.',\n",
       "    'n_publication_ref': 13,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Fast Homotopy for Spacecraft Rendezvous Trajectory Optimization with Discrete Logic',\n",
       "  'abstract': 'This paper presents a computationally efficient optimization algorithm for solving nonconvex optimal control problems that involve discrete logic constraints. Traditional solution methods for these constraints require binary variables and mixed-integer programming, which is prohibitively slow and computationally expensive. This paper targets a fast solution that is capable of real-time implementation onboard spacecraft. To do so, a novel algorithm is developed that blends sequential convex programming and numerical continuation into a single iterative solution process. Inside the algorithm, discrete logic constraints are approximated by smooth functions, and a homotopy parameter governs the accuracy of this approximation. As the algorithm converges, the homotopy parameter is updated such that the smooth approximations enforce the exact discrete logic. The effectiveness of this approach is numerically demonstrated for a realistic rendezvous scenario inspired by the Apollo Transposition and Docking maneuver.',\n",
       "  'paddleOCR': [[[[898.0, 3.0], [1189.0, 2.0], [1190.0, 32.0], [898.0, 34.0]],\n",
       "    ['Proposed method', 0.9999390244483948]],\n",
       "   [[[472.0, 83.0], [729.0, 86.0], [729.0, 122.0], [471.0, 118.0]],\n",
       "    ['Direct methods', 0.9992774128913879]],\n",
       "   [[[1140.0, 108.0], [1520.0, 108.0], [1520.0, 142.0], [1140.0, 142.0]],\n",
       "    ['Slack variable [25-27]', 0.9967581033706665]],\n",
       "   [[[896.0, 158.0], [1000.0, 163.0], [998.0, 208.0], [894.0, 204.0]],\n",
       "    ['STCs', 0.9993329048156738]],\n",
       "   [[[1126.0, 165.0], [1698.0, 165.0], [1698.0, 199.0], [1126.0, 199.0]],\n",
       "    [' Multiplicative coefficient [28, 29', 0.9953685402870178]],\n",
       "   [[[1121.0, 217.0], [1565.0, 225.0], [1564.0, 265.0], [1120.0, 258.0]],\n",
       "    ['. Compound logic [30, 31]', 0.9908269643783569]],\n",
       "   [[[21.0, 238.0], [301.0, 238.0], [301.0, 273.0], [21.0, 273.0]],\n",
       "    ['Nonconvex OCP', 0.9914584755897522]],\n",
       "   [[[886.0, 331.0], [1113.0, 331.0], [1113.0, 366.0], [886.0, 366.0]],\n",
       "    ['RASHS [22]', 0.9918716549873352]],\n",
       "   [[[459.0, 393.0], [741.0, 393.0], [741.0, 427.0], [459.0, 427.0]],\n",
       "    ['Indirect methods', 0.9708273410797119]],\n",
       "   [[[887.0, 447.0], [1122.0, 447.0], [1122.0, 483.0], [887.0, 483.0]],\n",
       "    ['CSC [23, 24]', 0.9416095614433289]]],\n",
       "  'ocr': [[[472.0, 83.0], [729.0, 86.0], [729.0, 122.0], [471.0, 118.0]],\n",
       "   ['Direct methods', 0.9992774128913879]]},\n",
       " '2010.04560v3-Figure9-1.png': {'ocr': [[[1357.0, 859.0],\n",
       "    [1509.0, 859.0],\n",
       "    [1509.0, 888.0],\n",
       "    [1357.0, 888.0]],\n",
       "   ['persuation', 0.9998480677604675]],\n",
       "  'True_Statements': ['Recommendations are combined with reasoning and persuasion to form explainable recommendations to end-user.',\n",
       "   'End-user provides feedback to persuasion.'],\n",
       "  'False_Statements': ['Recommendations are combined with persuasion only to form explainable recommendations to end-user.',\n",
       "   'End-user does not provide feedback to persuasion.'],\n",
       "  'Flowchart-to-Caption': 'Figure 9: Flowchart of an explainable energy recommender system.',\n",
       "  'caption': 'Figure 9: Flowchart of an explainable energy recommender system.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.04560v3-Figure9-1.png',\n",
       "  'sections': [{'heading': 'Explainable energy recommender systems at the COVID-19 pandemic',\n",
       "    'text': 'Power consumption in buildings has been completely changed in the COVID-19 pandemic due to the constraints on movement. This has widely triggered teleworking and e-learning, and hence has shifted activities and energy usage to domestic residents [232]. Therefore, the need for smart solutions to detect energy consumption anomalies with reference to the actual situation and other changes that could be occurred at any time is a current challenge. To that end, the use of recommender systems for supporting human decision making has recently received much interest [233,234]. However, with the aim of increasing the end-user trust and improving the acceptance of the generated recommendations, these systems should provide explanations.\\nIn this context, developing mechanisms for explainable and persuasive energy consumption recommendations that could be tailored based on the end-user preferences, habits and current circumstances will promptly reduce wasted energy and promote energy saving. Specifically, the explanations could justify the reasons for recommending each energy efficiency act [235]. On other the hand, the persuasiveness of fact-based explanations could be improved using persuasive and incentive aspects, such as emphasizing ecological impacts and economical saving benefits. Fig. 9 illustrates a general flowchart of an explainable energy recommender system proposed in the (EM) 3 framework [236]. Moreover, it is worth noting that explainable recommender systems are much appropriate to unexpected energy consumption situations (e.g. the COVID-19 pandemic) since the recommendations could be generated in real-time in addition to providing the end-user with more details (using contextual data) on each recommended action to increase its acceptance.',\n",
       "    'n_publication_ref': 5,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'ANOMALY DETECTION OF ENERGY CONSUMPTION IN BUILDINGS: A REVIEW, CURRENT TRENDS AND NEW PERSPECTIVES A PREPRINT',\n",
       "  'abstract': \"Enormous amounts of data are being produced everyday by submeters and smart sensors installed in different kinds of buildings. If leveraged properly, that data could assist end-users, energy producers and utility companies in detecting anomalous power consumption and understanding the causes of each anomaly. Therefore, anomaly detection could stop a minor problem to become widespread, costly and time-consuming issue. Moreover, this will help in better decision-making to reduce wasted energy and promote sustainable and energy efficiency behavior. In this regard, this paper is proposed to indepthly review existing frameworks of anomaly detection in power consumption and provide a critical analysis of existing solutions. Specifically, a comprehensive survey is introduced, in which a novel taxonomy is introduced to classify existing algorithms based on different factors adopted in their implementation, such as the machine learning algorithm, feature extraction approach, detection level, computing platform, application scenario and privacy preservation. To the best of the authors' knowledge, this is the first review article that discusses the anomaly detection in building energy consumption. Moving forward, important findings along with domain-specific problems, difficulties and challenges that remain unresolved are thoroughly discussed, including the absence of: (i) precise definitions of anomalous power consumptions, (ii) annotated datasets, (iii) unified metrics to assess the performance of existing solutions, and (iv) platforms for reproducibility. Following, insights about current research trends that anomaly detection technology needs to target for widespreading its application and facilitate its implementation are described before deriving a set of challenging future directions attracting significant research and development attention.\",\n",
       "  'paddleOCR': [[[[248.0, 0.0], [415.0, 4.0], [414.0, 33.0], [247.0, 27.0]],\n",
       "    ['Reasoning', 0.9999392628669739]],\n",
       "   [[[1119.0, 0.0], [1296.0, 2.0], [1295.0, 29.0], [1119.0, 28.0]],\n",
       "    ['Persuasion', 0.9988951683044434]],\n",
       "   [[[1267.0, 156.0], [1321.0, 156.0], [1321.0, 222.0], [1267.0, 222.0]],\n",
       "    ['$', 0.9995937943458557]],\n",
       "   [[[145.0, 258.0], [262.0, 258.0], [262.0, 287.0], [145.0, 287.0]],\n",
       "    ['General', 0.9997368454933167]],\n",
       "   [[[406.0, 254.0], [481.0, 258.0], [479.0, 289.0], [405.0, 285.0]],\n",
       "    ['User', 0.9997758865356445]],\n",
       "   [[[997.0, 254.0], [1148.0, 254.0], [1148.0, 288.0], [997.0, 288.0]],\n",
       "    ['Ecological', 0.9999482035636902]],\n",
       "   [[[1226.0, 256.0], [1400.0, 256.0], [1400.0, 288.0], [1226.0, 288.0]],\n",
       "    ['Economical', 0.9998982548713684]],\n",
       "   [[[153.0, 295.0], [262.0, 295.0], [262.0, 324.0], [153.0, 324.0]],\n",
       "    ['context', 0.9998151659965515]],\n",
       "   [[[368.0, 297.0], [527.0, 295.0], [527.0, 324.0], [368.0, 326.0]],\n",
       "    ['preference', 0.9994838833808899]],\n",
       "   [[[1022.0, 297.0], [1125.0, 297.0], [1125.0, 326.0], [1022.0, 326.0]],\n",
       "    ['imapct', 0.9990909099578857]],\n",
       "   [[[1260.0, 295.0], [1363.0, 295.0], [1363.0, 324.0], [1260.0, 324.0]],\n",
       "    ['benefit', 0.9997743368148804]],\n",
       "   [[[638.0, 367.0], [910.0, 368.0], [910.0, 398.0], [638.0, 396.0]],\n",
       "    ['Recommendations', 0.9998529553413391]],\n",
       "   [[[731.0, 526.0], [812.0, 526.0], [812.0, 637.0], [731.0, 637.0]],\n",
       "    ['+', 0.9648582339286804]],\n",
       "   [[[29.0, 818.0], [135.0, 818.0], [135.0, 852.0], [29.0, 852.0]],\n",
       "    ['Sellect', 0.9997242093086243]],\n",
       "   [[[1381.0, 816.0], [1513.0, 820.0], [1512.0, 853.0], [1380.0, 849.0]],\n",
       "    ['Sellect a', 0.9998728632926941]],\n",
       "   [[[27.0, 858.0], [152.0, 854.0], [153.0, 888.0], [28.0, 892.0]],\n",
       "    ['targeted', 0.9453708529472351]],\n",
       "   [[[1357.0, 859.0], [1509.0, 859.0], [1509.0, 888.0], [1357.0, 888.0]],\n",
       "    ['persuation', 0.9998480677604675]],\n",
       "   [[[29.0, 892.0], [129.0, 892.0], [129.0, 926.0], [29.0, 926.0]],\n",
       "    ['device', 0.9998517036437988]],\n",
       "   [[[1383.0, 891.0], [1511.0, 895.0], [1510.0, 931.0], [1382.0, 927.0]],\n",
       "    ['fact type', 0.9999182224273682]],\n",
       "   [[[685.0, 947.0], [855.0, 947.0], [855.0, 980.0], [685.0, 980.0]],\n",
       "    ['Explainable', 0.9980115294456482]],\n",
       "   [[[645.0, 988.0], [903.0, 988.0], [903.0, 1015.0], [645.0, 1015.0]],\n",
       "    ['recommendations', 0.9997723698616028]],\n",
       "   [[[1120.0, 1022.0], [1392.0, 1022.0], [1392.0, 1050.0], [1120.0, 1050.0]],\n",
       "    ['End-user feedback', 0.984368622303009]],\n",
       "   [[[710.0, 1261.0], [844.0, 1261.0], [844.0, 1291.0], [710.0, 1291.0]],\n",
       "    ['End-user', 0.9996384382247925]]]},\n",
       " '2010.07860v3-Figure1-1.png': {'ocr': [[[735.0, 406.0],\n",
       "    [871.0, 410.0],\n",
       "    [871.0, 440.0],\n",
       "    [735.0, 436.0]],\n",
       "   ['Interaction', 0.9998895525932312]],\n",
       "  'True_Statements': ['Structured Network and Deep Network are inputs for the Shift Predictor.',\n",
       "   'Transformed Distribution outputs a likelihood.'],\n",
       "  'False_Statements': ['Structured Network and Deep Network are outputs for the Shift Predictor.',\n",
       "   'Transformed Distribution takes a likelihood as input.'],\n",
       "  'Flowchart-to-Caption': 'Figure 1. Architecture of a deep conditional transformation model. Both the shift and interaction predictor can potentially be defined by a structured network including linear terms, (penalized) splines or other structured additive regression terms and deep neural network defined by an arbitrary network structure. While the shift predictor (CŒ®) is a sum of both subnetwork predictions, the interaction predictor (A B) is only multiplied with a final 1-hidden unit fully-connected layer (network head, vec(Œì)) after the structured predictors and latent features of the deep neural network are combined with the basis evaluated outcome. The shift and interaction network part together define the transformation function, which transforms the error distribution and yields the final likelihood used as loss function.',\n",
       "  'caption': 'Figure 1. Architecture of a deep conditional transformation model. Both the shift and interaction predictor can potentially be defined by a structured network including linear terms, (penalized) splines or other structured additive regression terms and deep neural network defined by an arbitrary network structure. While the shift predictor (CŒ®) is a sum of both subnetwork predictions, the interaction predictor (A B) is only multiplied with a final 1-hidden unit fully-connected layer (network head, vec(Œì)) after the structured predictors and latent features of the deep neural network are combined with the basis evaluated outcome. The shift and interaction network part together define the transformation function, which transforms the error distribution and yields the final likelihood used as loss function.',\n",
       "  'imageText': ['Shift',\n",
       "   'Predictor',\n",
       "   'Interaction',\n",
       "   'Predictor',\n",
       "   'c',\n",
       "   'Interaction',\n",
       "   'Predictor',\n",
       "   'Head',\n",
       "   'likelihood',\n",
       "   'Transformed',\n",
       "   'Distribution',\n",
       "   'Transformation',\n",
       "   'Function',\n",
       "   'Error',\n",
       "   'Distribution',\n",
       "   'Outcome',\n",
       "   'Basis',\n",
       "   'Evaluation',\n",
       "   'Deep',\n",
       "   'Network',\n",
       "   'Structured',\n",
       "   'Predictors',\n",
       "   'Deep',\n",
       "   'Network',\n",
       "   'Structured',\n",
       "   'Network',\n",
       "   '+'],\n",
       "  'image_file': '2010.07860v3-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Network Definition',\n",
       "    'text': 'Our network consists of two main parts: a feature transforming network (FTN) part, converting X = (x 1 , . . . , x n ) ‚àà R n√óp to B and an outcome transforming network (OTN) part, transforming y = (y 1 , . . . , y n ) ‚àà R n to h(y|X) ‚àà R n . In the OTN part the matrix Œì is learned, while the FTN part only contains additional parameters to be learned by the network if some feature(s) are defined using a deep neural network. In other words, if only structured linear effects or basis function transformations are used in the FTN part, Œì contains all trainable parameters. Figure 1 visualizes an exemplary architecture.\\nAfter the features are processed in the FTN part, the final transformed outcome is modeled using a conventional fully-connected layer with input A B, one hidden unit with linear activation function and weights corresponding to vec(Œì). The deep conditional transformation model as visualized in Figure 1 can also be defined with one common network which is split into one part that is added to the shift predictor and one part that is used in the interaction predictor.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 2}],\n",
       "  'title': 'Deep Conditional Transformation Models',\n",
       "  'abstract': 'Learning the cumulative distribution function (CDF) of an outcome variable conditional on a set of features remains challenging, especially in high-dimensional settings. Conditional transformation models provide a semi-parametric approach that allows to model a large class of conditional CDFs without an explicit parametric distribution assumption and with only a few parameters. Existing estimation approaches within this class are, however, either limited in their complexity and applicability to unstructured data sources such as images or text, lack interpretability, or are restricted to certain types of outcomes. We close this gap by introducing the class of deep conditional transformation models which unifies existing approaches and allows to learn both interpretable (non-)linear model terms and more complex neural network predictors in one holistic framework. To this end we propose a novel network architecture, provide details on different model definitions and derive suitable constraints as well as network regularization terms. We demonstrate the efficacy of our approach through numerical experiments and applications.',\n",
       "  'paddleOCR': [[[[516.0, 12.0], [674.0, 12.0], [674.0, 40.0], [516.0, 40.0]],\n",
       "    ['Transformed', 0.9999454021453857]],\n",
       "   [[[818.0, 25.0], [985.0, 25.0], [985.0, 55.0], [818.0, 55.0]],\n",
       "    ['>likelihood', 0.9952329993247986]],\n",
       "   [[[523.0, 47.0], [666.0, 47.0], [666.0, 75.0], [523.0, 75.0]],\n",
       "    ['Distribution', 0.9998273849487305]],\n",
       "   [[[559.0, 138.0], [632.0, 143.0], [630.0, 174.0], [557.0, 170.0]],\n",
       "    ['Error', 0.9999336004257202]],\n",
       "   [[[516.0, 174.0], [671.0, 177.0], [670.0, 208.0], [516.0, 204.0]],\n",
       "    ['Distribution', 0.9708542823791504]],\n",
       "   [[[494.0, 266.0], [695.0, 269.0], [695.0, 299.0], [493.0, 296.0]],\n",
       "    ['Transformation', 0.9999558329582214]],\n",
       "   [[[534.0, 302.0], [654.0, 306.0], [653.0, 336.0], [533.0, 332.0]],\n",
       "    ['Function', 0.9996769428253174]],\n",
       "   [[[285.0, 396.0], [462.0, 396.0], [462.0, 425.0], [285.0, 425.0]],\n",
       "    ['Shift Predictor', 0.9999591112136841]],\n",
       "   [[[735.0, 406.0], [871.0, 410.0], [871.0, 440.0], [735.0, 436.0]],\n",
       "    ['Interaction', 0.9998895525932312]],\n",
       "   [[[1161.0, 406.0], [1366.0, 406.0], [1366.0, 430.0], [1161.0, 430.0]],\n",
       "    ['Basis Evaluation', 0.9998736381530762]],\n",
       "   [[[1454.0, 421.0], [1563.0, 421.0], [1563.0, 450.0], [1454.0, 450.0]],\n",
       "    ['Outcome', 0.9997226595878601]],\n",
       "   [[[719.0, 445.0], [890.0, 445.0], [890.0, 468.0], [719.0, 468.0]],\n",
       "    ['Predictor Head', 0.9902340769767761]],\n",
       "   [[[826.0, 533.0], [1076.0, 533.0], [1076.0, 557.0], [826.0, 557.0]],\n",
       "    ['Interaction Predictor', 0.9985713958740234]],\n",
       "   [[[59.0, 625.0], [190.0, 625.0], [190.0, 654.0], [59.0, 654.0]],\n",
       "    ['Structured', 0.9998787641525269]],\n",
       "   [[[71.0, 660.0], [179.0, 660.0], [179.0, 689.0], [71.0, 689.0]],\n",
       "    ['Network', 0.9999147057533264]],\n",
       "   [[[902.0, 670.0], [1033.0, 670.0], [1033.0, 699.0], [902.0, 699.0]],\n",
       "    ['Structured', 0.9998822212219238]],\n",
       "   [[[376.0, 697.0], [449.0, 701.0], [447.0, 729.0], [374.0, 725.0]],\n",
       "    ['Deep', 0.9995818138122559]],\n",
       "   [[[902.0, 706.0], [1033.0, 706.0], [1033.0, 734.0], [902.0, 734.0]],\n",
       "    ['Predictors', 0.9998922348022461]],\n",
       "   [[[356.0, 730.0], [468.0, 736.0], [466.0, 765.0], [355.0, 759.0]],\n",
       "    ['Network', 0.9998623728752136]],\n",
       "   [[[1219.0, 742.0], [1293.0, 746.0], [1291.0, 776.0], [1217.0, 772.0]],\n",
       "    ['Deep', 0.9992834329605103]],\n",
       "   [[[1201.0, 779.0], [1309.0, 779.0], [1309.0, 808.0], [1201.0, 808.0]],\n",
       "    ['Network', 0.9999197721481323]]]},\n",
       " '2011.06192v3-Figure5-1.png': {'ocr': [[[574.0, 342.0],\n",
       "    [695.0, 342.0],\n",
       "    [695.0, 368.0],\n",
       "    [574.0, 368.0]],\n",
       "   ['Estimated', 0.9999081492424011]],\n",
       "  'True_Statements': ['In general, the delays caused during the demonstration and autonomous operation are different.',\n",
       "   'In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same.'],\n",
       "  'False_Statements': ['In general, the delays caused during the demonstration and autonomous operation are the same.',\n",
       "   'In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are different.'],\n",
       "  'Flowchart-to-Caption': 'Fig. 5. Overview of general IL and our bilateral control-based IL. In general, the delays caused during the demonstration and autonomous operation are different. Therefore, a general IL can realize only slow motion, which can ignore delays. In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same. Thus, in our bilateral control-based IL, fast motion with delays can be achieved.',\n",
       "  'caption': 'Fig. 5. Overview of general IL and our bilateral control-based IL. In general, the delays caused during the demonstration and autonomous operation are different. Therefore, a general IL can realize only slow motion, which can ignore delays. In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same. Thus, in our bilateral control-based IL, fast motion with delays can be achieved.',\n",
       "  'imageText': ['(a)',\n",
       "   'General',\n",
       "   'imitation',\n",
       "   'learning',\n",
       "   '(b)',\n",
       "   'Bilateral',\n",
       "   'control-based',\n",
       "   'imitation',\n",
       "   'learning',\n",
       "   'Delays',\n",
       "   'from',\n",
       "   'master‚Äôs',\n",
       "   'response',\n",
       "   'to',\n",
       "   'slave‚Äôs',\n",
       "   'response',\n",
       "   'Delays',\n",
       "   'from',\n",
       "   'master‚Äôs',\n",
       "   'response',\n",
       "   'to',\n",
       "   'slave‚Äôs',\n",
       "   'response',\n",
       "   'Slave',\n",
       "   'Slave',\n",
       "   'Slave‚Äôs',\n",
       "   'Controller',\n",
       "   'No',\n",
       "   'Delay',\n",
       "   'from',\n",
       "   'Demonstration',\n",
       "   'Slave‚Äôs',\n",
       "   'ResponseAI',\n",
       "   'AI',\n",
       "   'Master‚Äôs',\n",
       "   'Response',\n",
       "   'Slave‚Äôs',\n",
       "   'Controller',\n",
       "   'Autonomous',\n",
       "   'Operation',\n",
       "   'Master',\n",
       "   'Demonstration',\n",
       "   'Estimated',\n",
       "   'Master‚Äôs',\n",
       "   'Response',\n",
       "   'Slave‚Äôs',\n",
       "   'Response',\n",
       "   'Delays',\n",
       "   'from',\n",
       "   'response',\n",
       "   'to',\n",
       "   'next',\n",
       "   'response',\n",
       "   'Delays',\n",
       "   'from',\n",
       "   'command',\n",
       "   'to',\n",
       "   'response',\n",
       "   'AI',\n",
       "   'AI',\n",
       "   'Command',\n",
       "   'Controller',\n",
       "   'Due',\n",
       "   'to',\n",
       "   'Delay',\n",
       "   'Different',\n",
       "   'Response',\n",
       "   'Controller',\n",
       "   'Response',\n",
       "   'Autonomous',\n",
       "   'Operation',\n",
       "   'Demonstration',\n",
       "   'Estimated',\n",
       "   'Response',\n",
       "   'Response'],\n",
       "  'image_file': '2011.06192v3-Figure5-1.png',\n",
       "  'sections': [{'heading': 'B. Advantage',\n",
       "    'text': \"The main advantages of bilateral control-based IL are the following two points.\\n1) IL using force information can be realized: Bilateral control-based IL can realize tasks requiring a force adjustment. By using bilateral control, force information can be collected during the demonstrations. The master measures the operator's action force, and the slave measures the reaction force from the environment. When using the RFOB, each force can be measured without a force sensor.\\n2) Fast motion can be realized: The most notable advantage of a bilateral control-based IL is that robots can achieve fast motion. One of the common issues of conventional ILs [11]- [17] is that robot motion is extremely slow compared to human motion. As shown in Fig. 5-(a), in general IL, the response values collected during the demonstrations are given as the command values during the autonomous operation because the command values cannot be measured directly during the demonstrations. In robot control, eliminating control delays is virtually impossible. In addition, when performing tasks, including contact with the environment, delays due to physical interactions occur. In general, the robots cannot reproduce the same motion as the demonstrations because of the different delays during the autonomous operation and the demonstrations. For this reason, a general IL can achieve only slow motion and can ignore the delays. From the above, the following two points must be satisfied to realize fast motion in the IL.\\n(i) Command values must be predicted during autonomous operation, i.e., the command values must be collected during the demonstrations, (ii) The same control system must be implemented during the demonstrations and autonomous operation.\\nOur bilateral control-based IL can satisfy these two points for the following reasons. First, in bilateral control, the command values of the slave are the response values of the master. Therefore, the command values and the response values of the slave can be measured separately. As a result, the command values of the slave can be predicted during an autonomous operation. As shown in Fig. 5-(b), in our bilateral control-based IL, the delays that occur during the demonstrations similarly occur during the autonomous operation. Second, as shown in Fig. 3, in a bilateral control-based IL, the system is designed to reproduce bilateral control during an autonomous operation. Hence, the control system can be the same during the demonstrations and autonomous operation. During the demonstrations using bilateral control, humans collect data considering the delays, i.e., humans demonstrate skills to compensate for the delays. If the control system is different during the demonstrations and an autonomous operation, this compensation skill will be lost. However, our bilateral control-based IL can reproduce this skill during an autonomous operation. A bilateral controlbased IL can satisfy the above two points, and the method can execute tasks with fast motion performed through bilateral control. Therefore, this is a suitable method for IL because the robot can perform tasks requiring a force adjustment and achieve a fast motion.\",\n",
       "    'n_publication_ref': 2,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Motion Generation Using Bilateral Control-Based Imitation Learning with Autoregressive Learning',\n",
       "  'abstract': 'Robots that can execute various tasks automatically on behalf of humans are becoming an increasingly important focus of research in the field of robotics. Imitation learning has been studied as an efficient and high-performance method, and imitation learning based on bilateral control has been proposed as a method that can realize fast motion. However, because this method cannot implement autoregressive learning, this method may not generate desirable long-term behavior. Therefore, in this paper, we propose a method of autoregressive learning for bilateral control-based imitation learning. A new neural network model for implementing autoregressive learning is proposed. In this study, three types of experiments are conducted to verify the effectiveness of the proposed method. The performance is improved compared to conventional approaches; the proposed method has the highest rate of success. Owing to the structure and autoregressive learning of the proposed model, the proposed method can generate the desirable motion for successful tasks and have a high generalization ability for environmental changes.',\n",
       "  'paddleOCR': [[[[71.0, 12.0], [274.0, 12.0], [274.0, 42.0], [71.0, 42.0]],\n",
       "    ['Demonstration', 0.9996777176856995]],\n",
       "   [[[1262.0, 12.0], [1467.0, 12.0], [1467.0, 42.0], [1262.0, 42.0]],\n",
       "    ['Demonstration', 0.9996801018714905]],\n",
       "   [[[174.0, 76.0], [605.0, 79.0], [604.0, 109.0], [174.0, 106.0]],\n",
       "    ['Delays from command to response', 0.9838899374008179]],\n",
       "   [[[1408.0, 74.0], [2023.0, 74.0], [2023.0, 97.0], [1408.0, 97.0]],\n",
       "    [\"Delays from master's response to slave's response\", 0.9993351697921753]],\n",
       "   [[[1760.0, 129.0], [1823.0, 135.0], [1821.0, 163.0], [1758.0, 157.0]],\n",
       "    ['Slave', 0.9994567632675171]],\n",
       "   [[[1450.0, 143.0], [1558.0, 143.0], [1558.0, 169.0], [1450.0, 169.0]],\n",
       "    [\"Master's\", 0.9826079607009888]],\n",
       "   [[[9.0, 180.0], [139.0, 180.0], [139.0, 206.0], [9.0, 206.0]],\n",
       "    ['Command', 0.9998735189437866]],\n",
       "   [[[210.0, 176.0], [344.0, 176.0], [344.0, 208.0], [210.0, 208.0]],\n",
       "    ['Controller', 0.999184250831604]],\n",
       "   [[[1436.0, 171.0], [1566.0, 171.0], [1566.0, 204.0], [1436.0, 204.0]],\n",
       "    ['Response', 0.9998830556869507]],\n",
       "   [[[1597.0, 173.0], [1703.0, 173.0], [1703.0, 206.0], [1597.0, 206.0]],\n",
       "    [\"Slave's\", 0.9996795058250427]],\n",
       "   [[[1923.0, 173.0], [2019.0, 179.0], [2018.0, 211.0], [1921.0, 205.0]],\n",
       "    [\"Slave's\", 0.9993963241577148]],\n",
       "   [[[585.0, 183.0], [719.0, 183.0], [719.0, 208.0], [585.0, 208.0]],\n",
       "    [' Response', 0.941415548324585]],\n",
       "   [[[1582.0, 215.0], [1716.0, 215.0], [1716.0, 240.0], [1582.0, 240.0]],\n",
       "    ['Controller', 0.999221920967102]],\n",
       "   [[[1908.0, 213.0], [2034.0, 213.0], [2034.0, 238.0], [1908.0, 238.0]],\n",
       "    ['Response', 0.9999198317527771]],\n",
       "   [[[549.0, 250.0], [582.0, 250.0], [582.0, 280.0], [549.0, 280.0]],\n",
       "    ['Al', 0.7677311301231384]],\n",
       "   [[[763.0, 259.0], [891.0, 259.0], [891.0, 284.0], [763.0, 284.0]],\n",
       "    ['Different', 0.9998161792755127]],\n",
       "   [[[1271.0, 261.0], [1348.0, 261.0], [1348.0, 287.0], [1271.0, 287.0]],\n",
       "    ['Master', 0.9997270703315735]],\n",
       "   [[[1458.0, 271.0], [1489.0, 271.0], [1489.0, 301.0], [1458.0, 301.0]],\n",
       "    ['Al', 0.7793590426445007]],\n",
       "   [[[752.0, 294.0], [902.0, 294.0], [902.0, 326.0], [752.0, 326.0]],\n",
       "    ['Response', 0.9999191164970398]],\n",
       "   [[[1880.0, 287.0], [2014.0, 287.0], [2014.0, 319.0], [1880.0, 319.0]],\n",
       "    ['No Delay', 0.9997392296791077]],\n",
       "   [[[11.0, 333.0], [335.0, 333.0], [335.0, 363.0], [11.0, 363.0]],\n",
       "    ['Autonomous Operation', 0.999947726726532]],\n",
       "   [[[737.0, 331.0], [927.0, 331.0], [927.0, 363.0], [737.0, 363.0]],\n",
       "    ['Due to Delay', 0.9571811556816101]],\n",
       "   [[[1798.0, 326.0], [2096.0, 326.0], [2096.0, 356.0], [1798.0, 356.0]],\n",
       "    ['from Demonstration', 0.9828559160232544]],\n",
       "   [[[574.0, 342.0], [695.0, 342.0], [695.0, 368.0], [574.0, 368.0]],\n",
       "    ['Estimated', 0.9999081492424011]],\n",
       "   [[[1205.0, 340.0], [1527.0, 340.0], [1527.0, 370.0], [1205.0, 370.0]],\n",
       "    ['Autonomous Operation', 0.9999574422836304]],\n",
       "   [[[568.0, 370.0], [698.0, 375.0], [696.0, 407.0], [567.0, 402.0]],\n",
       "    ['Response', 0.9946151971817017]],\n",
       "   [[[1761.0, 365.0], [1825.0, 365.0], [1825.0, 391.0], [1761.0, 391.0]],\n",
       "    ['Slave', 0.9995805025100708]],\n",
       "   [[[1596.0, 386.0], [1704.0, 391.0], [1702.0, 426.0], [1595.0, 420.0]],\n",
       "    [\"Slave's\", 0.9990102648735046]],\n",
       "   [[[1926.0, 398.0], [2017.0, 398.0], [2017.0, 423.0], [1926.0, 423.0]],\n",
       "    [\"Slave's\", 0.9997599720954895]],\n",
       "   [[[652.0, 411.0], [804.0, 417.0], [802.0, 456.0], [650.0, 451.0]],\n",
       "    ['Controller', 0.9997631311416626]],\n",
       "   [[[549.0, 421.0], [589.0, 421.0], [589.0, 449.0], [549.0, 449.0]],\n",
       "    ['Al', 0.7628437876701355]],\n",
       "   [[[1456.0, 412.0], [1496.0, 412.0], [1496.0, 442.0], [1456.0, 442.0]],\n",
       "    ['AI', 0.7562621235847473]],\n",
       "   [[[982.0, 428.0], [1112.0, 428.0], [1112.0, 453.0], [982.0, 453.0]],\n",
       "    [' Response', 0.9570518136024475]],\n",
       "   [[[1583.0, 425.0], [1719.0, 430.0], [1718.0, 463.0], [1581.0, 458.0]],\n",
       "    ['Controller', 0.9998084306716919]],\n",
       "   [[[1905.0, 425.0], [2037.0, 431.0], [2036.0, 463.0], [1904.0, 457.0]],\n",
       "    ['Response', 0.9996957778930664]],\n",
       "   [[[1430.0, 451.0], [1558.0, 456.0], [1557.0, 488.0], [1429.0, 483.0]],\n",
       "    ['Estimated', 0.9997313618659973]],\n",
       "   [[[1374.0, 488.0], [1613.0, 488.0], [1613.0, 518.0], [1374.0, 518.0]],\n",
       "    [\"Master's Response.\", 0.9697355628013611]],\n",
       "   [[[596.0, 553.0], [1074.0, 553.0], [1074.0, 583.0], [596.0, 583.0]],\n",
       "    ['Delays from response to next response', 0.9835342168807983]],\n",
       "   [[[1405.0, 580.0], [2028.0, 580.0], [2028.0, 610.0], [1405.0, 610.0]],\n",
       "    [\"Delays from master's response to slave's response\", 0.9996531009674072]],\n",
       "   [[[219.0, 624.0], [664.0, 627.0], [664.0, 665.0], [218.0, 664.0]],\n",
       "    ['(a) General imitation learning', 0.9996135234832764]],\n",
       "   [[[1317.0, 629.0], [1975.0, 629.0], [1975.0, 661.0], [1317.0, 661.0]],\n",
       "    ['(b) Bilateral control-based imitation learning', 0.9891887307167053]]]},\n",
       " '2010.02256v1-Figure4-1.png': {'caption': 'Figure 4: The network architecture of the Surrounding Context model.',\n",
       "  'imageText': [],\n",
       "  'image_file': '2010.02256v1-Figure4-1.png',\n",
       "  'sections': [{'heading': 'Surrounding Context Model',\n",
       "    'text': \"Figure 4 demonstrates the proposed architecture for the Surrounding Context model. The surrounding context is defined as the sentence immediately before and the sentence immediately after the focus sentence. The most efficient size of the surrounding context can be determined through hyper-parameter tuning, which is beyond the scope of this work and is considered for future work. Each sentence is fed into a Bi-directional LSTM layer. The LSTM layer for the focus sentence comprises 64 units, whereas the LSTM layers of surrounding sentences have 16 units. Next, each Bi-LSTM layer's output sequence is fed into a max-over-time pooling layer to encode the sequence. The three sentence encoders' outputs are concatenated and passed into a fully-connected layer with 50 neurons and ReLU activation function. This layer is followed by a Dropout layer with a value of 50%. The weights are passed to a fullyconnected layer with ten neurons and a dropout value of 30%. Subsequently, the output is fed into a second fully-connected layer with seven neurons and the Softmax activation function to obtain the final prediction. In cases where the focus sentence appears at either the beginning or end of a report, we use an empty string for the sentence before or after.\",\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'An Ensemble Approach to Automatic Structuring of Radiology Reports',\n",
       "  'abstract': \"Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists' reporting style varies from one to another as sentences are telegraphic and do not follow general English grammar rules. We present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bidirectional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and stateof-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1% accuracy.\",\n",
       "  'paddleOCR': [[[[379.0, 9.0], [509.0, 11.0], [509.0, 31.0], [379.0, 29.0]],\n",
       "    ['Final Prediction', 0.9952574372291565]],\n",
       "   [[[390.0, 80.0], [499.0, 83.0], [499.0, 104.0], [390.0, 101.0]],\n",
       "    ['Output Layer', 0.9998630881309509]],\n",
       "   [[[353.0, 151.0], [535.0, 151.0], [535.0, 171.0], [353.0, 171.0]],\n",
       "    ['Fully-conntected layer', 0.9986150860786438]],\n",
       "   [[[362.0, 220.0], [545.0, 220.0], [545.0, 240.0], [362.0, 240.0]],\n",
       "    ['Fully-connected Layer', 0.9995741844177246]],\n",
       "   [[[462.0, 264.0], [568.0, 267.0], [568.0, 285.0], [462.0, 282.0]],\n",
       "    ['Concatenate', 0.9992713928222656]],\n",
       "   [[[103.0, 334.0], [182.0, 336.0], [182.0, 354.0], [102.0, 352.0]],\n",
       "    ['Sentence', 0.9982575178146362]],\n",
       "   [[[412.0, 333.0], [492.0, 335.0], [491.0, 353.0], [412.0, 351.0]],\n",
       "    ['Sentence', 0.9982418417930603]],\n",
       "   [[[721.0, 334.0], [801.0, 336.0], [800.0, 354.0], [721.0, 352.0]],\n",
       "    ['Sentence', 0.9552869200706482]],\n",
       "   [[[107.0, 358.0], [178.0, 358.0], [178.0, 375.0], [107.0, 375.0]],\n",
       "    ['Encoder', 0.9988141059875488]],\n",
       "   [[[416.0, 356.0], [488.0, 358.0], [487.0, 376.0], [415.0, 374.0]],\n",
       "    ['Encoder', 0.9983133673667908]],\n",
       "   [[[726.0, 359.0], [797.0, 359.0], [797.0, 376.0], [726.0, 376.0]],\n",
       "    ['Encoder', 0.9983813166618347]],\n",
       "   [[[68.0, 434.0], [232.0, 434.0], [232.0, 451.0], [68.0, 451.0]],\n",
       "    ['Bi-Directional LSTM', 0.9731659889221191]],\n",
       "   [[[373.0, 434.0], [538.0, 434.0], [538.0, 451.0], [373.0, 451.0]],\n",
       "    ['Bi-Directional LSTM', 0.9681841135025024]],\n",
       "   [[[679.0, 434.0], [843.0, 434.0], [843.0, 451.0], [679.0, 451.0]],\n",
       "    ['Bi-Directional LSTM', 0.9838720560073853]],\n",
       "   [[[314.0, 524.0], [323.0, 524.0], [323.0, 536.0], [314.0, 536.0]],\n",
       "    ['1', 0.9830850958824158]],\n",
       "   [[[283.0, 542.0], [291.0, 542.0], [291.0, 553.0], [283.0, 553.0]],\n",
       "    ['1', 0.9715275764465332]],\n",
       "   [[[314.0, 541.0], [323.0, 541.0], [323.0, 553.0], [314.0, 553.0]],\n",
       "    ['1', 0.9751767516136169]],\n",
       "   [[[69.0, 552.0], [110.0, 552.0], [110.0, 570.0], [69.0, 570.0]],\n",
       "    ['WV 1', 0.9792092442512512]],\n",
       "   [[[191.0, 552.0], [229.0, 552.0], [229.0, 570.0], [191.0, 570.0]],\n",
       "    ['wVi', 0.8298325538635254]],\n",
       "   [[[283.0, 559.0], [291.0, 559.0], [291.0, 570.0], [283.0, 570.0]],\n",
       "    ['1', 0.9529336094856262]],\n",
       "   [[[314.0, 559.0], [323.0, 559.0], [323.0, 571.0], [314.0, 571.0]],\n",
       "    ['1', 0.9538733959197998]],\n",
       "   [[[374.0, 552.0], [417.0, 552.0], [417.0, 570.0], [374.0, 570.0]],\n",
       "    ['WV 1', 0.9841371774673462]],\n",
       "   [[[490.0, 550.0], [527.0, 554.0], [525.0, 572.0], [488.0, 568.0]],\n",
       "    ['wVj', 0.6325679421424866]],\n",
       "   [[[684.0, 552.0], [726.0, 552.0], [726.0, 570.0], [684.0, 570.0]],\n",
       "    ['WV 1', 0.9665243625640869]],\n",
       "   [[[797.0, 550.0], [839.0, 552.0], [838.0, 571.0], [795.0, 569.0]],\n",
       "    ['WV k', 0.9486132264137268]],\n",
       "   [[[282.0, 576.0], [291.0, 576.0], [291.0, 588.0], [282.0, 588.0]],\n",
       "    ['1', 0.9861408472061157]],\n",
       "   [[[311.0, 576.0], [322.0, 573.0], [326.0, 588.0], [314.0, 591.0]],\n",
       "    ['1', 0.603718638420105]],\n",
       "   [[[283.0, 594.0], [291.0, 594.0], [291.0, 606.0], [283.0, 606.0]],\n",
       "    ['-', 0.8783469796180725]],\n",
       "   [[[314.0, 593.0], [323.0, 593.0], [323.0, 606.0], [314.0, 606.0]],\n",
       "    ['1', 0.9487818479537964]],\n",
       "   [[[620.0, 594.0], [627.0, 594.0], [627.0, 606.0], [620.0, 606.0]],\n",
       "    ['-', 0.9140086770057678]],\n",
       "   [[[283.0, 612.0], [291.0, 612.0], [291.0, 623.0], [283.0, 623.0]],\n",
       "    ['1', 0.9753950238227844]],\n",
       "   [[[62.0, 631.0], [116.0, 631.0], [116.0, 648.0], [62.0, 648.0]],\n",
       "    ['Token 1', 0.9531970024108887]],\n",
       "   [[[184.0, 631.0], [235.0, 631.0], [235.0, 648.0], [184.0, 648.0]],\n",
       "    ['Token i', 0.9985964894294739]],\n",
       "   [[[283.0, 629.0], [292.0, 629.0], [292.0, 640.0], [283.0, 640.0]],\n",
       "    ['1', 0.8612531423568726]],\n",
       "   [[[314.0, 629.0], [323.0, 629.0], [323.0, 640.0], [314.0, 640.0]],\n",
       "    ['1', 0.9660627841949463]],\n",
       "   [[[368.0, 631.0], [424.0, 631.0], [424.0, 648.0], [368.0, 648.0]],\n",
       "    ['Token 1', 0.9763633012771606]],\n",
       "   [[[482.0, 628.0], [534.0, 631.0], [533.0, 650.0], [481.0, 647.0]],\n",
       "    ['Token j', 0.9808072447776794]],\n",
       "   [[[620.0, 630.0], [627.0, 630.0], [627.0, 640.0], [620.0, 640.0]],\n",
       "    ['1', 0.8991991281509399]],\n",
       "   [[[678.0, 631.0], [732.0, 631.0], [732.0, 648.0], [678.0, 648.0]],\n",
       "    ['Token 1', 0.9709461331367493]],\n",
       "   [[[790.0, 631.0], [846.0, 631.0], [846.0, 648.0], [790.0, 648.0]],\n",
       "    ['Token k', 0.9536514282226562]],\n",
       "   [[[11.0, 647.0], [17.0, 647.0], [17.0, 656.0], [11.0, 656.0]],\n",
       "    ['-', 0.526188850402832]],\n",
       "   [[[283.0, 645.0], [292.0, 645.0], [292.0, 657.0], [283.0, 657.0]],\n",
       "    ['1', 0.8025349974632263]],\n",
       "   [[[312.0, 644.0], [324.0, 644.0], [324.0, 658.0], [312.0, 658.0]],\n",
       "    ['1', 0.9383662939071655]],\n",
       "   [[[620.0, 645.0], [627.0, 645.0], [627.0, 657.0], [620.0, 657.0]],\n",
       "    ['-', 0.8892728686332703]],\n",
       "   [[[895.0, 647.0], [901.0, 647.0], [901.0, 656.0], [895.0, 656.0]],\n",
       "    ['-', 0.7679548859596252]],\n",
       "   [[[283.0, 663.0], [291.0, 663.0], [291.0, 675.0], [283.0, 675.0]],\n",
       "    ['-', 0.9014716148376465]],\n",
       "   [[[314.0, 662.0], [324.0, 662.0], [324.0, 677.0], [314.0, 677.0]],\n",
       "    ['-', 0.8813327550888062]],\n",
       "   [[[316.0, 681.0], [324.0, 681.0], [324.0, 690.0], [316.0, 690.0]],\n",
       "    ['L', 0.9822728037834167]],\n",
       "   [[[71.0, 700.0], [227.0, 702.0], [226.0, 719.0], [71.0, 717.0]],\n",
       "    ['Previous sentence', 0.9897398352622986]],\n",
       "   [[[392.0, 700.0], [525.0, 702.0], [525.0, 719.0], [392.0, 717.0]],\n",
       "    ['Focus sentence', 0.9893215298652649]],\n",
       "   [[[707.0, 699.0], [828.0, 701.0], [828.0, 719.0], [707.0, 717.0]],\n",
       "    ['Next sentence', 0.9905604720115662]]],\n",
       "  'ocr': [[[283.0, 559.0], [291.0, 559.0], [291.0, 570.0], [283.0, 570.0]],\n",
       "   ['1', 0.9529336094856262]]},\n",
       " '2010.02825v1-Figure1-1.png': {'caption': 'Figure 1: Overview of a resistive memory bank.',\n",
       "  'imageText': ['¬∑¬∑¬∑¬∑¬∑¬∑',\n",
       "   '¬∑¬∑¬∑',\n",
       "   'N-',\n",
       "   '1]',\n",
       "   '[i+',\n",
       "   '1‚Ä¶',\n",
       "   'i]',\n",
       "   '[0',\n",
       "   '‚Ä¶',\n",
       "   'od',\n",
       "   'er',\n",
       "   'D',\n",
       "   'ec',\n",
       "   'ow',\n",
       "   'ba',\n",
       "   'l',\n",
       "   'R',\n",
       "   'G',\n",
       "   'lo',\n",
       "   'od',\n",
       "   'er',\n",
       "   'D',\n",
       "   'ec',\n",
       "   'ow',\n",
       "   'ca',\n",
       "   'l',\n",
       "   'R',\n",
       "   'er',\n",
       "   'Lo',\n",
       "   'ec',\n",
       "   'od',\n",
       "   'D',\n",
       "   'l',\n",
       "   'R',\n",
       "   'ow',\n",
       "   'Lo',\n",
       "   'ca',\n",
       "   'Address',\n",
       "   '(N',\n",
       "   'bits)',\n",
       "   'ck',\n",
       "   'B',\n",
       "   'lo',\n",
       "   'or',\n",
       "   'y',\n",
       "   'M',\n",
       "   'em',\n",
       "   'Subarray',\n",
       "   'N',\n",
       "   '-',\n",
       "   '1',\n",
       "   'Subarray',\n",
       "   '0',\n",
       "   'Subarray',\n",
       "   '1',\n",
       "   'Row',\n",
       "   'Buffer',\n",
       "   'er',\n",
       "   'ec',\n",
       "   'od',\n",
       "   'D',\n",
       "   'l',\n",
       "   'R',\n",
       "   'ow',\n",
       "   'Lo',\n",
       "   'ca',\n",
       "   't',\n",
       "   'el',\n",
       "   'ec',\n",
       "   'rra',\n",
       "   'y',\n",
       "   'S',\n",
       "   'Su',\n",
       "   'ba',\n",
       "   '¬∑¬∑¬∑'],\n",
       "  'image_file': '2010.02825v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Resistive Memory Organization',\n",
       "    'text': 'A resistive memory contains multiple independently controlled banks [45,50,71,89,94], similar to DRAM. A resistive memory bank (Figure 1) is composed of an array of memory cells organized into multiple subarrays (e.g., 64-128 [19, 20, 27, 43, 45, 50, 51, 71, 90-92, 94, 97]) of multiple rows (e.g., 512-1024 [19,20,27,43,45,50,51,71,[89][90][91][92]94]).',\n",
       "    'n_publication_ref': 19,\n",
       "    'n_figure_ref': 1},\n",
       "   {'heading': 'CPU Memory',\n",
       "    'text': 'Remapping and Swapping Subarrays. When a subarray receives many write accesses, WoLFRaM might decide to remap and swap the entire subarray (see Section 3.1.2). The process consists of two main steps. First, WoLFRaM selects a random subarray to perform the remap and swap operation. Second, WoLFRaM controller issues remap and swap commands to all blocks in the subarray. Because all subarrays in a bank share the row bu er [38,50,51,70,71,115] and the swap bu er, the remap and swap operation of each individual block is similar to the remap and swap operation within a subarray. The di erence is that for remapping and swapping a subarray, WoLFRaM reprograms the global PRAD instead of the local PRAD (see Figure 1).',\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'WoLFRaM: Enhancing Wear-Leveling and Fault Tolerance in Resistive Memories using Programmable Address Decoders',\n",
       "  'abstract': \"Resistive memories have limited lifetime caused by limited write endurance and highly non-uniform write access patterns. Two main techniques to mitigate endurance-related memory failures are 1) wear-leveling, to evenly distribute the writes across the entire memory, and 2) fault tolerance, to correct memory cell failures. However, one of the main open challenges in extending the lifetime of existing resistive memories is to make both techniques work together seamlessly and e ciently. To address this challenge, we propose WoLFRaM, a new mechanism that combines both wear-leveling and fault tolerance techniques at low cost by using a programmable resistive address decoder (PRAD). The key idea of WoLFRaM is to use PRAD for implementing 1) a new e cient wear-leveling mechanism that remaps write accesses to random physical locations on the y, and 2) a new e cient fault tolerance mechanism that recovers from faults by remapping failed memory blocks to available physical locations. Our evaluations show that, for a Phase Change Memory (PCM) based system with cell endurance of 10 8 writes, WoLFRaM increases the memory lifetime by 68% compared to a baseline that implements the best state-of-the-art wear-leveling and fault correction mechanisms. WoLFRaM's average / worst-case performance and energy overheads are 0.51% / 3.8% and 0.47% / 2.1% respectively.\",\n",
       "  'paddleOCR': [[[[434.0, 6.0], [459.0, 6.0], [459.0, 139.0], [434.0, 139.0]],\n",
       "    ['Local Row', 0.9998129606246948]],\n",
       "   [[[468.0, 16.0], [490.0, 16.0], [488.0, 125.0], [466.0, 125.0]],\n",
       "    ['Decoder', 0.9993225932121277]],\n",
       "   [[[657.0, 57.0], [816.0, 57.0], [816.0, 86.0], [657.0, 86.0]],\n",
       "    ['Subarray 0', 0.9518672823905945]],\n",
       "   [[[285.0, 86.0], [306.0, 86.0], [304.0, 241.0], [282.0, 241.0]],\n",
       "    ['Subarray Select', 0.9999105334281921]],\n",
       "   [[[958.0, 84.0], [986.0, 84.0], [986.0, 296.0], [958.0, 296.0]],\n",
       "    ['Memory Block', 0.9996726512908936]],\n",
       "   [[[432.0, 148.0], [457.0, 148.0], [459.0, 281.0], [434.0, 281.0]],\n",
       "    ['Local Row', 0.9996052980422974]],\n",
       "   [[[468.0, 163.0], [490.0, 163.0], [488.0, 271.0], [466.0, 271.0]],\n",
       "    ['Decoder', 0.9994505643844604]],\n",
       "   [[[178.0, 187.0], [206.0, 187.0], [206.0, 357.0], [178.0, 357.0]],\n",
       "    ['Global Row', 0.9992753267288208]],\n",
       "   [[[219.0, 210.0], [245.0, 211.0], [242.0, 335.0], [216.0, 334.0]],\n",
       "    ['Decoder', 0.9997072219848633]],\n",
       "   [[[657.0, 204.0], [815.0, 207.0], [814.0, 236.0], [657.0, 234.0]],\n",
       "    ['Subarray 1', 0.9660460352897644]],\n",
       "   [[[67.0, 233.0], [96.0, 233.0], [98.0, 359.0], [69.0, 360.0]],\n",
       "    ['[i+1...N-1]', 0.9782211184501648]],\n",
       "   [[[454.0, 296.0], [465.0, 296.0], [465.0, 323.0], [454.0, 323.0]],\n",
       "    ['...', 0.8698894381523132]],\n",
       "   [[[727.0, 298.0], [739.0, 298.0], [739.0, 322.0], [727.0, 322.0]],\n",
       "    ['...', 0.766651451587677]],\n",
       "   [[[432.0, 335.0], [457.0, 334.0], [459.0, 461.0], [434.0, 462.0]],\n",
       "    ['Local Row', 0.9995502829551697]],\n",
       "   [[[468.0, 345.0], [490.0, 346.0], [488.0, 452.0], [466.0, 452.0]],\n",
       "    ['Decoder', 0.999382495880127]],\n",
       "   [[[633.0, 389.0], [837.0, 389.0], [837.0, 416.0], [633.0, 416.0]],\n",
       "    ['Subarray N-1', 0.9597185254096985]],\n",
       "   [[[5.0, 404.0], [124.0, 407.0], [123.0, 434.0], [4.0, 431.0]],\n",
       "    ['Address', 0.9995150566101074]],\n",
       "   [[[11.0, 442.0], [114.0, 442.0], [114.0, 471.0], [11.0, 471.0]],\n",
       "    ['(s)!q N)', 0.8449373245239258]],\n",
       "   [[[631.0, 499.0], [845.0, 499.0], [845.0, 530.0], [631.0, 530.0]],\n",
       "    ['Row Buffer', 0.9996516108512878]]],\n",
       "  'ocr': [[[178.0, 187.0], [206.0, 187.0], [206.0, 357.0], [178.0, 357.0]],\n",
       "   ['Global Row', 0.9992753267288208]]},\n",
       " '2010.10651v1-Figure2-1.png': {'caption': 'Fig. 2: Overview of the exploration for skill set extension with',\n",
       "  'imageText': ['Fail',\n",
       "   'Precondition',\n",
       "   'discovery',\n",
       "   'Section',\n",
       "   'III-G',\n",
       "   'action(s)',\n",
       "   'Pre-condition',\n",
       "   'fulfillment',\n",
       "   'action(s)',\n",
       "   'and',\n",
       "   'goal',\n",
       "   'fulfillment',\n",
       "   'Extend',\n",
       "   'symbolic',\n",
       "   'description',\n",
       "   'Section',\n",
       "   'III-C',\n",
       "   'Completed',\n",
       "   'sequence',\n",
       "   'and',\n",
       "   'parameters',\n",
       "   'Key',\n",
       "   'actions',\n",
       "   'and',\n",
       "   'their',\n",
       "   'parameters',\n",
       "   'Run',\n",
       "   'completed',\n",
       "   'sequence',\n",
       "   'Success',\n",
       "   'Sequence',\n",
       "   'completion',\n",
       "   'Section',\n",
       "   'III-E',\n",
       "   'Sequence',\n",
       "   'sampling',\n",
       "   'Section',\n",
       "   'III-C',\n",
       "   'Fixed',\n",
       "   'actions',\n",
       "   '(can',\n",
       "   'be',\n",
       "   'empty)',\n",
       "   'Generalization',\n",
       "   'Section',\n",
       "   'III-D',\n",
       "   'GoalDemonstration(optional)'],\n",
       "  'image_file': '2010.10651v1-Figure2-1.png',\n",
       "  'sections': [{'heading': 'Precondition discovery',\n",
       "    'text': \"Section III-G Fail Fig. 2: Overview of the exploration for skill set extension with inputs , proposed algorithm components , a physics simulator and intermediate results .\\nno lid closing the container. Among the initial skills, there is none which has this goal as an effect, therefore planning will fail. Assume now that the agent already learned how to achieve this goal and that an appropriate skill was added to the symbolic description. If we want to reach the same goal, but now with a lid present on the container, the execution fails this time although planning succeeded, because the skill added earlier does not contain the precondition that no lid can be closing the container when attempting to place something into the container. The purpose of the exploration component is to find out how to achieve the goal and, upon success, to extend the symbolic description appropriately, such that the symbolic planner will be able to output valid plans for reaching the goal at hand in the future.\\nTo achieve this, Algorithm 1 is employed (visualized in Figure 2). At its core, sequences and their parameterizations are sampled, executed, and tested for success. To obtain a sequence, skills are sampled uniformly from the available ones (Section III-A) using the function SampleSequence. Since it can be inferred from other skills' preconditions, the navigation skill is excluded from sampling.\\nWhen parameterizing the sampled sequence using the function SampleParameters, we focus on sampling from entities that are likely to play a role in fulfilling the goal. More specifically, we sample from entities that occur in the goal specification (referred to as goal entities) and from entities that are spatially close to a goal entity (both returned by the function FindRelevantObjects). To determine which entities are spatially close, we include the ones that are located within a radius around a goal entity. As long as the exploration is not successful, this radius is increased successively.\\nOnce sequence and parametrization are determined, they are tested in a physics simulator using the Execute function. If the execution was successful, we test if the goal was reached by checking all goal predicates using the TestGoals function.\\nAfter a successful sequence was found, the symbolic domain description is extended. For this we determine the collective preconditions, parameters and effects of the sequence. Furthermore, new symbolic types are introduced for all parameter variables, branching off of the original types of the entities assigned to the parameter variables. Entities assigned to the variables are given the new types in addition to their existing ones. The reason for this is to keep an action only applicable to entities it was already tested with and thus ensuring that the symbolic description remains sound. For an example on this, refer to Figure 3.\\nIn the following sections, we introduce features of the exploration not explained in this section (such as SequenceCompletion and PreconditionDiscovery) that serve the purpose of making it more efficient and effective.\",\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 3}],\n",
       "  'title': 'Autonomous Extension of a Symbolic Mobile Manipulation Skill Set',\n",
       "  'abstract': \"Today's methods of programming mobile manipulation systems' behavior for operating in unstructured environments do not generalize well to unseen tasks or changes in the environment not anticipated at design time. Although symbolic planning makes this task more accessible to non-expert users by allowing a user to specify a desired goal, it reaches its limits when a task or the current environment is not soundly represented by the abstract domain or problem description. We propose a method that allows an agent to autonomously extend its skill set and thus the abstract description upon encountering such a situation. For this, we combine a set of four basic skills (grasp, place, navigate, move) with an off-the-shelf symbolic planner upon which we base a skill sequence exploration scheme. To make the search over skill sequences more efficient and effective, we introduce strategies for generalizing from previous experience, completing sequences of key skills and discovering preconditions. The resulting system is evaluated in simulation using object rearrangement tasks. We can show qualitatively that the skill set extension works as expected and quantitatively that our strategies for more efficient search make the approach computationally tractable.\",\n",
       "  'paddleOCR': [[[[20.0, 23.0], [215.0, 23.0], [215.0, 47.0], [20.0, 47.0]],\n",
       "    ['Demonstration', 0.9998540282249451]],\n",
       "   [[[337.0, 36.0], [403.0, 36.0], [403.0, 64.0], [337.0, 64.0]],\n",
       "    ['Goal', 0.999509334564209]],\n",
       "   [[[57.0, 56.0], [173.0, 53.0], [174.0, 81.0], [58.0, 84.0]],\n",
       "    ['(optional)', 0.9996070861816406]],\n",
       "   [[[416.0, 198.0], [465.0, 195.0], [466.0, 220.0], [418.0, 223.0]],\n",
       "    ['Fail', 0.9944040775299072]],\n",
       "   [[[114.0, 218.0], [303.0, 220.0], [303.0, 244.0], [114.0, 241.0]],\n",
       "    ['Generalization', 0.9997841119766235]],\n",
       "   [[[550.0, 221.0], [748.0, 221.0], [748.0, 245.0], [550.0, 245.0]],\n",
       "    ['Run completed', 0.9990116953849792]],\n",
       "   [[[134.0, 253.0], [284.0, 253.0], [284.0, 276.0], [134.0, 276.0]],\n",
       "    ['Section III-D', 0.9631704688072205]],\n",
       "   [[[585.0, 255.0], [712.0, 255.0], [712.0, 278.0], [585.0, 278.0]],\n",
       "    ['sequence', 0.9990674257278442]],\n",
       "   [[[606.0, 343.0], [700.0, 343.0], [700.0, 363.0], [606.0, 363.0]],\n",
       "    ['Success', 0.9991336464881897]],\n",
       "   [[[79.0, 370.0], [340.0, 371.0], [339.0, 395.0], [78.0, 393.0]],\n",
       "    ['Fixed actions (can be.', 0.9833825826644897]],\n",
       "   [[[165.0, 401.0], [253.0, 401.0], [253.0, 433.0], [165.0, 433.0]],\n",
       "    ['empty)', 0.9996309876441956]],\n",
       "   [[[496.0, 427.0], [800.0, 429.0], [800.0, 453.0], [495.0, 450.0]],\n",
       "    ['Precondition discovery', 0.9997606873512268]],\n",
       "   [[[572.0, 460.0], [723.0, 460.0], [723.0, 484.0], [572.0, 484.0]],\n",
       "    ['Section III-G', 0.9835913181304932]],\n",
       "   [[[80.0, 521.0], [336.0, 523.0], [336.0, 547.0], [80.0, 544.0]],\n",
       "    ['Sequence sampling', 0.9998341798782349]],\n",
       "   [[[135.0, 554.0], [285.0, 554.0], [285.0, 577.0], [135.0, 577.0]],\n",
       "    ['Section III-C', 0.9893097281455994]],\n",
       "   [[[567.0, 569.0], [729.0, 569.0], [729.0, 592.0], [567.0, 592.0]],\n",
       "    ['Pre-condition', 0.9994467496871948]],\n",
       "   [[[532.0, 598.0], [761.0, 601.0], [761.0, 628.0], [532.0, 625.0]],\n",
       "    ['fulfillment action(s)', 0.9842044115066528]],\n",
       "   [[[535.0, 635.0], [765.0, 635.0], [765.0, 658.0], [535.0, 658.0]],\n",
       "    ['and goal fulfillment', 0.998576283454895]],\n",
       "   [[[80.0, 672.0], [338.0, 671.0], [338.0, 694.0], [80.0, 695.0]],\n",
       "    ['Key actions and their..', 0.971535325050354]],\n",
       "   [[[594.0, 666.0], [701.0, 666.0], [701.0, 694.0], [594.0, 694.0]],\n",
       "    ['action(s)', 0.999846875667572]],\n",
       "   [[[138.0, 707.0], [281.0, 703.0], [281.0, 726.0], [139.0, 730.0]],\n",
       "    ['parameters', 0.998498260974884]],\n",
       "   [[[541.0, 760.0], [756.0, 760.0], [756.0, 782.0], [541.0, 782.0]],\n",
       "    ['Extend symbolic', 0.9957857131958008]],\n",
       "   [[[573.0, 789.0], [723.0, 791.0], [723.0, 818.0], [573.0, 816.0]],\n",
       "    ['description', 0.999898374080658]],\n",
       "   [[[67.0, 823.0], [352.0, 823.0], [352.0, 845.0], [67.0, 845.0]],\n",
       "    ['Sequence completion', 0.9997448921203613]],\n",
       "   [[[573.0, 823.0], [723.0, 823.0], [723.0, 846.0], [573.0, 846.0]],\n",
       "    ['Section III-C', 0.9920406341552734]],\n",
       "   [[[135.0, 854.0], [284.0, 854.0], [284.0, 877.0], [135.0, 877.0]],\n",
       "    ['Section III-E', 0.9936196804046631]],\n",
       "   [[[80.0, 982.0], [340.0, 985.0], [339.0, 1008.0], [80.0, 1006.0]],\n",
       "    ['Completed sequence', 0.9992926716804504]],\n",
       "   [[[114.0, 1017.0], [306.0, 1017.0], [306.0, 1041.0], [114.0, 1041.0]],\n",
       "    ['and parameters', 0.9997954964637756]]],\n",
       "  'ocr': [[[541.0, 760.0], [756.0, 760.0], [756.0, 782.0], [541.0, 782.0]],\n",
       "   ['Extend symbolic', 0.9957857131958008]]},\n",
       " '2010.10872v1-Figure6-1.png': {'caption': 'Fig. 6. A simplified satellite architecture with example compromise scenarios for onboard sub-systems.',\n",
       "  'imageText': ['Satellite',\n",
       "   'Control',\n",
       "   'Bus',\n",
       "   'scientific',\n",
       "   'module',\n",
       "   'injects',\n",
       "   'bus',\n",
       "   'commands',\n",
       "   'to',\n",
       "   'propulsion',\n",
       "   'system',\n",
       "   'causing',\n",
       "   'spin-out.',\n",
       "   'Mission',\n",
       "   'Payload',\n",
       "   'Backdoor',\n",
       "   'in',\n",
       "   '3rd-party',\n",
       "   'premature',\n",
       "   'end',\n",
       "   'of',\n",
       "   'life.',\n",
       "   'Power',\n",
       "   'Management',\n",
       "   'Malware',\n",
       "   'infection',\n",
       "   'disables',\n",
       "   'solar',\n",
       "   'charging',\n",
       "   'and',\n",
       "   'leads',\n",
       "   'to',\n",
       "   'event.',\n",
       "   'Thermal',\n",
       "   'Control',\n",
       "   'Falsified',\n",
       "   'temperature',\n",
       "   'measurements',\n",
       "   'lead',\n",
       "   'to',\n",
       "   'system',\n",
       "   'overheat/freeze',\n",
       "   'keeping',\n",
       "   'leads',\n",
       "   'to',\n",
       "   'premature',\n",
       "   'end',\n",
       "   'of',\n",
       "   'life.',\n",
       "   'Propulsion&',\n",
       "   'Reaction',\n",
       "   'Control',\n",
       "   'Tampered',\n",
       "   'fuel',\n",
       "   'record-',\n",
       "   'collision.',\n",
       "   'Positioning&',\n",
       "   'Navigation',\n",
       "   'Compromised',\n",
       "   'GPS',\n",
       "   'sensors',\n",
       "   'leads',\n",
       "   'to',\n",
       "   'inaccurate',\n",
       "   'orbit',\n",
       "   'determination',\n",
       "   'and',\n",
       "   'debris',\n",
       "   'functionality\"',\n",
       "   'safe-mode.\"',\n",
       "   'operating',\n",
       "   'system',\n",
       "   'puts',\n",
       "   'spacecraft',\n",
       "   'into',\n",
       "   'limited',\n",
       "   'On',\n",
       "   'Board',\n",
       "   'Computer',\n",
       "   'Malware',\n",
       "   'infection',\n",
       "   'in',\n",
       "   'real',\n",
       "   'time',\n",
       "   'implementation',\n",
       "   'permits',\n",
       "   'unauthenticated',\n",
       "   'commanding.',\n",
       "   'Telemetry',\n",
       "   'Logic',\n",
       "   'error',\n",
       "   'in',\n",
       "   'cryptographic'],\n",
       "  'image_file': '2010.10872v1-Figure6-1.png',\n",
       "  'sections': [{'heading': 'Satellite Control Bus',\n",
       "    'text': 'Fig. 6. A simplified satellite architecture with example compromise scenarios for onboard sub-systems.\\nrarely monolithic, incorporating third-party code for various components and increasing the risk of software backdoors.\\nIn sum, payload security is a critical but understudied topic. Prior work has demonstrated a wide range of severe and unmitigated attack vectors. While barriers to research are particularly acute, there is clear need for future technical work.',\n",
       "    'n_publication_ref': 0,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'SOK: Building a Launchpad for Impactful Satellite Cyber-Security Research',\n",
       "  'abstract': 'As the space industry approaches a period of rapid change, securing both emerging and legacy satellite missions will become vital. However, space technology has been largely overlooked by the systems security community. This systematization of knowledge paper seeks to understand why this is the case and to offer a starting point for technical security researchers seeking impactful contributions beyond the Earth\\'s mesosphere. The paper begins with a cross-disciplinary synthesis of relevant threat models from a diverse array of fields, ranging from legal and policy studies to aerospace engineering. This is presented as a \"threat matrix toolbox\" which security researchers may leverage to motivate technical research into given attack vectors and defenses. We subsequently apply this model to an original chronology of more than 100 significant satellite hacking incidents spanning the previous 60 years. Together, these are used to assess the state-of-the-art in satellite security across four sub-domains: satellite radio-link security, space hardware security, ground station security, and operational/mission security. In each area, we note significant findings and unresolved questions lingering in other disciplines which the systems security community is aptly poised to tackle. By consolidating this research, we present the case that satellite systems security researchers can build on strong, but disparate, academic foundations and rise to meet an urgent need for future space missions.',\n",
       "  'paddleOCR': [[[[117.0, 19.0], [322.0, 27.0], [320.0, 73.0], [115.0, 65.0]],\n",
       "    ['Telemetry', 0.9998588562011719]],\n",
       "   [[[755.0, 22.0], [1171.0, 24.0], [1170.0, 68.0], [755.0, 66.0]],\n",
       "    ['On Board Computer', 0.9999139308929443]],\n",
       "   [[[1529.0, 15.0], [2039.0, 24.0], [2038.0, 77.0], [1528.0, 68.0]],\n",
       "    ['Positioning& Navigation', 0.9884476065635681]],\n",
       "   [[[105.0, 94.0], [332.0, 94.0], [332.0, 131.0], [105.0, 131.0]],\n",
       "    ['Logic error in', 0.9860978722572327]],\n",
       "   [[[716.0, 96.0], [1211.0, 96.0], [1211.0, 127.0], [716.0, 127.0]],\n",
       "    ['Malware infection in real time', 0.9962664246559143]],\n",
       "   [[[1548.0, 94.0], [2024.0, 94.0], [2024.0, 131.0], [1548.0, 131.0]],\n",
       "    ['Compromised GPS sensors', 0.999477744102478]],\n",
       "   [[[101.0, 134.0], [339.0, 134.0], [339.0, 177.0], [101.0, 177.0]],\n",
       "    ['cryptographic', 0.9998271465301514]],\n",
       "   [[[457.0, 138.0], [617.0, 138.0], [617.0, 177.0], [457.0, 177.0]],\n",
       "    ['Satellite', 0.9995524883270264]],\n",
       "   [[[776.0, 140.0], [1151.0, 140.0], [1151.0, 171.0], [776.0, 171.0]],\n",
       "    ['operating system puts', 0.9998326897621155]],\n",
       "   [[[1580.0, 140.0], [1989.0, 140.0], [1989.0, 171.0], [1580.0, 171.0]],\n",
       "    ['Ieads to inaccurate orbit', 0.9624209403991699]],\n",
       "   [[[19.0, 182.0], [418.0, 182.0], [418.0, 212.0], [19.0, 212.0]],\n",
       "    ['implementation permits', 0.9712648987770081]],\n",
       "   [[[774.0, 180.0], [1151.0, 177.0], [1151.0, 215.0], [774.0, 217.0]],\n",
       "    ['spacecraft into limited', 0.9994811415672302]],\n",
       "   [[[461.0, 191.0], [613.0, 191.0], [613.0, 230.0], [461.0, 230.0]],\n",
       "    ['Control', 0.999882698059082]],\n",
       "   [[[1576.0, 182.0], [1994.0, 182.0], [1994.0, 212.0], [1576.0, 212.0]],\n",
       "    ['determination and debris.', 0.9906467199325562]],\n",
       "   [[[79.0, 219.0], [354.0, 219.0], [354.0, 256.0], [79.0, 256.0]],\n",
       "    ['unauthenticated', 0.9998455047607422]],\n",
       "   [[[748.0, 223.0], [1175.0, 223.0], [1175.0, 254.0], [748.0, 254.0]],\n",
       "    ['functionality\" safe-mode.\".', 0.9747941493988037]],\n",
       "   [[[1709.0, 221.0], [1859.0, 221.0], [1859.0, 258.0], [1709.0, 258.0]],\n",
       "    ['collision.', 0.9767734408378601]],\n",
       "   [[[496.0, 235.0], [582.0, 244.0], [578.0, 288.0], [492.0, 279.0]],\n",
       "    ['Bus', 0.9992664456367493]],\n",
       "   [[[99.0, 258.0], [333.0, 263.0], [332.0, 300.0], [98.0, 296.0]],\n",
       "    ['commanding.', 0.9998944997787476]],\n",
       "   [[[570.0, 329.0], [592.0, 329.0], [592.0, 350.0], [570.0, 350.0]],\n",
       "    ['A', 0.7557607889175415]],\n",
       "   [[[84.0, 445.0], [337.0, 445.0], [337.0, 489.0], [84.0, 489.0]],\n",
       "    ['Propulsion&', 0.9996806979179382]],\n",
       "   [[[530.0, 445.0], [864.0, 445.0], [864.0, 489.0], [530.0, 489.0]],\n",
       "    ['Thermal Control', 0.9997484683990479]],\n",
       "   [[[993.0, 445.0], [1415.0, 447.0], [1415.0, 493.0], [992.0, 491.0]],\n",
       "    ['Power Management', 0.9998914003372192]],\n",
       "   [[[1621.0, 447.0], [1957.0, 447.0], [1957.0, 491.0], [1621.0, 491.0]],\n",
       "    ['Mission Payload', 0.9998179078102112]],\n",
       "   [[[47.0, 499.0], [390.0, 499.0], [390.0, 537.0], [47.0, 537.0]],\n",
       "    ['Reaction Control', 0.9999350309371948]],\n",
       "   [[[519.0, 517.0], [875.0, 517.0], [875.0, 548.0], [519.0, 548.0]],\n",
       "    ['Falsified temperature', 0.9999121427536011]],\n",
       "   [[[965.0, 515.0], [1430.0, 517.0], [1430.0, 554.0], [965.0, 552.0]],\n",
       "    ['Malware infection disables', 0.9996315836906433]],\n",
       "   [[[1604.0, 512.0], [1972.0, 517.0], [1972.0, 554.0], [1603.0, 550.0]],\n",
       "    ['Backdoor in 3rd-party', 0.9998239874839783]],\n",
       "   [[[36.0, 565.0], [399.0, 565.0], [399.0, 602.0], [36.0, 602.0]],\n",
       "    ['Tampered fuel record-', 0.9960564970970154]],\n",
       "   [[[508.0, 556.0], [883.0, 556.0], [883.0, 587.0], [508.0, 587.0]],\n",
       "    ['measurements lead to', 0.9805529713630676]],\n",
       "   [[[967.0, 559.0], [1434.0, 559.0], [1434.0, 596.0], [967.0, 596.0]],\n",
       "    ['solar charging and leads to', 0.9920912384986877]],\n",
       "   [[[1550.0, 556.0], [2026.0, 559.0], [2026.0, 596.0], [1550.0, 594.0]],\n",
       "    ['scientific module injects bus', 0.9998666644096375]],\n",
       "   [[[495.0, 598.0], [896.0, 598.0], [896.0, 635.0], [495.0, 635.0]],\n",
       "    ['system overheat/freeze', 0.9996362924575806]],\n",
       "   [[[75.0, 609.0], [362.0, 609.0], [362.0, 646.0], [75.0, 646.0]],\n",
       "    ['keeping leads tos.', 0.9197695851325989]],\n",
       "   [[[1018.0, 602.0], [1380.0, 600.0], [1381.0, 637.0], [1018.0, 640.0]],\n",
       "    ['premature end of life.', 0.9622718691825867]],\n",
       "   [[[1578.0, 602.0], [1998.0, 602.0], [1998.0, 640.0], [1578.0, 640.0]],\n",
       "    ['commands to propulsion', 0.9971936941146851]],\n",
       "   [[[34.0, 653.0], [396.0, 648.0], [397.0, 686.0], [35.0, 690.0]],\n",
       "    ['premature end of life.', 0.9999033212661743]],\n",
       "   [[[643.0, 644.0], [746.0, 644.0], [746.0, 677.0], [643.0, 677.0]],\n",
       "    ['event.', 0.9939301609992981]],\n",
       "   [[[1578.0, 644.0], [1998.0, 644.0], [1998.0, 681.0], [1578.0, 681.0]],\n",
       "    ['system causing spin-out.', 0.9851159453392029]]],\n",
       "  'ocr': [[[1529.0, 15.0], [2039.0, 24.0], [2038.0, 77.0], [1528.0, 68.0]],\n",
       "   ['Positioning& Navigation', 0.9884476065635681]]},\n",
       " '2101.03207v1-Figure1-1.png': {'caption': 'Figure 1: Model Overview',\n",
       "  'imageText': ['loss',\n",
       "   'concat',\n",
       "   'Predicted',\n",
       "   'Label',\n",
       "   'True',\n",
       "   'Label',\n",
       "   'Transformer',\n",
       "   'Encoder',\n",
       "   'ClassiÔ¨Åer',\n",
       "   'Static',\n",
       "   'Transformer',\n",
       "   'emoji2vec',\n",
       "   'Hashtags',\n",
       "   '#BBMAsTopSocial',\n",
       "   '#JUNGKOOK',\n",
       "   '#Ï†ïÍµ≠',\n",
       "   'Emojis',\n",
       "   'Cleaned',\n",
       "   'Text',\n",
       "   ':',\n",
       "   'Dont',\n",
       "   'disturb',\n",
       "   'please,',\n",
       "   'he',\n",
       "   'is',\n",
       "   'enjoying',\n",
       "   'his',\n",
       "   'snacks',\n",
       "   'while',\n",
       "   'making',\n",
       "   'those',\n",
       "   'little',\n",
       "   'dance',\n",
       "   'BTS',\n",
       "   '#BBMAsTopSocial',\n",
       "   'BTS',\n",
       "   '#JUNGKOOK',\n",
       "   '#Ï†ïÍµ≠‚Ä¶',\n",
       "   'RT',\n",
       "   '@jeonggukpics:',\n",
       "   'Don‚Äôt',\n",
       "   'disturb',\n",
       "   'please,',\n",
       "   'he',\n",
       "   'is',\n",
       "   'enjoying',\n",
       "   'his',\n",
       "   'snacks',\n",
       "   'while',\n",
       "   'making',\n",
       "   'those',\n",
       "   'little',\n",
       "   'dance',\n",
       "   'Raw',\n",
       "   'Tweet',\n",
       "   'Text'],\n",
       "  'image_file': '2101.03207v1-Figure1-1.png',\n",
       "  'sections': [{'heading': 'Proposed Transformer-based Models',\n",
       "    'text': \"We leverage Transformer-based [1] masked language models to generate semantic embeddings for the cleaned tweet text. In addition to the cleaned tweet's embedding, we generate and utilize semantic vector representations for all the emojis and segmented hashtags available within the tweet. The segmented hash embeddings are generated using the same pre-trained Transformer model such that the text and hashtag embeddings are grounded in the same latent space. emoji2vec is used to create the emojis' semantic embeddings. The Transformer layers encoding the cleaned tweet text are updated during the fine-tuning process on the available training data. For classification, we use the concatenation of the cleaned tweet's embedding with the collective embedding vector for segmented hashtags and emojis. We are required to encode a list of emojis & a list of segmented hashtags, both of which can be of variable lengths. Therefore, we average the vector representations of all the individual emojis or segmented hashtags as the case may be, to generate the centralised emoji or hashtag representation. This is simple, intuitive, and earlier work on averaging local word embeddings to generate global sentence embeddings [19] has showed that this yields a comprehensive vector representation for sentences. We assume the same to hold true for emojis and hashtags as well.\\nThe concatenated feature-set is then passed to a two layer multi-layer perceptron (MLP). The loss from the classifier is propagated back through the cleaned tweet Transformer encoder during training. We experimented with XLM-RoBERTa (XLMR) [20]  models such as mBERT(multilingual BERT) [2] and multilingual-distilBERT [21] on various downstream tasks. We therefore chose XLMR as our base Transformer model for the purpose of the shared task. A high level overview of our model flow is shown in figure 1.\\nFor fine-tuning our XLMR Transformer weights, we perform learning rate scheduling based on the actual computed macro F1-scores on the validation split instead of using the validation loss. As opposed to simply using early-stopping to prevent overfitting, we consider the change in validation performance at the end of each training iteration. If the validation performance goes down across an iteration, we trace back to the previous model weights and scale down our learning rate. Training stops when the learning rate reaches a very small value 6 . Although expensive, this form of scheduling ensures that we maximize our Macro F1-score on the validation split. For further details on specific implementation nuances and choice of hyperparameters, refer to Section 6.\",\n",
       "    'n_publication_ref': 6,\n",
       "    'n_figure_ref': 1}],\n",
       "  'title': 'Leveraging Multilingual Transformers for Hate Speech Detection',\n",
       "  'abstract': \"Detecting and classifying instances of hate in social media text has been a problem of interest in Natural Language Processing in the recent years. Our work leverages state of the art Transformer language models to identify hate speech in a multilingual setting. Capturing the intent of a post or a comment on social media involves careful evaluation of the language style, semantic content and additional pointers such as hashtags and emojis. In this paper, we look at the problem of identifying whether a Twitter post is hateful and offensive or not. We further discriminate the detected toxic content into one of the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN) and (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text encoder at the base, we are able to successfully identify and classify hate speech from multiple languages. On the provided testing corpora, we achieve Macro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi respectively while performing hate speech detection and of 60.70, 53.28 and 49.74 during fine-grained classification. In our experiments, we show the efficacy of Perspective API features for hate speech classification and the effects of exploiting a multilingual training scheme. A feature selection study is provided to illustrate impacts of specific features upon the architecture's classification head.\",\n",
       "  'paddleOCR': [[[[524.0, 11.0], [651.0, 14.0], [650.0, 55.0], [522.0, 52.0]],\n",
       "    ['Emojis', 0.9997916221618652]],\n",
       "   [[[836.0, 94.0], [1016.0, 94.0], [1016.0, 132.0], [836.0, 132.0]],\n",
       "    ['emoji2vec', 0.9995659589767456]],\n",
       "   [[[56.0, 222.0], [345.0, 222.0], [345.0, 255.0], [56.0, 255.0]],\n",
       "    ['Raw Tweet Text', 0.9822397828102112]],\n",
       "   [[[1305.0, 256.0], [1473.0, 256.0], [1473.0, 288.0], [1305.0, 288.0]],\n",
       "    ['True Label', 0.9996479749679565]],\n",
       "   [[[474.0, 305.0], [715.0, 305.0], [715.0, 338.0], [474.0, 338.0]],\n",
       "    ['Cleaned Text.', 0.9422107338905334]],\n",
       "   [[[12.0, 337.0], [389.0, 337.0], [389.0, 374.0], [12.0, 374.0]],\n",
       "    [\"RT @jeonggukpics: Don't\", 0.994255542755127]],\n",
       "   [[[55.0, 384.0], [343.0, 384.0], [343.0, 415.0], [55.0, 415.0]],\n",
       "    ['disturb please, he is', 0.9996641874313354]],\n",
       "   [[[468.0, 381.0], [715.0, 382.0], [715.0, 411.0], [468.0, 409.0]],\n",
       "    [': Dont disturb please,', 0.9995397925376892]],\n",
       "   [[[820.0, 410.0], [1033.0, 414.0], [1033.0, 447.0], [819.0, 443.0]],\n",
       "    ['Transformer', 0.9998475909233093]],\n",
       "   [[[16.0, 431.0], [381.0, 428.0], [381.0, 459.0], [16.0, 463.0]],\n",
       "    ['enjoying his snacks while', 0.9623305797576904]],\n",
       "   [[[490.0, 423.0], [693.0, 423.0], [693.0, 450.0], [490.0, 450.0]],\n",
       "    ['he is enjoying his.', 0.9712087512016296]],\n",
       "   [[[1111.0, 436.0], [1223.0, 436.0], [1223.0, 464.0], [1111.0, 464.0]],\n",
       "    ['concat', 0.9997448325157166]],\n",
       "   [[[1306.0, 434.0], [1470.0, 434.0], [1470.0, 472.0], [1306.0, 472.0]],\n",
       "    ['Classifier', 0.9997404217720032]],\n",
       "   [[[470.0, 459.0], [715.0, 461.0], [715.0, 492.0], [470.0, 491.0]],\n",
       "    ['snacks while making.', 0.9824720621109009]],\n",
       "   [[[852.0, 458.0], [1002.0, 458.0], [1002.0, 495.0], [852.0, 495.0]],\n",
       "    ['Encoder', 0.99964839220047]],\n",
       "   [[[19.0, 475.0], [379.0, 475.0], [379.0, 506.0], [19.0, 506.0]],\n",
       "    ['making those little dance', 0.9999403953552246]],\n",
       "   [[[465.0, 502.0], [721.0, 498.0], [721.0, 525.0], [465.0, 529.0]],\n",
       "    ['those little dance BTS', 0.9990219473838806]],\n",
       "   [[[30.0, 566.0], [368.0, 565.0], [368.0, 598.0], [30.0, 599.0]],\n",
       "    ['#BBMAsTopSocial BTS', 0.9772497415542603]],\n",
       "   [[[1214.0, 577.0], [1278.0, 588.0], [1272.0, 623.0], [1208.0, 612.0]],\n",
       "    ['loss', 0.9983539581298828]],\n",
       "   [[[31.0, 610.0], [364.0, 610.0], [364.0, 642.0], [31.0, 642.0]],\n",
       "    ['#JUNGKOOK #33...', 0.9408791065216064]],\n",
       "   [[[513.0, 695.0], [692.0, 700.0], [690.0, 744.0], [511.0, 739.0]],\n",
       "    ['Hashtags', 0.9998440146446228]],\n",
       "   [[[1312.0, 719.0], [1465.0, 719.0], [1465.0, 752.0], [1312.0, 752.0]],\n",
       "    ['Predicted', 0.9996667504310608]],\n",
       "   [[[875.0, 750.0], [980.0, 750.0], [980.0, 790.0], [875.0, 790.0]],\n",
       "    ['Static', 0.9999275803565979]],\n",
       "   [[[1340.0, 761.0], [1436.0, 761.0], [1436.0, 796.0], [1340.0, 796.0]],\n",
       "    ['Label', 0.9980863332748413]],\n",
       "   [[[496.0, 774.0], [715.0, 774.0], [715.0, 805.0], [496.0, 805.0]],\n",
       "    ['#BBMAsTopSocial', 0.9993168711662292]],\n",
       "   [[[821.0, 799.0], [1035.0, 802.0], [1034.0, 837.0], [821.0, 833.0]],\n",
       "    ['Transformer', 0.9998906850814819]],\n",
       "   [[[521.0, 812.0], [690.0, 812.0], [690.0, 838.0], [521.0, 838.0]],\n",
       "    ['#JUNGKOOK', 0.9996374845504761]],\n",
       "   [[[565.0, 843.0], [646.0, 843.0], [646.0, 878.0], [565.0, 878.0]],\n",
       "    ['#3F', 0.8034917712211609]]],\n",
       "  'ocr': [[[55.0, 384.0], [343.0, 384.0], [343.0, 415.0], [55.0, 415.0]],\n",
       "   ['disturb please, he is', 0.9996641874313354]]}}"
      ]
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_meta(img_source_dir, raw_data, file_index):\n",
    "    \n",
    "    # (1) Load image\n",
    "    all_files = os.listdir(img_source_dir)\n",
    "    all_files.sort()\n",
    "\n",
    "    image_file_name = all_files[file_index]\n",
    "    image_path = img_source_dir + image_file_name\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # (2) Encode image\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # (3) Load associated metadata\n",
    "    img_data = raw_data[all_files[file_index]]\n",
    "    \n",
    "    return image_file_name, image, base64_image, img_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Analyze the image and its metadata to generate two questions and answers considering the spatial arrangement of the text. \\\n",
    "    Provide the result in json format, were each pair of question and answer is a dict, and the whole responce is a list of dicts. \\\n",
    "    Avoid questions that requre solely the description of elements of the image. \\\n",
    "    Text data should be used as much as possible. \\\n",
    "    Do not use neither in question, nor in answer any visual elements (arrows, shapes, lines) of the schema, their colour, style or shape. \\\n",
    "    Do not mention the image/schema/figure as a whole. \\\n",
    "    To answer a question it should be sufficient to look at the image only, without usage of external sources of infromation\"\n",
    "\n",
    "def get_question_answer_pairs(prompt, base64_image, img_descr, openai_key):\n",
    "    # (1) Call open AI \n",
    "    openai.api_key = openai_key\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": f\"Image metadata: {img_descr}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "\n",
    "    # (2) Parse request\n",
    "    response_parsed = response.choices[0].message.content\n",
    "    pattern = r'```(?:json)?\\n(.*?)\\n```'\n",
    "    match = re.search(pattern, response_parsed, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_content = match.group(1).strip()\n",
    "        try:\n",
    "            response_parsed_clean = json.loads(json_content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding failed: {e}\")\n",
    "    else:\n",
    "        print(\"No JSON content found.\")\n",
    "\n",
    "    return response_parsed_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# for file_index in range(18, 78):  # Indices from 0 to 77\n",
    "#     image_file_name, image, base64_image, img_data = load_image_and_meta(img_source_dir, raw_data, file_index)\n",
    "    \n",
    "#     # Display image and metadata\n",
    "#     print(f\"{file_index}\\n\")\n",
    "#     print(f\"Caption: {img_data.get('caption', 'No caption')}\")\n",
    "#     print(f\"Title: {img_data.get('title', 'No title')}\")\n",
    "#     display(image)\n",
    "    \n",
    "#     # Wait for 5 seconds\n",
    "#     time.sleep(2)\n",
    "    \n",
    "#     # Clear the output to display the next image\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: 2101.03207v1-Figure1-1.png\n",
      "Caption: Figure 1: Model Overview\n",
      "Titile: Leveraging Multilingual Transformers for Hate Speech Detection\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1498x906>"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_index = 7 # Starting from 22 inclusive, no visual elements described\n",
    "image_file_name, image, base64_image, img_data = load_image_and_meta(img_source_dir, raw_data, file_index)\n",
    "\n",
    "# print(f\"Base 64 img: {base64_image}\")\n",
    "print(f\"File name: {image_file_name}\")\n",
    "print(f\"Caption: {img_data['caption']}\")\n",
    "print(f\"Titile: {img_data['title']}\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How are emojis processed in the system?',\n",
       "  'answer': 'Emojis are converted using emoji2vec before being concatenated with the output of the Transformer Encoder.'},\n",
       " {'question': 'What components are involved in generating the predicted label?',\n",
       "  'answer': 'The predicted label is generated by the Classifier, which takes inputs from the concatenation of the Transformer Encoder and emoji2vec outputs, as well as the Static Transformer.'}]"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_descr = img_data['caption']\n",
    "response_parsed_clean = get_question_answer_pairs(prompt, base64_image, img_descr, OPENAI_API_KEY)\n",
    "response_parsed_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How are emojis processed in the system?',\n",
       "  'answer': 'Emojis are converted using emoji2vec before being concatenated with the output of the Transformer Encoder.'},\n",
       " {'question': 'What components are involved in generating the predicted label?',\n",
       "  'answer': 'The predicted label is generated by the Classifier, which takes inputs from the concatenation of the Transformer Encoder and emoji2vec outputs, as well as the Static Transformer.'}]"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_parsed_clean[0]['question'] = \"How are emojis processed in the system?\"\n",
    "response_parsed_clean[0]['answer'] = \"Emojis are converted using emoji2vec before being concatenated with the output of the Transformer Encoder.\"\n",
    "response_parsed_clean[1]['question'] = \"What components are involved in generating the predicted label?\"\n",
    "response_parsed_clean[1]['answer'] = \"The predicted label is generated by the Classifier, which takes inputs from the concatenation of the Transformer Encoder and emoji2vec outputs, as well as the Static Transformer.\"\n",
    "response_parsed_clean_save = response_parsed_clean\n",
    "response_parsed_clean_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Add questions and answers to the request and save dataset\n",
    "\n",
    "with open(output_file_path, 'r') as file:\n",
    "    file_with_questions = json.load(file)\n",
    "    \n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file_with_questions[image_file_name]['questions'] = response_parsed_clean_save\n",
    "    json.dump(file_with_questions, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset for it to be ready for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ./data/images/2102.03044v2-Figure2.7-1.png\n",
      "File not found: ./data/images/2103.07592v2-Figure1-1.png\n",
      "File not found: ./data/images/2103.08622v1-Figure1-1.png\n",
      "File not found: ./data/images/2202.07734v2-Figure2-1.png\n",
      "Deleted: ./data/images/2204.00034v1-Figure1-1.png\n",
      "File not found: ./data/images/2204.04738v1-Figure1-1.png\n",
      "File not found: ./data/images/2205.02090v1-Figure5-1.png\n",
      "File not found: ./data/images/2205.08731v1-Figure1-1.png\n"
     ]
    }
   ],
   "source": [
    "# # 40, (53), 54, (71), 77, (80), (85), (86) \n",
    "\n",
    "img_source_dir = './data/images/'\n",
    "to_remove = ['2102.03044v2-Figure2.7-1.png', '2103.07592v2-Figure1-1.png',\n",
    "             '2103.08622v1-Figure1-1.png', '2202.07734v2-Figure2-1.png',\n",
    "             '2204.00034v1-Figure1-1.png', '2204.04738v1-Figure1-1.png',\n",
    "             '2205.02090v1-Figure5-1.png', '2205.08731v1-Figure1-1.png'\n",
    "             ]\n",
    "\n",
    "for filename in to_remove:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(img_source_dir, filename)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'r') as file:\n",
    "    file_with_questions = json.load(file)\n",
    "    \n",
    "with open(img_meta_file_path, 'r') as file:\n",
    "    img_meta_file = json.load(file)\n",
    "\n",
    "for key in to_remove:\n",
    "    file_with_questions.pop(key, None) \n",
    "    img_meta_file.pop(key, None) \n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(file_with_questions, file, indent=4)\n",
    "\n",
    "with open(img_meta_file_path, 'w') as file:\n",
    "    json.dump(img_meta_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
