{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are an assistant, which answers questions for the given text and *optional* image as a context. \"\n",
    "\"If it's indicated that an image is passed - then pay much more attention to the image to capture text and structure from there.\"\n",
    "\"Don't use your prior knowledge of the particalar object passed to you. Base your answer on the info given from the user.\"\n",
    "\"If you can't answet the question based on input, say `no answer`. Don't come up with wage explanations.\"\n",
    "\n",
    "TASK_PROMPT_TEXT_INPUT = \"Given the context with text description of a diagram, create a short answer for the following question: {}\"\n",
    "\n",
    "TASK_PROMPT_TEXT_IMAGE_INPUT = \"Given the context with text description of a diagram and the *diagram image itself*, create a short answer for the following question: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "alibaba_client = OpenAI(\n",
    "    api_key=os.getenv(\"ALIBABA_API_KEY\"),\n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTROPIC_API_KEY'))\n",
    "\n",
    "\n",
    "def load_image(img_fpath):\n",
    "    image = Image.open(img_fpath)\n",
    "    with open(img_fpath, 'rb') as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return image, base64_image\n",
    "\n",
    "    \n",
    "def build_img_decription(img_data: dict) -> str:\n",
    "    caption = f\"Caption: `{img_data['caption']}`\"\n",
    "    ocr_text = f\"Ocr text: `{' '.join([t for t in img_data['imageText']])}`\" \n",
    "    title = f\"Title: `{img_data['title']}`\"\n",
    "    img_descr = \"\\n\".join([caption, ocr_text, title])\n",
    "    return img_descr\n",
    "\n",
    "\n",
    "def answer_openai(client: OpenAI, model: str, prompt: str, img_descr: str, base64_image=None):\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "    ]\n",
    "    if base64_image:\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}})\n",
    "    if img_descr:\n",
    "        content.append({\"type\": \"text\", \"text\": f\"Image description: {img_descr}\"})\n",
    "\n",
    "    messages = []\n",
    "    if not model.startswith(\"qwen\"):\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT,\n",
    "            },\n",
    "        )\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=256,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    response_parsed = response.choices[0].message.content\n",
    "    return response_parsed\n",
    "\n",
    "\n",
    "def answer_anthropic(prompt, img_descr, base64_image=None):\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "    ]\n",
    "    if base64_image:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"image\", \n",
    "                \"source\": {\n",
    "                    \"type\": \"base64\",\n",
    "                    \"media_type\": \"image/png\",\n",
    "                    \"data\": base64_image,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    if img_descr:\n",
    "        content.append({\"type\": \"text\", \"text\": f\"Image description: {img_descr}\"})\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        },\n",
    "    ]\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-latest\",\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=messages,\n",
    "        max_tokens=256,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    response_parsed = \"no answer\"\n",
    "    if len(response.content):\n",
    "        response_parsed = response.content[0].text\n",
    "    return response_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_img_path = Path(\"./data/images\")\n",
    "with open(\"./data/03_img_json_with_questions.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "img_fnames = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"img_fname\": [],\n",
    "    \"question\": [],\n",
    "    \"gpt_text\": [], \n",
    "    \"gpt_image\": [], \n",
    "    \"qwen_text\": [], \n",
    "    \"qwen_image\": [], \n",
    "    \"claude_text\": [],\n",
    "    \"claude_image\": [],\n",
    "    \"answer\": [],\n",
    "}\n",
    "for img_fname in tqdm(img_fnames):\n",
    "    results[\"img_fname\"].append(img_fname)\n",
    "\n",
    "    img_data = data[img_fname]\n",
    "    img_descr = build_img_decription(img_data)\n",
    "    qna_pairs = img_data[\"questions\"]\n",
    "    _, base64_image = load_image(base_img_path.joinpath(img_fname))\n",
    "\n",
    "    for qna in qna_pairs:\n",
    "        results[\"question\"].append(qna[\"question\"])\n",
    "        results[\"answer\"].append(qna[\"answer\"])\n",
    "\n",
    "        response_gpt_text = answer_openai(\n",
    "            openai_client,\n",
    "            \"gpt-4o\",\n",
    "            TASK_PROMPT_TEXT_INPUT.format(qna[\"question\"]), \n",
    "            img_descr,\n",
    "        )\n",
    "        results[\"gpt_text\"].append(response_gpt_text)\n",
    "\n",
    "        response_gpt_image = answer_openai(\n",
    "            openai_client,\n",
    "            \"gpt-4o\",\n",
    "            TASK_PROMPT_TEXT_IMAGE_INPUT.format(qna[\"question\"]), \n",
    "            img_descr, \n",
    "            base64_image,\n",
    "        )\n",
    "        results[\"gpt_image\"].append(response_gpt_image)\n",
    "\n",
    "        response_qwen_text = answer_openai(\n",
    "            alibaba_client,\n",
    "            \"qwen-vl-max\",\n",
    "            TASK_PROMPT_TEXT_INPUT.format(qna[\"question\"]), \n",
    "            img_descr,\n",
    "        )\n",
    "        results[\"qwen_text\"].append(response_qwen_text)\n",
    "\n",
    "        response_qwen_image = answer_openai(\n",
    "            alibaba_client,\n",
    "            \"qwen-vl-max\",\n",
    "            TASK_PROMPT_TEXT_IMAGE_INPUT.format(qna[\"question\"]), \n",
    "            img_descr,\n",
    "            base64_image,\n",
    "        )\n",
    "        results[\"qwen_image\"].append(response_qwen_image)\n",
    "\n",
    "        response_claude_text = answer_anthropic(\n",
    "            TASK_PROMPT_TEXT_INPUT.format(qna[\"question\"]), \n",
    "            img_descr, \n",
    "        )\n",
    "        results[\"claude_text\"].append(response_claude_text)\n",
    "\n",
    "        response_claude_image = answer_anthropic(\n",
    "            TASK_PROMPT_TEXT_IMAGE_INPUT.format(qna[\"question\"]), \n",
    "            img_descr, \n",
    "            base64_image,\n",
    "        )\n",
    "        results[\"claude_image\"].append(response_claude_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/qna_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
